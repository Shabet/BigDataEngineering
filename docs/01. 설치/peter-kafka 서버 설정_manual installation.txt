# systemctl status zookeeper-server
# systemctl status kafka-server
# systemctl status nifi
# systemctl status filebeat
# systemctl status elasticsearch

ref) https://eyeballs.tistory.com/289
export JAVA_HOME=/usr/lib/jvm/java-1.8.0-openjdk/


o 서버벌 설치 S/W

  host         |     IP         | MariaDB | Hive | Zookeeper | Kafka  | NiFi   |Hadoop | Spark
---------------+----------------+---------+------+-----------+--------+--------+-------+-------
peter-kafka001 | 192.168.126.71 |    O    |   O  |     O     |   O    |   O    |   O   |   O   
peter-kafka002 | 192.168.126.72 |    O    |   O  |     O     |   O    |   O    |   O   |       
peter-kafka003 | 192.168.126.73 |         |      |     O     |   O    |        |   O   |       
---------------+----------------+---------+------+-----------+--------+--------+-------+--------

설치 프로그램은 /opt에 설치

아래의 순서대로 설치)
Zookeeper: 3.5.9 버전 설치 (3.5.x branch 중 최신 버전)
Kafka: 2.13-2.7.0 버전 설치(scala-kafka 버전, 2021-02-15 현재 가장 최신 버전)
Hadoop: 2.10.1 버전 설치


OS설정
======
o 방화벽 중지
    # systemctl status firewalld
    # systemctl stop firewalld
    # systemctl disable firewalld

o SELinux 기능 제거
    # setenforce 0
    # vi /etc/selinux/config
    SELINUX=disabled

Java 설치
=========
# rpm -qa |grep openjdk
# rpm -e java-1.7.0-openjdk-1.7.0.261-2.6.22.2.el7_8.x86_64
# rpm -e java-1.7.0-openjdk-headless-1.7.0.261-2.6.22.2.el7_8.x86_64
# yum -y install java-1.8.0-openjdk-devel
# rpm -qa|grep openjdk
# javac -version
javac 1.8.0_282
# java -version
openjdk version "1.8.0_282"
OpenJDK Runtime Environment (build 1.8.0_282-b08)
OpenJDK 64-Bit Server VM (build 25.282-b08, mixed mode)


hosts 파일 설정
===============
# vi /etc/hosts

##### Kafka(VM 테스트) #####
192.168.126.71	peter-kafka001	peter-zk001
192.168.126.72	peter-kafka002	peter-zk002
192.168.126.73	peter-kafka003	peter-zk003
192.168.126.79	peter-client

NOTE. 호스트3대를 이용하여 zookeeper와 kafka를 설치


IPv4 설정
==========
# vi /etc/sysctl.conf
net.ipv6.conf.all.disable_ipv6 = 1
# sysctl -p

ref) https://stackoverflow.com/questions/11850655/how-can-i-disable-ipv6-stack-use-for-ipv4-ips-on-jre


Zookeeper 설치
==============
1. 계정 추가
# groupadd -g 2181 zookeepeer
# useradd zookeeper -u 2181 -g zookeeper

2. 설치
# mkdir ~/work
# cd ~/work
# wget https://downloads.apache.org/zookeeper/zookeeper-3.5.9/apache-zookeeper-3.5.9-bin.tar.gz
# cd /opt
# tar zxvf ~/work/apache-zookeeper-3.5.9-bin.tar.gz
# ln -s apache-zookeeper-3.5.9-bin zookeeper

3. 저장소 설정
# mkdir /zkdata

in peter-kafka001,
[root@peter-kafka001]# echo 1 > /zkdata/myid
in peter-kafka002,
[root@peter-kafka002]# echo 2 > /zkdata/myid
in peter-kafka003,
[root@peter-kafka003]# echo 3 > /zkdata/myid

4. 설정
# cd /opt/zookeeper/conf
# cp -p zoo_sample.cfg zoo.cfg
# vi zoo.cfg
<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
...
admin.serverPort=9001
dataDir=/zkdata

server.1=peter-zk001:2888:3888
server.2=peter-zk002:2888:3888
server.3=peter-zk003:2888:3888
...
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>

5. 실행 계정 설정
# chown -R zookeeper:zookeeper /zkdata/
# chown -R zookeeper:zookeeper /opt/apache-zookeeper-3.5.9-bin/
# chown -R zookeeper:zookeeper /opt/zookeeper

6. 자동 실행 등록(systemd)
# vi /etc/systemd/system/zookeeper-server.service
<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
[Unit]
Description=zookeeper-server
After=network.target

[Service]
Type=forking
User=zookeeper
Group=zookeeper
SyslogIdentifier=zookeeper-server
WorkingDirectory=/opt/zookeeper
Restart=always
RestartSec=0s
ExecStart=/opt/zookeeper/bin/zkServer.sh start
ExecStop=/opt/zookeeper/bin/zkServer.sh stop

[Install]
WantedBy=multi-user.target
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>

7. 시작
# systemctl daemon-reload
# systemctl start zookeeper-server.service
# systemctl status zookeeper-server.service

8. 확인
# jps
# ps -ef|grep zookeeper
# netstat -ntlp|grep 2181
tcp        0      0 0.0.0.0:2181            0.0.0.0:*               LISTEN      2715/java           #<- tcp6이 아닌 tcp인지 여부도 확인!
# netstat -ntlp|grep 9001
tcp        0      0 0.0.0.0:9001            0.0.0.0:*               LISTEN      46237/java     
# su - zookeeper
$ /opt/zookeeper/bin/zkCli.sh
ls /

9. 시스템 재시작시 자동 실행 등록
# systemctl enable zookeeper-server.service

10. 편의를 위한 경로 추가
# vi /etc/profile.d/zookeeper.sh
export PATH=$PATH:/opt/zookeeper/bin

.END OF ZOOKEEPER


Kafka 설치
==========
1. 계정 추가
# groupadd -g 9092 kafka
# useradd kafka -u 9092 -g kafka

2. 설치
# cd ~/work
# wget https://archive.apache.org/dist/kafka/2.7.0/kafka_2.13-2.7.0.tgz
# cd /opt
# tar zxvf ~/work/kafka_2.13-2.7.0.tgz
# ln -s kafka_2.13-2.7.0 kafka

3. 저장소 설정
# mkdir /kfdata1 /kfdata2

4. 설정
# cd /opt/kafka/config
# cp -p server.properties server.properties.orig
# vi server.properties
<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
...
broker.id=1 #<- 1 or 2 or 3(각 호스트마다 다르게 설정)
log.dirs=/kfdata1,/kfdata2
zookeeper.connect=peter-zk001:2181,peter-zk002:2181,peter-zk003:2181/peter-kafka
...
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>

# vi jmx
JMX_PORT=9999

5. 실행 계정 설정
# chown -R kafka:kafka /kfdata1/
# chown -R kafka:kafka /kfdata2/
# chown -R kafka:kafka /opt/kafka_2.13-2.7.0/
# chown -R kafka:kafka /opt/kafka

6. 자동 실행 등록(systemd)
# vi /etc/systemd/system/kafka-server.service
<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
[Unit]
Description=kafka-server
After=network.target

[Service]
Type=simple
User=kafka
Group=kafka
SyslogIdentifier=kafka-server
WorkingDirectory=/opt/kafka
Restart=no
RestartSec=0s
ExecStart=/opt/kafka/bin/kafka-server-start.sh /opt/kafka/config/server.properties
ExecStop=/opt/kafka/bin/kafka-server-stop.sh
EnvironmentFile=/opt/kafka/config/jmx
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>

7. 시작
# systemctl daemon-reload
# systemctl start kafka-server.service
# systemctl status kafka-server.service

8. 확인
# jps
# ps -ef|grep kafka
# netstat -ntlp |grep 9092
tcp        0      0 0.0.0.0:9092            0.0.0.0:*               LISTEN      3751/java       #<- Kafka 기동 확인
# /opt/zookeeper/bin/zkCli.sh # <- 모든 서버에서 확인
[zk: localhost:2181(CONNECTED) 1] ls /peter-kafka/brokers/ids
[1, 2, 3]

9. 로그 확인
o Kafka 로그 확인
# tail -F /opt/kafka/logs/server.log

o Kafka 로드시 Config 확인
# cat /opt/kafka/logs/server.log
{
[2021-01-26 13:37:04,206] INFO KafkaConfig values: 
...
 (kafka.server.KafkaConfig)
} #<- Kafka 로드시 Config 정보 확인할수 있는 로그 블록

10. 시스템 재시작시 자동 실행 등록
# systemctl enable kafka-server.service
NOTE. 테스트 결과 안됨(Kafka는 시스템 시작시 수동으로 실행하자!)

11. 편의를 위한 경로 추가
# vi /etc/profile.d/kafka.sh
export PATH=$PATH:/opt/kafka/bin

12. 토픽 생성, 삭제
o 생성
# /usr/local/kafka/bin/kafka-topics.sh \
> --zookeeper peter-zk001:2181,peter-zk002:2181,peter-zk003:2181/peter-kafka \
> --replication-factor 1 --partitions 1 --topic peter-topic --create
Created topic peter-topic.

o 삭제
# /usr/local/kafka/bin/kafka-topics.sh \
> --zookeeper peter-zk001:2181,peter-zk002:2181,peter-zk003:2181/peter-kafka \
> --topic peter-topic --delete
Topic peter-topic is marked for deletion.
Note: This will have no impact if delete.topic.enable is not set to true.

13. 프로듀서, 컨슈머
o 프로듀서 생성
# /usr/local/kafka/bin/kafka-console-producer.sh \
> --broker-list peter-kafka001:9092,peter-kafka002:9092,peter-kafka003:9092 \
> --topic peter-topic
>       # <- 입력 프롬프트

o 컨슈머 생성
# /usr/local/kafka/bin/kafka-console-consumer.sh \
> --bootstrap-server peter-kafka001:9092,peter-kafka002:9092,peter-kafka003:9092 \
> --topic peter-topic --from-beginning

.END OF KAFKA


Yahoo/CMAK 설치(구 kafka-manager)
================================
# cd ~/work
# wget https://github.com/AdoptOpenJDK/openjdk11-binaries/releases/download/jdk-11.0.10%2B9/OpenJDK11U-jdk_x64_linux_hotspot_11.0.10_9.tar.gz
# cd /usr/local
# tar zxvf ~/work/OpenJDK11U-jdk_x64_linux_hotspot_11.0.10_9.tar.gz
# /usr/local/jdk-11.0.10+9/bin/javac -version

NOTE. 최신 버전은 java-11에서만 실행이 됨. (별도의 서버에서 Java11을 설치해서 해도 됨)

# cd ~/work
# wget https://github.com/yahoo/CMAK/archive/3.0.0.5.tar.gz
# cd CMAK-3.0.0.5
# cp -p sbt sbt.orig
# vi sbt
setJavaHome() {
  java_cmd="$1/bin/java"
  setThisBuild javaHome "_root_.scala.Some(file(\"$1\"))"
  export JAVA_HOME="/usr/local/jdk-11.0.10+9" # <- here.
  export JDK_HOME="/usr/local/jdk-11.0.10+9" # <- here.
  export PATH="$JAVA_HOME/bin:$PATH"
}

[info] Your package is ready in /root/work/CMAK-3.0.0.5/target/universal/cmak-3.0.0.5.zip

/opt/kafka-manager/bin/cmak -Dconfig.file=/opt/kafka-manager/conf/application.conf -Dhttp.port=9000

@@@Yahoo/CMAK 설치(구 kafka-manager)
@@@=================================
@@@ref) https://log-laboratory.tistory.com/180
@@@
@@@NOTE. 3.X.X.X 버전은 JAVA 11부터 되는 것 같아서 안전하게 CMAK-1.3.3.23.tar 다운로드하였다.
@@@
@@@# cd ~/work
@@@# wget https://github.com/yahoo/CMAK/archive/2.0.0.2.tar.gz
@@@# tar zxvf 2.0.0.2.tar.gz
@@@# cd CMAK-2.0.0.2
@@@# ./sbt clean dist
@@@(sbt-launch.jar 를 download할수 없다고 에러가 발생)
@@@
@@@# cd ~/work
@@@# wget https://github.com/sbt/sbt/releases/download/v1.2.8/sbt-1.2.8.tgz
@@@# tar zxvf sbt-1.2.8.tgz
@@@(sbt/bin/sbt-launch.jar 가 있음)
@@@# cd CMAK-2.0.0.2
@@@# cp -p ../sbt/bin/sbt-launch.jar /root/.sbt/launchers/1.2.8/.
@@@# ./sbt clean dist


filebeat 설치
=============
ref) https://www.elastic.co/downloads/beats/filebeat
     https://www.elastic.co/guide/en/beats/filebeat/7.10/setup-repositories.html#_yum


# rpm --import https://packages.elastic.co/GPG-KEY-elasticsearch
# vi /etc/yum.repos.d/elastic.repo
[elastic-7.x]
name=Elastic repository for 7.x packages
baseurl=https://artifacts.elastic.co/packages/7.x/yum
gpgcheck=1
gpgkey=https://artifacts.elastic.co/GPG-KEY-elasticsearch
enabled=1
autorefresh=1
type=rpm-md
# yum search filebeat
# yum -y install filebeat
# cd /etc/filebeat
# cp -p filebeat.yml filebeat.yml.orig
# vi filebeat.yml
...
# systemctl start filebeat.service
# systemctl status filebeat.service


NiFi 설치
==========
1. 계정 추가
# groupadd -g 8080 nifi
# useradd  nifi -u 8080 -g nifi

2. 설치
# cd ~/work
# wget https://archive.apache.org/dist/nifi/1.11.4/nifi-1.11.4-bin.tar.gz
# wget https://archive.apache.org/dist/nifi/1.11.4/nifi-toolkit-1.11.4-bin.tar.gz
# wget https://archive.apache.org/dist/nifi/1.11.4/nifi-1.11.4-source-release.zip
# cd /opt
# tar zxvf ~/work/nifi-1.11.4-bin.tar.gz
# ln -s nifi-1.11.4 nifi

3. 설정
# cd /opt/nifi/conf

o nifi.properties
# cp -p nifi.properties nifi.properties.orig
# vi nifi.properties
<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
...
nifi.web.http.host=peter-kafka00[1|2|3]
nifi.web.http.port=8080
#nifi.web.http.port=8081 # <- zookeeper에서 8080포트를 사용함으로 변경!

nifi.cluster.is.node=true
nifi.cluster.node.address=peter-kafka00[1|2|3]
nifi.cluster.node.protocol.port=8082

#nifi.zookeeper.connect.string=192.168.126.71:2181,192.168.126.72:2181,192.168.126.73:2181
nifi.zookeeper.connect.string=peter-kafka001:2181,peter-kafka002:2181,peter-kafka003:2181
#nifi.zookeeper.connect.string=peter-zk001:2181,peter-zk002:2181,peter-zk003:2181   # <- 이건 이상하게 동작함. 클라스터 멤버가 3이 아닌 6으로 인식하는 문제 발생. 끊임없는 leader선출...
...
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>

o bootstrap.conf
# cp -p bootstrap.conf bootstrap.conf.orig
# vi bootstrap.conf
<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
...
run.as=nifi

# JVM memory settings
java.arg.2=-Xms1024m  # <- default set to 512m
java.arg.3=-Xmx1024m  # <- default set to 512m
...
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>

o state-management.xml
ref) https://stackoverflow.com/questions/59826510/a-hostprovider-may-not-be-empty-after-upgrading-to-nifi-1-10

# cp -p state-management.xml state-management.xml.orig
# vi state-management.xml
<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
...
<cluster-provider>
        <id>zk-provider</id>
        <class>org.apache.nifi.controller.state.providers.zookeeper.ZooKeeperStateProvider</class>
        <property name="Connect String">peter-kafka001:2181,peter-kafka002:2181,peter-kafka003:2181</property> # <- 여기 값 추가
        <property name="Root Node">/nifi</property>
        <property name="Session Timeout">10 seconds</property>
        <property name="Access Control">Open</property>
    </cluster-provider>
...
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>

NOTE. ListSFTP 와 같은 processor에서 위의 zookeeper 설정값을 사용함!(설정을 하지 않을시 HostProvider may not empty 에러 발생)

o nifi-env.sh 파일 설정
# cd /opt/nifi/bin
# cp -p nifi-env.sh nifi-env.sh.orig
# vi nifi-env.sh
<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
...
export JAVA_HOME=/usr/lib/jvm/java-1.8.0-openjdk/
...
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>

4. 불필요한 파일 삭제(in Linux)
# cd /opt/nifi
# /bin/rm -f bin/*.bat

5. 기타설정
# vi /etc/profile.d/java.sh
export JAVA_HOME=/usr/lib/jvm/java-1.8.0-openjdk/

6. 실행 계정 설정
# chown -R nifi:nifi /opt/nifi-1.11.4/
# chown -R nifi:nifi /opt/nifi

7. 자동 실행 등록(systemd)
# /opt/nifi/bin/nifi.sh install nifi
# source /etc/profile.d/java.sh
# systemctl daemon-reload
# systemctl start nifi.service
# systemctl status nifi.service

8. 확인
# jps
# ps -ef|grep nifi
# netstat -ntlp | grep 8080
# /opt/zookeeper/bin/zkCli.sh
ls /nifi

9. 웹 접속
웹URL: http://peter-kafka001:8080/nifi/
       http://peter-kafka002:8080/nifi/
       http://peter-kafka003:8080/nifi/

.END OF KAFKA


elasticsearch 설치
==================
[root@peter-kafka001]# yum install elasticsearch
# cd /etc/elasticsearch/
# cp -p elasticsearch.yml elasticsearch.yml.orig
# vi elasticsearch.yml

# systemctl status elasticsearch
# systemctl start elasticsearch

# yum install kibana


Hadoop 설치
==========
ref) https://excelsior-cjh.tistory.com/73

0. 구성
-----------------+--------------------------------------------------+----------------------
 구분            | Host                                             | 비고
-----------------+--------------------------------------------------+----------------------
NodeManager      | peter-kafka001, peter-kafka002, peter-kafka003   | 
ResourceManager  | peter-kafka001, peter-kafka002                   | Active
Namenode         | peter-kafka001, peter-kafka002                   | Namenode Active
Datanode         | peter-kafka001, peter-kafka002, peter-kafka003   | 
journalnode      | peter-kafka001, peter-kafka002, peter-kafka003   | /w Zookeeper
JobHistoryServer | peter-kafka001                                   | 
-----------------+--------------------------------------------------+----------------------

1. 계정 추가
# groupadd -g 1004 hadoop

# useradd hadoop -u 1004 -g hadoop
# useradd hdfs -u 8020 -g hadoop
# useradd yarn -u 8032 -g hadoop
# useradd mapred -u 19888 -g hadoop

NOTE. hadoop 유저 계정은 사용하지 않음.

# su - hdfs
$ ssh-keygen -t rsa
$ exit
# passwd hdfs
(input password: ****)
$ su - hdfs
$ ssh-copy-id -i ~/.ssh/id_rsa.pub hdfs@peter-kafka001
$ ssh-copy-id -i ~/.ssh/id_rsa.pub hdfs@peter-kafka002
$ ssh-copy-id -i ~/.ssh/id_rsa.pub hdfs@peter-kafka003

# su - yarn
$ ssh-keygen -t rsa
$ exit
# passwd yarn
(input password: ****)
$ su - yarn
$ ssh-copy-id -i ~/.ssh/id_rsa.pub yarn@peter-kafka001
$ ssh-copy-id -i ~/.ssh/id_rsa.pub yarn@peter-kafka002
$ ssh-copy-id -i ~/.ssh/id_rsa.pub yarn@peter-kafka003

# su - mapred
$ ssh-keygen -t rsa
$ exit
# passwd mapred
(input password: ******)
$ su - mapred
$ ssh-copy-id -i ~/.ssh/id_rsa.pub mapred@peter-kafka001
$ ssh-copy-id -i ~/.ssh/id_rsa.pub mapred@peter-kafka002
$ ssh-copy-id -i ~/.ssh/id_rsa.pub mapred@peter-kafka003

2. 설치
# cd ~/work
# wget https://downloads.apache.org/hadoop/common/hadoop-2.10.1/hadoop-2.10.1.tar.gz
# cd /opt
# tar zxvf ~/work/hadoop-2.10.1.tar.gz
# ln -s hadoop-2.10.1 hadoop
(in Linux)
# find /opt/hadoop/ -name "*.cmd" -delete -print

3. 설정
# cd /opt/hadoop/etc/hadoop/
# cp -p slaves slaves.orig
# vi slaves
peter-kafka001
peter-kafka002
peter-kafka003
# cat slaves > include_server

# cp -p core-site.xml core-site.xml.orig
# vi core-site.xml
<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
<configuration>
  <property>
    <name>fs.defaultFS</name>
    <value>hdfs://peter-cluster</value>
  </property>
  <property>
    <name>ha.zookeeper.quorum</name>
    <value>peter-kafka001:2181,peter-kafka002:2181,peter-kafka003:2181</value>
  </property>
  <property>
    <name>ha.zookeeper.session-timeout.ms</name>
    <value>300000</value>
  </property>
  <property>
    <name>fs.trash.interval</name>
    <value>60</value>
  </property>
  <property>
    <name>hadoop.proxyuser.yarn.hosts</name>
    <value>*</value>
  </property>
  <property>
    <name>hadoop.proxyuser.yarn.groups</name>
    <value>*</value>
  </property>
  <!-- Add Tuning Parameters -->
  <property>
    <name>io.file.buffer.size</name>
    <value>131072</value>
  </property>
  <property>
    <name>io.compression.codecs</name>
    <value>
      org.apache.hadoop.io.compress.GzipCodec,
      org.apache.hadoop.io.compress.DefaultCodec,
      org.apache.hadoop.io.compress.BZip2Codec,
      org.apache.hadoop.io.compress.SnappyCodec
    </value>
  </property>
  <property>
    <name>ipc.client.connect.timeout</name>
    <value>90000</value>
  </property>
  <!-- hive -->
  <property>
    <name>hadoop.proxyuser.hive.groups</name>
    <value>*</value>
  </property>
  <property>
    <name>hadoop.proxyuser.hive.hosts</name>
    <value>*</value>
  </property>
</configuration>
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>

# cp -p hdfs-site.xml hdfs-site.xml.orig
# vi hdfs-site.xml
<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
<configuration>
  <property>
    <name>dfs.replication</name>
    <value>2</value>
  </property>
  <property>
    <name>dfs.namenode.name.dir</name>
    <!-- <value>${lightningdb.home}/data/hadoop/nn</value> -->
    <!-- <value>/home/ltdb/lightningdb/data/hadoop/nn</value> -->
    <!-- <value>/data/hdfs/nn</value> -->
    <value>/dfs/nn</value>
  </property>
  <property>
    <name>dfs.datanode.data.dir</name>
    <!--<value>/data01/hdfs/dn,/data02/hdfs/dn,/data03/hdfs/dn,/data04/hdfs/dn,/data05/hdfs/dn,/data06/hdfs/dn</value>-->
    <value>/dfs/dn</value>
  </property>
  <property>
    <name>dfs.hosts</name>
    <!-- <value>${hadoop.home}/etc/hadoop/include_server</value> -->
    <value>/opt/hadoop/etc/hadoop/include_server</value>
  </property>
  <property>
    <name>fs.hdfs.impl</name>
    <value>org.apache.hadoop.hdfs.DistributedFileSystem</value>
    <description>The FileSystem for hdfs: uris.</description>
  </property>

  <!-- Common server name -->
  <property>
    <name>hadoop.tmp.dir</name>
    <!-- <value>${lightningdb.home}/data/hadoop/jn/tmp</value> -->
    <!-- <value>/home/ltdb/lightningdb/data/hadoop/jn/tmp</value> -->
    <value>/dfs/jn/tmp</value>
  </property>
  <property>
    <name>dfs.nameservices</name>
    <!-- <value>lightningdb-cluster</value> -->
    <value>peter-cluster</value>
  </property>
  <property>
    <name>dfs.journalnode.edits.dir</name>
    <!-- <value>${lightningdb.home}/data/hadoop/jn</value> -->
    <!-- <value>/home/ltdb/lightningdb/data/hadoop/jn</value> -->
    <value>/dfs/jn</value>
  </property>

  <!-- HA configuration -->
  <property>
    <name>dfs.ha.namenodes.peter-cluster</name>
    <value>nn1,nn2</value>
  </property>
  <property>
    <name>dfs.namenode.rpc-address.peter-cluster.nn1</name>
    <value>peter-kafka001:8020</value>
  </property>
  <property>
    <name>dfs.namenode.rpc-address.peter-cluster.nn2</name>
    <value>peter-kafka002:8020</value>
  </property>
  <property>
    <name>dfs.namenode.http-address.peter-cluster.nn1</name>
    <value>peter-kafka001:50070</value>
  </property>
    <property>
    <name>dfs.namenode.http-address.peter-cluster.nn2</name>
  <value>peter-kafka002:50070</value>
  </property>
  <property>
    <name>dfs.namenode.http-bind-host.peter-cluster.nn1</name>
    <value>0.0.0.0</value>
  </property>
  <property>
    <name>dfs.namenode.http-bind-host.peter-cluster.nn2</name>
    <value>0.0.0.0</value>
  </property>

  <!-- Storage for edits' files -->
  <property>
    <name>dfs.namenode.shared.edits.dir</name>
    <value>qjournal://peter-kafka001:8485;peter-kafka002:8485;peter-kafka003:8485/peter-cluster</value>
  </property>
  <property>
    <name>dfs.qjournal.start-segment.timeout.ms</name>
    <value>90000</value>
  </property>
  <property>
    <name>dfs.qjournal.select-input-streams.timeout.ms</name>
    <value>90000</value>
  </property>
  <property>
    <name>dfs.qjournal.write-txns.timeout.ms</name>
    <value>90000</value>
  </property>
  <!-- Client failover -->
  <property>
    <name>dfs.client.failover.proxy.provider.peter-cluster</name>
    <value>org.apache.hadoop.hdfs.server.namenode.ha.ConfiguredFailoverProxyProvider</value>
  </property>
  <property>
    <name>dfs.ha.fencing.methods</name>
    <value>sshfence</value>
  </property>
  <property>
    <name>dfs.ha.fencing.ssh.private-key-files</name>
    <value>/home/hdfs/.ssh/id_rsa</value>
  </property>

  <!-- Automatic failover configuration -->
  <property>
    <name>dfs.ha.automatic-failover.enabled</name>
    <value>true</value>
  </property>

  <!-- appended settings -->
  <property>
    <name>dfs.datanode.max.transfer.threads</name>
    <value>8192</value>
  </property>
  <property>
    <name>dfs.datanode.handler.count</name>
    <value>200</value>
  </property>
  <!--Add Tuning Parameter -->
  <property>
    <name>dfs.blocksize</name>
    <value>134217728</value>
  </property>
  <property>
    <name>dfs.namenode.handler.count</name>
    <value>80</value>
  </property>
  <property>
    <name>dfs.datanode.balance.bandwidthPerSec</name>
    <value>104857600</value>
  </property>
</configuration>
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>


# cp -p yarn-site.xml yarn-site.xml.orig
# vi yarn-site.xml
<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
<configuration>
  <property>
    <description>Classpath for typical applications.</description>
    <name>yarn.application.classpath</name>
    <value>
      $HADOOP_CONF_DIR,
      $HADOOP_COMMON_HOME/share/hadoop/common/*,
      $HADOOP_COMMON_HOME/share/hadoop/common/lib/*,
      $HADOOP_HDFS_HOME/share/hadoop/hdfs/*,
      $HADOOP_HDFS_HOME/share/hadoop/hdfs/lib/*,
      $HADOOP_MAPRED_HOME/share/hadoop/mapreduce/*,
      $HADOOP_MAPRED_HOME/share/hadoop/mapreduce/lib/*,
      $HADOOP_YARN_HOME/share/hadoop/yarn/*,
      $HADOOP_YARN_HOME/share/hadoop/yarn/lib/*
    </value>
  </property>

  <!-- =======================================================================
  JobHistoryServer
  ======================================================================== -->
  <property>
    <description>URL for job history server</description>
    <name>yarn.log.server.url</name>
    <value>http://peter-kafka001:19888/jobhistory/logs/</value>
  </property>

  <!-- =======================================================================
  NodeManager
  ======================================================================== -->
  <property>
    <name>yarn.nodemanager.local-dirs</name>
    <!-- <value>${lightningdb.home}/data/hadoop/yarn/local</value> -->
    <!-- <value>/home/ltdb/lightningdb/data/hadoop/yarn/local</value> -->
    <!-- <value>/data01/yarn/local,/data02/yarn/local,/data03/yarn/local,/data04/yarn/local,/data05/yarn/local,/data06/yarn/local</value> -->
    <value>/yarn/nm</value>
  </property>
  <property>
    <name>yarn.nodemanager.log-dirs</name>
    <!-- <value>${lightningdb.home}/log/hadoop</value> -->
    <!-- <value>/home/ltdb/lightningdb/log/hadoop</value> -->
    <!-- <value>/data01/yarn/log,/data02/yarn/log,/data03/yarn/log,/data04/yarn/log,/data05/yarn/log,/data06/yarn/log</value> -->
    <value>/yarn/container-logs</value>
  </property>
  <property>
    <description>Where to aggregate logs to.</description>
    <name>yarn.nodemanager.remote-app-log-dir</name>
    <!-- <value>${lightningdb.home}/data/hadoop/yarn</value> -->
    <!-- <value>/home/ltdb/lightningdb/data/hadoop/yarn</value> -->
    <value>/LOG</value>
  </property>

  <!-- =======================================================================
  for spark
  ======================================================================== -->
  <!-- property>
    <name>yarn.nodemanager.aux-services</name>
    <value>mapreduce_shuffle</value>
  </property>
  <property>
    <name>yarn.nodemanager.aux-services.mapreduce.shuffle.class</name>
    <value>org.apache.hadoop.mapred.ShuffleHandler</value>
  </property -->

  <property>
    <name>yarn.nodemanager.aux-services</name>
    <value>mapreduce_shuffle,spark_shuffle</value>
  </property>
  <property>
    <name>yarn.nodemanager.aux-services.spark_shuffle.class</name>
    <value>org.apache.spark.network.yarn.YarnShuffleService</value>
  </property>

  <!-- optional -->
  <!-- property>
    <name>spark.shuffle.service.port</name>
    <value>10000</value>
  </property -->

  <property>
    <name>spark.authenticate</name>
    <value>true</value>
  </property>

  <!-- =======================================================================
  Resource Manger
  ======================================================================== -->
  <property>
    <name>yarn.resourcemanager.cluster-id</name>
    <value>resourcemanager-cluster</value>
  </property>
  <property>
    <name>yarn.resourcemanager.ha.rm-ids</name>
    <value>rm1,rm2</value>
  </property>
  <property>
    <name>yarn.resourcemanager.hostname.rm1</name>
    <value>peter-kafka001</value>
  </property>
  <property>
    <name>yarn.resourcemanager.hostname.rm2</name>
    <value>peter-kafka002</value>
  </property>
  <property>
    <name>yarn.resourcemanager.zk-address</name>
    <value>peter-kafka001:2181,peter-kafka002:2181,peter-kafka003:2181</value>
  </property>
  <property>
    <name>yarn.resourcemanager.ha.enabled</name>
    <value>true</value>
  </property>
  <property>
    <name>yarn.resourcemanager.ha.automatic-failover.enabled</name>
    <value>true</value>
  </property>
  <property>
    <name>yarn.resourcemanager.ha.automatic-failover.embedded</name>
    <value>true</value>
  </property>
  <property>
    <description>Optional setting. The default value is /yarn-leader-election</description>
    <name>yarn.resourcemanager.ha.automatic-failover.zk-base-path</name>
    <value>/yarn-leader-election</value>
  </property>
  <property>
    <name>yarn.resourcemanager.store.class</name>
    <value>org.apache.hadoop.yarn.server.resourcemanager.recovery.ZKRMStateStore</value>
  </property>
  <property>
    <name>yarn.resourcemanager.recovery.enabled</name>
    <value>true</value>
  </property>
  <property>
    <name>yarn.resourcemanager.scheduler.class</name>
    <value>org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.CapacityScheduler</value>
  </property>
  <property>
    <name>yarn.resourcemanager.connect.retry-interval.ms</name>
    <value>2000</value>
  </property>
  <property>
    <name>yarn.client.failover-proxy-provider</name>
    <value>org.apache.hadoop.yarn.client.ConfiguredRMFailoverProxyProvider</value>
  </property>

   <!-- streaming token check -->
  <property>
    <name>yarn.resourcemanager.proxy-user-privileges.enabled</name>
    <value>true</value>
  </property>

  <!-- Resource Configs -->
  <property>
    <name>yarn.nodemanager.resource.memory-mb</name>
    <value>204800</value>
  </property>
  <property>
    <name>yarn.nodemanager.resource.cpu-vcores</name>
    <value>48</value>
  </property>

  <!-- Resource Manger 1 -->
  <property>
    <name>yarn.resourcemanager.address.rm1</name>
    <value>peter-kafka001:8032</value>
  </property>
  <property>
    <name>yarn.resourcemanager.scheduler.address.rm1</name>
    <value>peter-kafka001:8030</value>
  </property>
  <property>
    <name>yarn.resourcemanager.webapp.address.rm1</name>
    <value>0.0.0.0:8088</value>
  </property>
  <property>
    <name>yarn.resourcemanager.resource-tracker.address.rm1</name>
    <value>peter-kafka001:8031</value>
  </property>
  <property>
    <name>yarn.resourcemanager.admin.address.rm1</name>
    <value>peter-kafka001:8033</value>
  </property>

  <!-- Resource Manger 2 -->
  <property>
    <name>yarn.resourcemanager.address.rm2</name>
    <value>peter-kafka002:8032</value>
  </property>
  <property>
    <name>yarn.resourcemanager.scheduler.address.rm2</name>
    <value>peter-kafka002:8030</value>
  </property>
  <property>
    <name>yarn.resourcemanager.webapp.address.rm2</name>
    <value>peter-kafka002:8088</value>
  </property>
  <property>
    <name>yarn.resourcemanager.resource-tracker.address.rm2</name>
    <value>peter-kafka002:8031</value>
  </property>
  <property>
    <name>yarn.resourcemanager.admin.address.rm2</name>
    <value>peter-kafka002:8033</value>
  </property>

  <!-- =======================================================================
  Scheduler
  ======================================================================== -->
  <property>
    <name>yarn.scheduler.maximum-allocation-mb</name>
    <value>131072</value>
  </property>
  <property>
    <name>yarn.scheduler.minimum-allocation-mb</name>
    <value>2048</value>
  </property>
  <property>
    <name>yarn.scheduler.maximum-allocation-vcores</name>
    <value>48</value>
  </property>

  <!-- =======================================================================
  etc...
  ======================================================================== -->
  <property>
    <name>yarn.web-proxy.address</name>
    <value>peter-kafka001:8088</value>
  </property>
  <property>
    <name>yarn.nodemanager.vmem-check-enabled</name>
    <value>false</value>
  </property>
  <property>
    <name>yarn.nodemanager.pmem-check-enabled</name>
    <value>false</value>
  </property>
  <property>
    <name>yarn.log-aggregation-enable</name>
    <value>true</value>
  </property>

  <property>
    <name>yarn.log-aggregation.retain-seconds</name>
    <value>259200</value>
  </property>
  <property>
    <name>yarn.log-aggregation.retain-check-interval-seconds</name>
    <value>3600</value>
  </property>
</configuration>
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>

4. spark-2.3.4-yarn-shuffle.jar 복사 (hadoop-2.10.0.tar.gz 설치시 필요했음.)
# cd ~/work
# wget https://archive.apache.org/dist/spark/spark-2.3.4/spark-2.3.4-bin-hadoop2.7.tgz
# tar zxvf spark-2.3.4-bin-hadoop2.7.tgz
# cp -p /root/work/spark-2.3.4-bin-hadoop2.7/yarn/spark-2.3.4-yarn-shuffle.jar /opt/hadoop/share/hadoop/yarn/lib/.
# chown hadoop:hadoop /opt/hadoop/share/hadoop/yarn/lib/spark-2.3.4-yarn-shuffle.jar

(in peter-kafka002, peter-kafka003)
# scp -p root@peter-kafka001:/opt/hadoop/share/hadoop/yarn/lib/spark-2.3.4-yarn-shuffle.jar /opt/hadoop/share/hadoop/yarn/lib/.
# chown hadoop:hadoop /opt/hadoop/share/hadoop/yarn/lib/spark-2.3.4-yarn-shuffle.jar

5. 저장소 설정
# mkdir /dfs/
# mkdir /dfs/nn/ /dfs/dn/ /dfs/jn/ /dfs/jn/tmp/ /LOG/

# chown -R hdfs:hadoop /dfs/nn/
# chown -R hdfs:hadoop /dfs/dn/
# chown -R hdfs:hadoop /dfs/jn/
# chown -R hdfs:hadoop /dfs/jn/tmp/
# chown -R yarn:hadoop /LOG/

# chmod 775 /dfs/
# chmod 775 /dfs/nn/
# chmod 770 /dfs/dn/
# chmod 775 /dfs/jn/
# chmod 775 /dfs/jn/tmp/

6. 권한 설정
# chown -R hadoop:hadoop /opt/hadoop-2.10.1/
# chown -R hadoop:hadoop /opt/hadoop
# chmod -R g+w /opt/hadoop-2.10.1/

# mkdir /opt/hadoop/logs/
# chown hadoop:hadoop
# chmod 775 /opt/hadoop/logs/

7. 전체 노드에 설정 복사 : in pater-kafka001
# cd  /opt/hadoop/etc/hadoop/
# scp -p slaves include_server core-site.xml hdfs-site.xml yarn-site.xml root@peter-kafka002:/opt/hadoop/etc/hadoop/.
# scp -p slaves include_server core-site.xml hdfs-site.xml yarn-site.xml root@peter-kafka003:/opt/hadoop/etc/hadoop/.

>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
8. 실행
o JournalNode
    1. Zookeeper file system 포맷 : in peter-kafka001
    # su - hdfs
    $ /opt/hadoop/bin/hdfs zkfc -formatZK

    [hdfs@peter-kafka001 ~]$ /opt/hadoop/bin/hdfs zkfc -formatZK
    21/02/21 12:51:13 INFO tools.DFSZKFailoverController: STARTUP_MSG: 
    /************************************************************
    STARTUP_MSG: Starting DFSZKFailoverController
    STARTUP_MSG:   host = peter-kafka001/192.168.126.71
    STARTUP_MSG:   args = [-formatZK]
    STARTUP_MSG:   version = 2.10.1
    STARTUP_MSG:   classpath = /opt/hadoop-2.10.1/etc/hadoop:/opt/hadoop-2.10.1/share/hadoop/common/lib/commons-collections-3.2.2.jar:/opt/hadoop-2.10.1/share/hadoop/common/lib/servlet-api-2.5.jar:/opt/hadoop-2.10.1/share/hadoop/common/lib/jetty-6.1.26.jar:/opt/hadoop-2.10.1/share/hadoop/common/lib/jetty-util-6.1.26.jar:/opt/hadoop-2.10.1/share/hadoop/common/lib/jetty-sslengine-6.1.26.jar:/opt/hadoop-2.10.1/share/hadoop/common/lib/jsp-api-2.1.jar:/opt/hadoop-2.10.1/share/hadoop/common/lib/jersey-core-1.9.jar:/opt/hadoop-2.10.1/share/hadoop/common/lib/jersey-json-1.9.jar:/opt/hadoop-2.10.1/share/hadoop/common/lib/jettison-1.1.jar:/opt/hadoop-2.10.1/share/hadoop/common/lib/jaxb-impl-2.2.3-1.jar:/opt/hadoop-2.10.1/share/hadoop/common/lib/jaxb-api-2.2.2.jar:/opt/hadoop-2.10.1/share/hadoop/common/lib/stax-api-1.0-2.jar:/opt/hadoop-2.10.1/share/hadoop/common/lib/activation-1.1.jar:/opt/hadoop-2.10.1/share/hadoop/common/lib/jackson-core-asl-1.9.13.jar:/opt/hadoop-2.10.1/share/hadoop/common/lib/jackson-mapper-asl-1.9.13.jar:/opt/hadoop-2.10.1/share/hadoop/common/lib/jackson-jaxrs-1.9.13.jar:/opt/hadoop-2.10.1/share/hadoop/common/lib/jackson-xc-1.9.13.jar:/opt/hadoop-2.10.1/share/hadoop/common/lib/jersey-server-1.9.jar:/opt/hadoop-2.10.1/share/hadoop/common/lib/asm-3.2.jar:/opt/hadoop-2.10.1/share/hadoop/common/lib/log4j-1.2.17.jar:/opt/hadoop-2.10.1/share/hadoop/common/lib/jets3t-0.9.0.jar:/opt/hadoop-2.10.1/share/hadoop/common/lib/java-xmlbuilder-0.4.jar:/opt/hadoop-2.10.1/share/hadoop/common/lib/commons-lang-2.6.jar:/opt/hadoop-2.10.1/share/hadoop/common/lib/commons-configuration-1.6.jar:/opt/hadoop-2.10.1/share/hadoop/common/lib/commons-digester-1.8.jar:/opt/hadoop-2.10.1/share/hadoop/common/lib/commons-beanutils-1.9.4.jar:/opt/hadoop-2.10.1/share/hadoop/common/lib/commons-lang3-3.4.jar:/opt/hadoop-2.10.1/share/hadoop/common/lib/slf4j-api-1.7.25.jar:/opt/hadoop-2.10.1/share/hadoop/common/lib/slf4j-log4j12-1.7.25.jar:/opt/hadoop-2.10.1/share/hadoop/common/lib/avro-1.7.7.jar:/opt/hadoop-2.10.1/share/hadoop/common/lib/paranamer-2.3.jar:/opt/hadoop-2.10.1/share/hadoop/common/lib/snappy-java-1.0.5.jar:/opt/hadoop-2.10.1/share/hadoop/common/lib/commons-compress-1.19.jar:/opt/hadoop-2.10.1/share/hadoop/common/lib/protobuf-java-2.5.0.jar:/opt/hadoop-2.10.1/share/hadoop/common/lib/gson-2.2.4.jar:/opt/hadoop-2.10.1/share/hadoop/common/lib/hadoop-auth-2.10.1.jar:/opt/hadoop-2.10.1/share/hadoop/common/lib/nimbus-jose-jwt-7.9.jar:/opt/hadoop-2.10.1/share/hadoop/common/lib/jcip-annotations-1.0-1.jar:/opt/hadoop-2.10.1/share/hadoop/common/lib/json-smart-1.3.1.jar:/opt/hadoop-2.10.1/share/hadoop/common/lib/apacheds-kerberos-codec-2.0.0-M15.jar:/opt/hadoop-2.10.1/share/hadoop/common/lib/apacheds-i18n-2.0.0-M15.jar:/opt/hadoop-2.10.1/share/hadoop/common/lib/api-asn1-api-1.0.0-M20.jar:/opt/hadoop-2.10.1/share/hadoop/common/lib/api-util-1.0.0-M20.jar:/opt/hadoop-2.10.1/share/hadoop/common/lib/zookeeper-3.4.14.jar:/opt/hadoop-2.10.1/share/hadoop/common/lib/spotbugs-annotations-3.1.9.jar:/opt/hadoop-2.10.1/share/hadoop/common/lib/audience-annotations-0.5.0.jar:/opt/hadoop-2.10.1/share/hadoop/common/lib/netty-3.10.6.Final.jar:/opt/hadoop-2.10.1/share/hadoop/common/lib/curator-framework-2.13.0.jar:/opt/hadoop-2.10.1/share/hadoop/common/lib/curator-client-2.13.0.jar:/opt/hadoop-2.10.1/share/hadoop/common/lib/jsch-0.1.55.jar:/opt/hadoop-2.10.1/share/hadoop/common/lib/curator-recipes-2.13.0.jar:/opt/hadoop-2.10.1/share/hadoop/common/lib/htrace-core4-4.1.0-incubating.jar:/opt/hadoop-2.10.1/share/hadoop/common/lib/stax2-api-3.1.4.jar:/opt/hadoop-2.10.1/share/hadoop/common/lib/woodstox-core-5.0.3.jar:/opt/hadoop-2.10.1/share/hadoop/common/lib/junit-4.11.jar:/opt/hadoop-2.10.1/share/hadoop/common/lib/hamcrest-core-1.3.jar:/opt/hadoop-2.10.1/share/hadoop/common/lib/mockito-all-1.8.5.jar:/opt/hadoop-2.10.1/share/hadoop/common/lib/hadoop-annotations-2.10.1.jar:/opt/hadoop-2.10.1/share/hadoop/common/lib/guava-11.0.2.jar:/opt/hadoop-2.10.1/share/hadoop/common/lib/jsr305-3.0.2.jar:/opt/hadoop-2.10.1/share/hadoop/common/lib/commons-cli-1.2.jar:/opt/hadoop-2.10.1/share/hadoop/common/lib/commons-math3-3.1.1.jar:/opt/hadoop-2.10.1/share/hadoop/common/lib/xmlenc-0.52.jar:/opt/hadoop-2.10.1/share/hadoop/common/lib/httpclient-4.5.2.jar:/opt/hadoop-2.10.1/share/hadoop/common/lib/httpcore-4.4.4.jar:/opt/hadoop-2.10.1/share/hadoop/common/lib/commons-logging-1.1.3.jar:/opt/hadoop-2.10.1/share/hadoop/common/lib/commons-codec-1.4.jar:/opt/hadoop-2.10.1/share/hadoop/common/lib/commons-io-2.4.jar:/opt/hadoop-2.10.1/share/hadoop/common/lib/commons-net-3.1.jar:/opt/hadoop-2.10.1/share/hadoop/common/hadoop-common-2.10.1.jar:/opt/hadoop-2.10.1/share/hadoop/common/hadoop-common-2.10.1-tests.jar:/opt/hadoop-2.10.1/share/hadoop/common/hadoop-nfs-2.10.1.jar:/opt/hadoop-2.10.1/share/hadoop/hdfs:/opt/hadoop-2.10.1/share/hadoop/hdfs/lib/commons-codec-1.4.jar:/opt/hadoop-2.10.1/share/hadoop/hdfs/lib/log4j-1.2.17.jar:/opt/hadoop-2.10.1/share/hadoop/hdfs/lib/commons-logging-1.1.3.jar:/opt/hadoop-2.10.1/share/hadoop/hdfs/lib/jsr305-3.0.2.jar:/opt/hadoop-2.10.1/share/hadoop/hdfs/lib/netty-3.10.6.Final.jar:/opt/hadoop-2.10.1/share/hadoop/hdfs/lib/guava-11.0.2.jar:/opt/hadoop-2.10.1/share/hadoop/hdfs/lib/commons-cli-1.2.jar:/opt/hadoop-2.10.1/share/hadoop/hdfs/lib/xmlenc-0.52.jar:/opt/hadoop-2.10.1/share/hadoop/hdfs/lib/commons-io-2.4.jar:/opt/hadoop-2.10.1/share/hadoop/hdfs/lib/servlet-api-2.5.jar:/opt/hadoop-2.10.1/share/hadoop/hdfs/lib/jetty-6.1.26.jar:/opt/hadoop-2.10.1/share/hadoop/hdfs/lib/jetty-util-6.1.26.jar:/opt/hadoop-2.10.1/share/hadoop/hdfs/lib/jersey-core-1.9.jar:/opt/hadoop-2.10.1/share/hadoop/hdfs/lib/jackson-core-asl-1.9.13.jar:/opt/hadoop-2.10.1/share/hadoop/hdfs/lib/jackson-mapper-asl-1.9.13.jar:/opt/hadoop-2.10.1/share/hadoop/hdfs/lib/jersey-server-1.9.jar:/opt/hadoop-2.10.1/share/hadoop/hdfs/lib/asm-3.2.jar:/opt/hadoop-2.10.1/share/hadoop/hdfs/lib/commons-lang-2.6.jar:/opt/hadoop-2.10.1/share/hadoop/hdfs/lib/protobuf-java-2.5.0.jar:/opt/hadoop-2.10.1/share/hadoop/hdfs/lib/htrace-core4-4.1.0-incubating.jar:/opt/hadoop-2.10.1/share/hadoop/hdfs/lib/hadoop-hdfs-client-2.10.1.jar:/opt/hadoop-2.10.1/share/hadoop/hdfs/lib/okhttp-2.7.5.jar:/opt/hadoop-2.10.1/share/hadoop/hdfs/lib/okio-1.6.0.jar:/opt/hadoop-2.10.1/share/hadoop/hdfs/lib/commons-daemon-1.0.13.jar:/opt/hadoop-2.10.1/share/hadoop/hdfs/lib/netty-all-4.1.50.Final.jar:/opt/hadoop-2.10.1/share/hadoop/hdfs/lib/xercesImpl-2.12.0.jar:/opt/hadoop-2.10.1/share/hadoop/hdfs/lib/xml-apis-1.4.01.jar:/opt/hadoop-2.10.1/share/hadoop/hdfs/lib/leveldbjni-all-1.8.jar:/opt/hadoop-2.10.1/share/hadoop/hdfs/lib/jackson-databind-2.9.10.6.jar:/opt/hadoop-2.10.1/share/hadoop/hdfs/lib/jackson-annotations-2.9.10.jar:/opt/hadoop-2.10.1/share/hadoop/hdfs/lib/jackson-core-2.9.10.jar:/opt/hadoop-2.10.1/share/hadoop/hdfs/hadoop-hdfs-2.10.1.jar:/opt/hadoop-2.10.1/share/hadoop/hdfs/hadoop-hdfs-2.10.1-tests.jar:/opt/hadoop-2.10.1/share/hadoop/hdfs/hadoop-hdfs-client-2.10.1.jar:/opt/hadoop-2.10.1/share/hadoop/hdfs/hadoop-hdfs-client-2.10.1-tests.jar:/opt/hadoop-2.10.1/share/hadoop/hdfs/hadoop-hdfs-native-client-2.10.1.jar:/opt/hadoop-2.10.1/share/hadoop/hdfs/hadoop-hdfs-native-client-2.10.1-tests.jar:/opt/hadoop-2.10.1/share/hadoop/hdfs/hadoop-hdfs-rbf-2.10.1.jar:/opt/hadoop-2.10.1/share/hadoop/hdfs/hadoop-hdfs-rbf-2.10.1-tests.jar:/opt/hadoop-2.10.1/share/hadoop/hdfs/hadoop-hdfs-nfs-2.10.1.jar:/opt/hadoop-2.10.1/share/hadoop/yarn:/opt/hadoop-2.10.1/share/hadoop/yarn/lib/commons-cli-1.2.jar:/opt/hadoop-2.10.1/share/hadoop/yarn/lib/log4j-1.2.17.jar:/opt/hadoop-2.10.1/share/hadoop/yarn/lib/commons-compress-1.19.jar:/opt/hadoop-2.10.1/share/hadoop/yarn/lib/servlet-api-2.5.jar:/opt/hadoop-2.10.1/share/hadoop/yarn/lib/commons-codec-1.4.jar:/opt/hadoop-2.10.1/share/hadoop/yarn/lib/jetty-util-6.1.26.jar:/opt/hadoop-2.10.1/share/hadoop/yarn/lib/jersey-core-1.9.jar:/opt/hadoop-2.10.1/share/hadoop/yarn/lib/jersey-client-1.9.jar:/opt/hadoop-2.10.1/share/hadoop/yarn/lib/jackson-core-asl-1.9.13.jar:/opt/hadoop-2.10.1/share/hadoop/yarn/lib/jackson-mapper-asl-1.9.13.jar:/opt/hadoop-2.10.1/share/hadoop/yarn/lib/jackson-jaxrs-1.9.13.jar:/opt/hadoop-2.10.1/share/hadoop/yarn/lib/jackson-xc-1.9.13.jar:/opt/hadoop-2.10.1/share/hadoop/yarn/lib/guice-servlet-3.0.jar:/opt/hadoop-2.10.1/share/hadoop/yarn/lib/guice-3.0.jar:/opt/hadoop-2.10.1/share/hadoop/yarn/lib/javax.inject-1.jar:/opt/hadoop-2.10.1/share/hadoop/yarn/lib/aopalliance-1.0.jar:/opt/hadoop-2.10.1/share/hadoop/yarn/lib/commons-io-2.4.jar:/opt/hadoop-2.10.1/share/hadoop/yarn/lib/jersey-server-1.9.jar:/opt/hadoop-2.10.1/share/hadoop/yarn/lib/asm-3.2.jar:/opt/hadoop-2.10.1/share/hadoop/yarn/lib/jersey-json-1.9.jar:/opt/hadoop-2.10.1/share/hadoop/yarn/lib/jettison-1.1.jar:/opt/hadoop-2.10.1/share/hadoop/yarn/lib/jaxb-impl-2.2.3-1.jar:/opt/hadoop-2.10.1/share/hadoop/yarn/lib/jersey-guice-1.9.jar:/opt/hadoop-2.10.1/share/hadoop/yarn/lib/commons-math3-3.1.1.jar:/opt/hadoop-2.10.1/share/hadoop/yarn/lib/xmlenc-0.52.jar:/opt/hadoop-2.10.1/share/hadoop/yarn/lib/httpclient-4.5.2.jar:/opt/hadoop-2.10.1/share/hadoop/yarn/lib/httpcore-4.4.4.jar:/opt/hadoop-2.10.1/share/hadoop/yarn/lib/commons-net-3.1.jar:/opt/hadoop-2.10.1/share/hadoop/yarn/lib/commons-collections-3.2.2.jar:/opt/hadoop-2.10.1/share/hadoop/yarn/lib/jetty-6.1.26.jar:/opt/hadoop-2.10.1/share/hadoop/yarn/lib/jetty-sslengine-6.1.26.jar:/opt/hadoop-2.10.1/share/hadoop/yarn/lib/jsp-api-2.1.jar:/opt/hadoop-2.10.1/share/hadoop/yarn/lib/jets3t-0.9.0.jar:/opt/hadoop-2.10.1/share/hadoop/yarn/lib/java-xmlbuilder-0.4.jar:/opt/hadoop-2.10.1/share/hadoop/yarn/lib/commons-configuration-1.6.jar:/opt/hadoop-2.10.1/share/hadoop/yarn/lib/commons-digester-1.8.jar:/opt/hadoop-2.10.1/share/hadoop/yarn/lib/commons-beanutils-1.9.4.jar:/opt/hadoop-2.10.1/share/hadoop/yarn/lib/commons-lang3-3.4.jar:/opt/hadoop-2.10.1/share/hadoop/yarn/lib/avro-1.7.7.jar:/opt/hadoop-2.10.1/share/hadoop/yarn/lib/paranamer-2.3.jar:/opt/hadoop-2.10.1/share/hadoop/yarn/lib/snappy-java-1.0.5.jar:/opt/hadoop-2.10.1/share/hadoop/yarn/lib/gson-2.2.4.jar:/opt/hadoop-2.10.1/share/hadoop/yarn/lib/nimbus-jose-jwt-7.9.jar:/opt/hadoop-2.10.1/share/hadoop/yarn/lib/jcip-annotations-1.0-1.jar:/opt/hadoop-2.10.1/share/hadoop/yarn/lib/json-smart-1.3.1.jar:/opt/hadoop-2.10.1/share/hadoop/yarn/lib/apacheds-kerberos-codec-2.0.0-M15.jar:/opt/hadoop-2.10.1/share/hadoop/yarn/lib/apacheds-i18n-2.0.0-M15.jar:/opt/hadoop-2.10.1/share/hadoop/yarn/lib/api-asn1-api-1.0.0-M20.jar:/opt/hadoop-2.10.1/share/hadoop/yarn/lib/api-util-1.0.0-M20.jar:/opt/hadoop-2.10.1/share/hadoop/yarn/lib/zookeeper-3.4.14.jar:/opt/hadoop-2.10.1/share/hadoop/yarn/lib/spotbugs-annotations-3.1.9.jar:/opt/hadoop-2.10.1/share/hadoop/yarn/lib/audience-annotations-0.5.0.jar:/opt/hadoop-2.10.1/share/hadoop/yarn/lib/netty-3.10.6.Final.jar:/opt/hadoop-2.10.1/share/hadoop/yarn/lib/curator-framework-2.13.0.jar:/opt/hadoop-2.10.1/share/hadoop/yarn/lib/curator-client-2.13.0.jar:/opt/hadoop-2.10.1/share/hadoop/yarn/lib/jsch-0.1.55.jar:/opt/hadoop-2.10.1/share/hadoop/yarn/lib/curator-recipes-2.13.0.jar:/opt/hadoop-2.10.1/share/hadoop/yarn/lib/htrace-core4-4.1.0-incubating.jar:/opt/hadoop-2.10.1/share/hadoop/yarn/lib/stax2-api-3.1.4.jar:/opt/hadoop-2.10.1/share/hadoop/yarn/lib/woodstox-core-5.0.3.jar:/opt/hadoop-2.10.1/share/hadoop/yarn/lib/leveldbjni-all-1.8.jar:/opt/hadoop-2.10.1/share/hadoop/yarn/lib/geronimo-jcache_1.0_spec-1.0-alpha-1.jar:/opt/hadoop-2.10.1/share/hadoop/yarn/lib/ehcache-3.3.1.jar:/opt/hadoop-2.10.1/share/hadoop/yarn/lib/HikariCP-java7-2.4.12.jar:/opt/hadoop-2.10.1/share/hadoop/yarn/lib/mssql-jdbc-6.2.1.jre7.jar:/opt/hadoop-2.10.1/share/hadoop/yarn/lib/metrics-core-3.0.1.jar:/opt/hadoop-2.10.1/share/hadoop/yarn/lib/fst-2.50.jar:/opt/hadoop-2.10.1/share/hadoop/yarn/lib/java-util-1.9.0.jar:/opt/hadoop-2.10.1/share/hadoop/yarn/lib/json-io-2.5.1.jar:/opt/hadoop-2.10.1/share/hadoop/yarn/lib/commons-lang-2.6.jar:/opt/hadoop-2.10.1/share/hadoop/yarn/lib/guava-11.0.2.jar:/opt/hadoop-2.10.1/share/hadoop/yarn/lib/jsr305-3.0.2.jar:/opt/hadoop-2.10.1/share/hadoop/yarn/lib/commons-logging-1.1.3.jar:/opt/hadoop-2.10.1/share/hadoop/yarn/lib/jaxb-api-2.2.2.jar:/opt/hadoop-2.10.1/share/hadoop/yarn/lib/stax-api-1.0-2.jar:/opt/hadoop-2.10.1/share/hadoop/yarn/lib/activation-1.1.jar:/opt/hadoop-2.10.1/share/hadoop/yarn/lib/protobuf-java-2.5.0.jar:/opt/hadoop-2.10.1/share/hadoop/yarn/hadoop-yarn-api-2.10.1.jar:/opt/hadoop-2.10.1/share/hadoop/yarn/hadoop-yarn-common-2.10.1.jar:/opt/hadoop-2.10.1/share/hadoop/yarn/hadoop-yarn-registry-2.10.1.jar:/opt/hadoop-2.10.1/share/hadoop/yarn/hadoop-yarn-server-common-2.10.1.jar:/opt/hadoop-2.10.1/share/hadoop/yarn/hadoop-yarn-server-nodemanager-2.10.1.jar:/opt/hadoop-2.10.1/share/hadoop/yarn/hadoop-yarn-server-web-proxy-2.10.1.jar:/opt/hadoop-2.10.1/share/hadoop/yarn/hadoop-yarn-server-applicationhistoryservice-2.10.1.jar:/opt/hadoop-2.10.1/share/hadoop/yarn/hadoop-yarn-server-resourcemanager-2.10.1.jar:/opt/hadoop-2.10.1/share/hadoop/yarn/hadoop-yarn-server-tests-2.10.1.jar:/opt/hadoop-2.10.1/share/hadoop/yarn/hadoop-yarn-client-2.10.1.jar:/opt/hadoop-2.10.1/share/hadoop/yarn/hadoop-yarn-server-sharedcachemanager-2.10.1.jar:/opt/hadoop-2.10.1/share/hadoop/yarn/hadoop-yarn-server-timeline-pluginstorage-2.10.1.jar:/opt/hadoop-2.10.1/share/hadoop/yarn/hadoop-yarn-server-router-2.10.1.jar:/opt/hadoop-2.10.1/share/hadoop/yarn/hadoop-yarn-applications-distributedshell-2.10.1.jar:/opt/hadoop-2.10.1/share/hadoop/yarn/hadoop-yarn-applications-unmanaged-am-launcher-2.10.1.jar:/opt/hadoop-2.10.1/share/hadoop/mapreduce/lib/protobuf-java-2.5.0.jar:/opt/hadoop-2.10.1/share/hadoop/mapreduce/lib/avro-1.7.7.jar:/opt/hadoop-2.10.1/share/hadoop/mapreduce/lib/jackson-core-asl-1.9.13.jar:/opt/hadoop-2.10.1/share/hadoop/mapreduce/lib/jackson-mapper-asl-1.9.13.jar:/opt/hadoop-2.10.1/share/hadoop/mapreduce/lib/paranamer-2.3.jar:/opt/hadoop-2.10.1/share/hadoop/mapreduce/lib/snappy-java-1.0.5.jar:/opt/hadoop-2.10.1/share/hadoop/mapreduce/lib/commons-compress-1.19.jar:/opt/hadoop-2.10.1/share/hadoop/mapreduce/lib/hadoop-annotations-2.10.1.jar:/opt/hadoop-2.10.1/share/hadoop/mapreduce/lib/commons-io-2.4.jar:/opt/hadoop-2.10.1/share/hadoop/mapreduce/lib/jersey-core-1.9.jar:/opt/hadoop-2.10.1/share/hadoop/mapreduce/lib/jersey-server-1.9.jar:/opt/hadoop-2.10.1/share/hadoop/mapreduce/lib/asm-3.2.jar:/opt/hadoop-2.10.1/share/hadoop/mapreduce/lib/log4j-1.2.17.jar:/opt/hadoop-2.10.1/share/hadoop/mapreduce/lib/netty-3.10.6.Final.jar:/opt/hadoop-2.10.1/share/hadoop/mapreduce/lib/leveldbjni-all-1.8.jar:/opt/hadoop-2.10.1/share/hadoop/mapreduce/lib/guice-3.0.jar:/opt/hadoop-2.10.1/share/hadoop/mapreduce/lib/javax.inject-1.jar:/opt/hadoop-2.10.1/share/hadoop/mapreduce/lib/aopalliance-1.0.jar:/opt/hadoop-2.10.1/share/hadoop/mapreduce/lib/jersey-guice-1.9.jar:/opt/hadoop-2.10.1/share/hadoop/mapreduce/lib/guice-servlet-3.0.jar:/opt/hadoop-2.10.1/share/hadoop/mapreduce/lib/junit-4.11.jar:/opt/hadoop-2.10.1/share/hadoop/mapreduce/lib/hamcrest-core-1.3.jar:/opt/hadoop-2.10.1/share/hadoop/mapreduce/hadoop-mapreduce-client-core-2.10.1.jar:/opt/hadoop-2.10.1/share/hadoop/mapreduce/hadoop-mapreduce-client-common-2.10.1.jar:/opt/hadoop-2.10.1/share/hadoop/mapreduce/hadoop-mapreduce-client-shuffle-2.10.1.jar:/opt/hadoop-2.10.1/share/hadoop/mapreduce/hadoop-mapreduce-client-app-2.10.1.jar:/opt/hadoop-2.10.1/share/hadoop/mapreduce/hadoop-mapreduce-client-hs-2.10.1.jar:/opt/hadoop-2.10.1/share/hadoop/mapreduce/hadoop-mapreduce-client-jobclient-2.10.1.jar:/opt/hadoop-2.10.1/share/hadoop/mapreduce/hadoop-mapreduce-client-hs-plugins-2.10.1.jar:/opt/hadoop-2.10.1/share/hadoop/mapreduce/hadoop-mapreduce-examples-2.10.1.jar:/opt/hadoop-2.10.1/share/hadoop/mapreduce/hadoop-mapreduce-client-jobclient-2.10.1-tests.jar:/contrib/capacity-scheduler/*.jar
    STARTUP_MSG:   build = https://github.com/apache/hadoop -r 1827467c9a56f133025f28557bfc2c562d78e816; compiled by 'centos' on 2020-09-14T13:17Z
    STARTUP_MSG:   java = 1.8.0_282
    ************************************************************/
    21/02/21 12:51:13 INFO tools.DFSZKFailoverController: registered UNIX signal handlers for [TERM, HUP, INT]
    21/02/21 12:51:13 INFO tools.DFSZKFailoverController: Failover controller configured for NameNode NameNode at peter-kafka001/192.168.126.71:8020
    21/02/21 12:51:13 INFO zookeeper.ZooKeeper: Client environment:zookeeper.version=3.4.14-4c25d480e66aadd371de8bd2fd8da255ac140bcf, built on 03/06/2019 16:18 GMT
    21/02/21 12:51:13 INFO zookeeper.ZooKeeper: Client environment:host.name=peter-kafka001
    21/02/21 12:51:13 INFO zookeeper.ZooKeeper: Client environment:java.version=1.8.0_282
    21/02/21 12:51:13 INFO zookeeper.ZooKeeper: Client environment:java.vendor=Red Hat, Inc.
    21/02/21 12:51:13 INFO zookeeper.ZooKeeper: Client environment:java.home=/usr/lib/jvm/java-1.8.0-openjdk-1.8.0.282.b08-1.el7_9.x86_64/jre
    21/02/21 12:51:13 INFO zookeeper.ZooKeeper: Client environment:java.class.path=/opt/hadoop-2.10.1/etc/hadoop:/opt/hadoop-2.10.1/share/hadoop/common/lib/commons-collections-3.2.2.jar:/opt/hadoop-2.10.1/share/hadoop/common/lib/servlet-api-2.5.jar:/opt/hadoop-2.10.1/share/hadoop/common/lib/jetty-6.1.26.jar:/opt/hadoop-2.10.1/share/hadoop/common/lib/jetty-util-6.1.26.jar:/opt/hadoop-2.10.1/share/hadoop/common/lib/jetty-sslengine-6.1.26.jar:/opt/hadoop-2.10.1/share/hadoop/common/lib/jsp-api-2.1.jar:/opt/hadoop-2.10.1/share/hadoop/common/lib/jersey-core-1.9.jar:/opt/hadoop-2.10.1/share/hadoop/common/lib/jersey-json-1.9.jar:/opt/hadoop-2.10.1/share/hadoop/common/lib/jettison-1.1.jar:/opt/hadoop-2.10.1/share/hadoop/common/lib/jaxb-impl-2.2.3-1.jar:/opt/hadoop-2.10.1/share/hadoop/common/lib/jaxb-api-2.2.2.jar:/opt/hadoop-2.10.1/share/hadoop/common/lib/stax-api-1.0-2.jar:/opt/hadoop-2.10.1/share/hadoop/common/lib/activation-1.1.jar:/opt/hadoop-2.10.1/share/hadoop/common/lib/jackson-core-asl-1.9.13.jar:/opt/hadoop-2.10.1/share/hadoop/common/lib/jackson-mapper-asl-1.9.13.jar:/opt/hadoop-2.10.1/share/hadoop/common/lib/jackson-jaxrs-1.9.13.jar:/opt/hadoop-2.10.1/share/hadoop/common/lib/jackson-xc-1.9.13.jar:/opt/hadoop-2.10.1/share/hadoop/common/lib/jersey-server-1.9.jar:/opt/hadoop-2.10.1/share/hadoop/common/lib/asm-3.2.jar:/opt/hadoop-2.10.1/share/hadoop/common/lib/log4j-1.2.17.jar:/opt/hadoop-2.10.1/share/hadoop/common/lib/jets3t-0.9.0.jar:/opt/hadoop-2.10.1/share/hadoop/common/lib/java-xmlbuilder-0.4.jar:/opt/hadoop-2.10.1/share/hadoop/common/lib/commons-lang-2.6.jar:/opt/hadoop-2.10.1/share/hadoop/common/lib/commons-configuration-1.6.jar:/opt/hadoop-2.10.1/share/hadoop/common/lib/commons-digester-1.8.jar:/opt/hadoop-2.10.1/share/hadoop/common/lib/commons-beanutils-1.9.4.jar:/opt/hadoop-2.10.1/share/hadoop/common/lib/commons-lang3-3.4.jar:/opt/hadoop-2.10.1/share/hadoop/common/lib/slf4j-api-1.7.25.jar:/opt/hadoop-2.10.1/share/hadoop/common/lib/slf4j-log4j12-1.7.25.jar:/opt/hadoop-2.10.1/share/hadoop/common/lib/avro-1.7.7.jar:/opt/hadoop-2.10.1/share/hadoop/common/lib/paranamer-2.3.jar:/opt/hadoop-2.10.1/share/hadoop/common/lib/snappy-java-1.0.5.jar:/opt/hadoop-2.10.1/share/hadoop/common/lib/commons-compress-1.19.jar:/opt/hadoop-2.10.1/share/hadoop/common/lib/protobuf-java-2.5.0.jar:/opt/hadoop-2.10.1/share/hadoop/common/lib/gson-2.2.4.jar:/opt/hadoop-2.10.1/share/hadoop/common/lib/hadoop-auth-2.10.1.jar:/opt/hadoop-2.10.1/share/hadoop/common/lib/nimbus-jose-jwt-7.9.jar:/opt/hadoop-2.10.1/share/hadoop/common/lib/jcip-annotations-1.0-1.jar:/opt/hadoop-2.10.1/share/hadoop/common/lib/json-smart-1.3.1.jar:/opt/hadoop-2.10.1/share/hadoop/common/lib/apacheds-kerberos-codec-2.0.0-M15.jar:/opt/hadoop-2.10.1/share/hadoop/common/lib/apacheds-i18n-2.0.0-M15.jar:/opt/hadoop-2.10.1/share/hadoop/common/lib/api-asn1-api-1.0.0-M20.jar:/opt/hadoop-2.10.1/share/hadoop/common/lib/api-util-1.0.0-M20.jar:/opt/hadoop-2.10.1/share/hadoop/common/lib/zookeeper-3.4.14.jar:/opt/hadoop-2.10.1/share/hadoop/common/lib/spotbugs-annotations-3.1.9.jar:/opt/hadoop-2.10.1/share/hadoop/common/lib/audience-annotations-0.5.0.jar:/opt/hadoop-2.10.1/share/hadoop/common/lib/netty-3.10.6.Final.jar:/opt/hadoop-2.10.1/share/hadoop/common/lib/curator-framework-2.13.0.jar:/opt/hadoop-2.10.1/share/hadoop/common/lib/curator-client-2.13.0.jar:/opt/hadoop-2.10.1/share/hadoop/common/lib/jsch-0.1.55.jar:/opt/hadoop-2.10.1/share/hadoop/common/lib/curator-recipes-2.13.0.jar:/opt/hadoop-2.10.1/share/hadoop/common/lib/htrace-core4-4.1.0-incubating.jar:/opt/hadoop-2.10.1/share/hadoop/common/lib/stax2-api-3.1.4.jar:/opt/hadoop-2.10.1/share/hadoop/common/lib/woodstox-core-5.0.3.jar:/opt/hadoop-2.10.1/share/hadoop/common/lib/junit-4.11.jar:/opt/hadoop-2.10.1/share/hadoop/common/lib/hamcrest-core-1.3.jar:/opt/hadoop-2.10.1/share/hadoop/common/lib/mockito-all-1.8.5.jar:/opt/hadoop-2.10.1/share/hadoop/common/lib/hadoop-annotations-2.10.1.jar:/opt/hadoop-2.10.1/share/hadoop/common/lib/guava-11.0.2.jar:/opt/hadoop-2.10.1/share/hadoop/common/lib/jsr305-3.0.2.jar:/opt/hadoop-2.10.1/share/hadoop/common/lib/commons-cli-1.2.jar:/opt/hadoop-2.10.1/share/hadoop/common/lib/commons-math3-3.1.1.jar:/opt/hadoop-2.10.1/share/hadoop/common/lib/xmlenc-0.52.jar:/opt/hadoop-2.10.1/share/hadoop/common/lib/httpclient-4.5.2.jar:/opt/hadoop-2.10.1/share/hadoop/common/lib/httpcore-4.4.4.jar:/opt/hadoop-2.10.1/share/hadoop/common/lib/commons-logging-1.1.3.jar:/opt/hadoop-2.10.1/share/hadoop/common/lib/commons-codec-1.4.jar:/opt/hadoop-2.10.1/share/hadoop/common/lib/commons-io-2.4.jar:/opt/hadoop-2.10.1/share/hadoop/common/lib/commons-net-3.1.jar:/opt/hadoop-2.10.1/share/hadoop/common/hadoop-common-2.10.1.jar:/opt/hadoop-2.10.1/share/hadoop/common/hadoop-common-2.10.1-tests.jar:/opt/hadoop-2.10.1/share/hadoop/common/hadoop-nfs-2.10.1.jar:/opt/hadoop-2.10.1/share/hadoop/hdfs:/opt/hadoop-2.10.1/share/hadoop/hdfs/lib/commons-codec-1.4.jar:/opt/hadoop-2.10.1/share/hadoop/hdfs/lib/log4j-1.2.17.jar:/opt/hadoop-2.10.1/share/hadoop/hdfs/lib/commons-logging-1.1.3.jar:/opt/hadoop-2.10.1/share/hadoop/hdfs/lib/jsr305-3.0.2.jar:/opt/hadoop-2.10.1/share/hadoop/hdfs/lib/netty-3.10.6.Final.jar:/opt/hadoop-2.10.1/share/hadoop/hdfs/lib/guava-11.0.2.jar:/opt/hadoop-2.10.1/share/hadoop/hdfs/lib/commons-cli-1.2.jar:/opt/hadoop-2.10.1/share/hadoop/hdfs/lib/xmlenc-0.52.jar:/opt/hadoop-2.10.1/share/hadoop/hdfs/lib/commons-io-2.4.jar:/opt/hadoop-2.10.1/share/hadoop/hdfs/lib/servlet-api-2.5.jar:/opt/hadoop-2.10.1/share/hadoop/hdfs/lib/jetty-6.1.26.jar:/opt/hadoop-2.10.1/share/hadoop/hdfs/lib/jetty-util-6.1.26.jar:/opt/hadoop-2.10.1/share/hadoop/hdfs/lib/jersey-core-1.9.jar:/opt/hadoop-2.10.1/share/hadoop/hdfs/lib/jackson-core-asl-1.9.13.jar:/opt/hadoop-2.10.1/share/hadoop/hdfs/lib/jackson-mapper-asl-1.9.13.jar:/opt/hadoop-2.10.1/share/hadoop/hdfs/lib/jersey-server-1.9.jar:/opt/hadoop-2.10.1/share/hadoop/hdfs/lib/asm-3.2.jar:/opt/hadoop-2.10.1/share/hadoop/hdfs/lib/commons-lang-2.6.jar:/opt/hadoop-2.10.1/share/hadoop/hdfs/lib/protobuf-java-2.5.0.jar:/opt/hadoop-2.10.1/share/hadoop/hdfs/lib/htrace-core4-4.1.0-incubating.jar:/opt/hadoop-2.10.1/share/hadoop/hdfs/lib/hadoop-hdfs-client-2.10.1.jar:/opt/hadoop-2.10.1/share/hadoop/hdfs/lib/okhttp-2.7.5.jar:/opt/hadoop-2.10.1/share/hadoop/hdfs/lib/okio-1.6.0.jar:/opt/hadoop-2.10.1/share/hadoop/hdfs/lib/commons-daemon-1.0.13.jar:/opt/hadoop-2.10.1/share/hadoop/hdfs/lib/netty-all-4.1.50.Final.jar:/opt/hadoop-2.10.1/share/hadoop/hdfs/lib/xercesImpl-2.12.0.jar:/opt/hadoop-2.10.1/share/hadoop/hdfs/lib/xml-apis-1.4.01.jar:/opt/hadoop-2.10.1/share/hadoop/hdfs/lib/leveldbjni-all-1.8.jar:/opt/hadoop-2.10.1/share/hadoop/hdfs/lib/jackson-databind-2.9.10.6.jar:/opt/hadoop-2.10.1/share/hadoop/hdfs/lib/jackson-annotations-2.9.10.jar:/opt/hadoop-2.10.1/share/hadoop/hdfs/lib/jackson-core-2.9.10.jar:/opt/hadoop-2.10.1/share/hadoop/hdfs/hadoop-hdfs-2.10.1.jar:/opt/hadoop-2.10.1/share/hadoop/hdfs/hadoop-hdfs-2.10.1-tests.jar:/opt/hadoop-2.10.1/share/hadoop/hdfs/hadoop-hdfs-client-2.10.1.jar:/opt/hadoop-2.10.1/share/hadoop/hdfs/hadoop-hdfs-client-2.10.1-tests.jar:/opt/hadoop-2.10.1/share/hadoop/hdfs/hadoop-hdfs-native-client-2.10.1.jar:/opt/hadoop-2.10.1/share/hadoop/hdfs/hadoop-hdfs-native-client-2.10.1-tests.jar:/opt/hadoop-2.10.1/share/hadoop/hdfs/hadoop-hdfs-rbf-2.10.1.jar:/opt/hadoop-2.10.1/share/hadoop/hdfs/hadoop-hdfs-rbf-2.10.1-tests.jar:/opt/hadoop-2.10.1/share/hadoop/hdfs/hadoop-hdfs-nfs-2.10.1.jar:/opt/hadoop-2.10.1/share/hadoop/yarn:/opt/hadoop-2.10.1/share/hadoop/yarn/lib/commons-cli-1.2.jar:/opt/hadoop-2.10.1/share/hadoop/yarn/lib/log4j-1.2.17.jar:/opt/hadoop-2.10.1/share/hadoop/yarn/lib/commons-compress-1.19.jar:/opt/hadoop-2.10.1/share/hadoop/yarn/lib/servlet-api-2.5.jar:/opt/hadoop-2.10.1/share/hadoop/yarn/lib/commons-codec-1.4.jar:/opt/hadoop-2.10.1/share/hadoop/yarn/lib/jetty-util-6.1.26.jar:/opt/hadoop-2.10.1/share/hadoop/yarn/lib/jersey-core-1.9.jar:/opt/hadoop-2.10.1/share/hadoop/yarn/lib/jersey-client-1.9.jar:/opt/hadoop-2.10.1/share/hadoop/yarn/lib/jackson-core-asl-1.9.13.jar:/opt/hadoop-2.10.1/share/hadoop/yarn/lib/jackson-mapper-asl-1.9.13.jar:/opt/hadoop-2.10.1/share/hadoop/yarn/lib/jackson-jaxrs-1.9.13.jar:/opt/hadoop-2.10.1/share/hadoop/yarn/lib/jackson-xc-1.9.13.jar:/opt/hadoop-2.10.1/share/hadoop/yarn/lib/guice-servlet-3.0.jar:/opt/hadoop-2.10.1/share/hadoop/yarn/lib/guice-3.0.jar:/opt/hadoop-2.10.1/share/hadoop/yarn/lib/javax.inject-1.jar:/opt/hadoop-2.10.1/share/hadoop/yarn/lib/aopalliance-1.0.jar:/opt/hadoop-2.10.1/share/hadoop/yarn/lib/commons-io-2.4.jar:/opt/hadoop-2.10.1/share/hadoop/yarn/lib/jersey-server-1.9.jar:/opt/hadoop-2.10.1/share/hadoop/yarn/lib/asm-3.2.jar:/opt/hadoop-2.10.1/share/hadoop/yarn/lib/jersey-json-1.9.jar:/opt/hadoop-2.10.1/share/hadoop/yarn/lib/jettison-1.1.jar:/opt/hadoop-2.10.1/share/hadoop/yarn/lib/jaxb-impl-2.2.3-1.jar:/opt/hadoop-2.10.1/share/hadoop/yarn/lib/jersey-guice-1.9.jar:/opt/hadoop-2.10.1/share/hadoop/yarn/lib/commons-math3-3.1.1.jar:/opt/hadoop-2.10.1/share/hadoop/yarn/lib/xmlenc-0.52.jar:/opt/hadoop-2.10.1/share/hadoop/yarn/lib/httpclient-4.5.2.jar:/opt/hadoop-2.10.1/share/hadoop/yarn/lib/httpcore-4.4.4.jar:/opt/hadoop-2.10.1/share/hadoop/yarn/lib/commons-net-3.1.jar:/opt/hadoop-2.10.1/share/hadoop/yarn/lib/commons-collections-3.2.2.jar:/opt/hadoop-2.10.1/share/hadoop/yarn/lib/jetty-6.1.26.jar:/opt/hadoop-2.10.1/share/hadoop/yarn/lib/jetty-sslengine-6.1.26.jar:/opt/hadoop-2.10.1/share/hadoop/yarn/lib/jsp-api-2.1.jar:/opt/hadoop-2.10.1/share/hadoop/yarn/lib/jets3t-0.9.0.jar:/opt/hadoop-2.10.1/share/hadoop/yarn/lib/java-xmlbuilder-0.4.jar:/opt/hadoop-2.10.1/share/hadoop/yarn/lib/commons-configuration-1.6.jar:/opt/hadoop-2.10.1/share/hadoop/yarn/lib/commons-digester-1.8.jar:/opt/hadoop-2.10.1/share/hadoop/yarn/lib/commons-beanutils-1.9.4.jar:/opt/hadoop-2.10.1/share/hadoop/yarn/lib/commons-lang3-3.4.jar:/opt/hadoop-2.10.1/share/hadoop/yarn/lib/avro-1.7.7.jar:/opt/hadoop-2.10.1/share/hadoop/yarn/lib/paranamer-2.3.jar:/opt/hadoop-2.10.1/share/hadoop/yarn/lib/snappy-java-1.0.5.jar:/opt/hadoop-2.10.1/share/hadoop/yarn/lib/gson-2.2.4.jar:/opt/hadoop-2.10.1/share/hadoop/yarn/lib/nimbus-jose-jwt-7.9.jar:/opt/hadoop-2.10.1/share/hadoop/yarn/lib/jcip-annotations-1.0-1.jar:/opt/hadoop-2.10.1/share/hadoop/yarn/lib/json-smart-1.3.1.jar:/opt/hadoop-2.10.1/share/hadoop/yarn/lib/apacheds-kerberos-codec-2.0.0-M15.jar:/opt/hadoop-2.10.1/share/hadoop/yarn/lib/apacheds-i18n-2.0.0-M15.jar:/opt/hadoop-2.10.1/share/hadoop/yarn/lib/api-asn1-api-1.0.0-M20.jar:/opt/hadoop-2.10.1/share/hadoop/yarn/lib/api-util-1.0.0-M20.jar:/opt/hadoop-2.10.1/share/hadoop/yarn/lib/zookeeper-3.4.14.jar:/opt/hadoop-2.10.1/share/hadoop/yarn/lib/spotbugs-annotations-3.1.9.jar:/opt/hadoop-2.10.1/share/hadoop/yarn/lib/audience-annotations-0.5.0.jar:/opt/hadoop-2.10.1/share/hadoop/yarn/lib/netty-3.10.6.Final.jar:/opt/hadoop-2.10.1/share/hadoop/yarn/lib/curator-framework-2.13.0.jar:/opt/hadoop-2.10.1/share/hadoop/yarn/lib/curator-client-2.13.0.jar:/opt/hadoop-2.10.1/share/hadoop/yarn/lib/jsch-0.1.55.jar:/opt/hadoop-2.10.1/share/hadoop/yarn/lib/curator-recipes-2.13.0.jar:/opt/hadoop-2.10.1/share/hadoop/yarn/lib/htrace-core4-4.1.0-incubating.jar:/opt/hadoop-2.10.1/share/hadoop/yarn/lib/stax2-api-3.1.4.jar:/opt/hadoop-2.10.1/share/hadoop/yarn/lib/woodstox-core-5.0.3.jar:/opt/hadoop-2.10.1/share/hadoop/yarn/lib/leveldbjni-all-1.8.jar:/opt/hadoop-2.10.1/share/hadoop/yarn/lib/geronimo-jcache_1.0_spec-1.0-alpha-1.jar:/opt/hadoop-2.10.1/share/hadoop/yarn/lib/ehcache-3.3.1.jar:/opt/hadoop-2.10.1/share/hadoop/yarn/lib/HikariCP-java7-2.4.12.jar:/opt/hadoop-2.10.1/share/hadoop/yarn/lib/mssql-jdbc-6.2.1.jre7.jar:/opt/hadoop-2.10.1/share/hadoop/yarn/lib/metrics-core-3.0.1.jar:/opt/hadoop-2.10.1/share/hadoop/yarn/lib/fst-2.50.jar:/opt/hadoop-2.10.1/share/hadoop/yarn/lib/java-util-1.9.0.jar:/opt/hadoop-2.10.1/share/hadoop/yarn/lib/json-io-2.5.1.jar:/opt/hadoop-2.10.1/share/hadoop/yarn/lib/commons-lang-2.6.jar:/opt/hadoop-2.10.1/share/hadoop/yarn/lib/guava-11.0.2.jar:/opt/hadoop-2.10.1/share/hadoop/yarn/lib/jsr305-3.0.2.jar:/opt/hadoop-2.10.1/share/hadoop/yarn/lib/commons-logging-1.1.3.jar:/opt/hadoop-2.10.1/share/hadoop/yarn/lib/jaxb-api-2.2.2.jar:/opt/hadoop-2.10.1/share/hadoop/yarn/lib/stax-api-1.0-2.jar:/opt/hadoop-2.10.1/share/hadoop/yarn/lib/activation-1.1.jar:/opt/hadoop-2.10.1/share/hadoop/yarn/lib/protobuf-java-2.5.0.jar:/opt/hadoop-2.10.1/share/hadoop/yarn/hadoop-yarn-api-2.10.1.jar:/opt/hadoop-2.10.1/share/hadoop/yarn/hadoop-yarn-common-2.10.1.jar:/opt/hadoop-2.10.1/share/hadoop/yarn/hadoop-yarn-registry-2.10.1.jar:/opt/hadoop-2.10.1/share/hadoop/yarn/hadoop-yarn-server-common-2.10.1.jar:/opt/hadoop-2.10.1/share/hadoop/yarn/hadoop-yarn-server-nodemanager-2.10.1.jar:/opt/hadoop-2.10.1/share/hadoop/yarn/hadoop-yarn-server-web-proxy-2.10.1.jar:/opt/hadoop-2.10.1/share/hadoop/yarn/hadoop-yarn-server-applicationhistoryservice-2.10.1.jar:/opt/hadoop-2.10.1/share/hadoop/yarn/hadoop-yarn-server-resourcemanager-2.10.1.jar:/opt/hadoop-2.10.1/share/hadoop/yarn/hadoop-yarn-server-tests-2.10.1.jar:/opt/hadoop-2.10.1/share/hadoop/yarn/hadoop-yarn-client-2.10.1.jar:/opt/hadoop-2.10.1/share/hadoop/yarn/hadoop-yarn-server-sharedcachemanager-2.10.1.jar:/opt/hadoop-2.10.1/share/hadoop/yarn/hadoop-yarn-server-timeline-pluginstorage-2.10.1.jar:/opt/hadoop-2.10.1/share/hadoop/yarn/hadoop-yarn-server-router-2.10.1.jar:/opt/hadoop-2.10.1/share/hadoop/yarn/hadoop-yarn-applications-distributedshell-2.10.1.jar:/opt/hadoop-2.10.1/share/hadoop/yarn/hadoop-yarn-applications-unmanaged-am-launcher-2.10.1.jar:/opt/hadoop-2.10.1/share/hadoop/mapreduce/lib/protobuf-java-2.5.0.jar:/opt/hadoop-2.10.1/share/hadoop/mapreduce/lib/avro-1.7.7.jar:/opt/hadoop-2.10.1/share/hadoop/mapreduce/lib/jackson-core-asl-1.9.13.jar:/opt/hadoop-2.10.1/share/hadoop/mapreduce/lib/jackson-mapper-asl-1.9.13.jar:/opt/hadoop-2.10.1/share/hadoop/mapreduce/lib/paranamer-2.3.jar:/opt/hadoop-2.10.1/share/hadoop/mapreduce/lib/snappy-java-1.0.5.jar:/opt/hadoop-2.10.1/share/hadoop/mapreduce/lib/commons-compress-1.19.jar:/opt/hadoop-2.10.1/share/hadoop/mapreduce/lib/hadoop-annotations-2.10.1.jar:/opt/hadoop-2.10.1/share/hadoop/mapreduce/lib/commons-io-2.4.jar:/opt/hadoop-2.10.1/share/hadoop/mapreduce/lib/jersey-core-1.9.jar:/opt/hadoop-2.10.1/share/hadoop/mapreduce/lib/jersey-server-1.9.jar:/opt/hadoop-2.10.1/share/hadoop/mapreduce/lib/asm-3.2.jar:/opt/hadoop-2.10.1/share/hadoop/mapreduce/lib/log4j-1.2.17.jar:/opt/hadoop-2.10.1/share/hadoop/mapreduce/lib/netty-3.10.6.Final.jar:/opt/hadoop-2.10.1/share/hadoop/mapreduce/lib/leveldbjni-all-1.8.jar:/opt/hadoop-2.10.1/share/hadoop/mapreduce/lib/guice-3.0.jar:/opt/hadoop-2.10.1/share/hadoop/mapreduce/lib/javax.inject-1.jar:/opt/hadoop-2.10.1/share/hadoop/mapreduce/lib/aopalliance-1.0.jar:/opt/hadoop-2.10.1/share/hadoop/mapreduce/lib/jersey-guice-1.9.jar:/opt/hadoop-2.10.1/share/hadoop/mapreduce/lib/guice-servlet-3.0.jar:/opt/hadoop-2.10.1/share/hadoop/mapreduce/lib/junit-4.11.jar:/opt/hadoop-2.10.1/share/hadoop/mapreduce/lib/hamcrest-core-1.3.jar:/opt/hadoop-2.10.1/share/hadoop/mapreduce/hadoop-mapreduce-client-core-2.10.1.jar:/opt/hadoop-2.10.1/share/hadoop/mapreduce/hadoop-mapreduce-client-common-2.10.1.jar:/opt/hadoop-2.10.1/share/hadoop/mapreduce/hadoop-mapreduce-client-shuffle-2.10.1.jar:/opt/hadoop-2.10.1/share/hadoop/mapreduce/hadoop-mapreduce-client-app-2.10.1.jar:/opt/hadoop-2.10.1/share/hadoop/mapreduce/hadoop-mapreduce-client-hs-2.10.1.jar:/opt/hadoop-2.10.1/share/hadoop/mapreduce/hadoop-mapreduce-client-jobclient-2.10.1.jar:/opt/hadoop-2.10.1/share/hadoop/mapreduce/hadoop-mapreduce-client-hs-plugins-2.10.1.jar:/opt/hadoop-2.10.1/share/hadoop/mapreduce/hadoop-mapreduce-examples-2.10.1.jar:/opt/hadoop-2.10.1/share/hadoop/mapreduce/hadoop-mapreduce-client-jobclient-2.10.1-tests.jar:/contrib/capacity-scheduler/*.jar
    21/02/21 12:51:13 INFO zookeeper.ZooKeeper: Client environment:java.library.path=/opt/hadoop-2.10.1/lib/native
    21/02/21 12:51:13 INFO zookeeper.ZooKeeper: Client environment:java.io.tmpdir=/tmp
    21/02/21 12:51:13 INFO zookeeper.ZooKeeper: Client environment:java.compiler=<NA>
    21/02/21 12:51:13 INFO zookeeper.ZooKeeper: Client environment:os.name=Linux
    21/02/21 12:51:13 INFO zookeeper.ZooKeeper: Client environment:os.arch=amd64
    21/02/21 12:51:13 INFO zookeeper.ZooKeeper: Client environment:os.version=3.10.0-1160.el7.x86_64
    21/02/21 12:51:13 INFO zookeeper.ZooKeeper: Client environment:user.name=hdfs
    21/02/21 12:51:13 INFO zookeeper.ZooKeeper: Client environment:user.home=/home/hdfs
    21/02/21 12:51:13 INFO zookeeper.ZooKeeper: Client environment:user.dir=/home/hdfs
    21/02/21 12:51:13 INFO zookeeper.ZooKeeper: Initiating client connection, connectString=peter-zk001:2181,peter-zk002:2181,peter-zk003:2181 sessionTimeout=300000 watcher=org.apache.hadoop.ha.ActiveStandbyElector$WatcherWithClientRef@7c729a55
    21/02/21 12:51:13 INFO zookeeper.ClientCnxn: Opening socket connection to server peter-zk001/192.168.126.71:2181. Will not attempt to authenticate using SASL (unknown error)
    21/02/21 12:51:13 INFO zookeeper.ClientCnxn: Socket connection established to peter-zk001/192.168.126.71:2181, initiating session
    21/02/21 12:51:13 INFO zookeeper.ClientCnxn: Session establishment complete on server peter-zk001/192.168.126.71:2181, sessionid = 0x100000027490002, negotiated timeout = 40000
    21/02/21 12:51:13 INFO ha.ActiveStandbyElector: Session connected.
    21/02/21 12:51:13 INFO ha.ActiveStandbyElector: Successfully created /hadoop-ha/peter-cluster in ZK.
    21/02/21 12:51:13 INFO zookeeper.ZooKeeper: Session: 0x100000027490002 closed
    21/02/21 12:51:13 INFO zookeeper.ClientCnxn: EventThread shut down for session: 0x100000027490002
    21/02/21 12:51:13 INFO tools.DFSZKFailoverController: SHUTDOWN_MSG: 
    /************************************************************
    SHUTDOWN_MSG: Shutting down DFSZKFailoverController at peter-kafka001/192.168.126.71
    ************************************************************/
    [hdfs@peter-kafka001 ~]$

    2. 설정 확인
    $ /opt/zookeeper/bin/zkCli.sh
    [zk: localhost:2181(CONNECTED) 0] ls /
    [hadoop-ha, kafka, zookeeper]
    [zk: localhost:2181(CONNECTED) 1] ls /hadoop-ha
    [peter-cluster]
    [zk: localhost:2181(CONNECTED) 2] 

    3. 실행
@@@ vi /etc/profile.d/hadoop.sh 설정
    # su - yarn
    $ cd /opt/hadoop
    $ sbin/hadoop-daemon.sh start journalnode
    $ jps

    [yarn@peter-kafka001 ~]$ cd /opt/hadoop
    [yarn@peter-kafka001 hadoop]$ sbin/hadoop-daemon.sh start journalnode
    starting journalnode, logging to /opt/hadoop-2.10.1/logs/hadoop-yarn-journalnode-peter-kafka001.out
    [yarn@peter-kafka001 hadoop]$ jps
    5292 JournalNode            # <--- 실행후 추가됨.
    5358 Jps

    [yarn@peter-kafka002 ~]$ cd /opt/hadoop
    [yarn@peter-kafka002 hadoop]$ sbin/hadoop-daemon.sh start journalnode
    starting journalnode, logging to /opt/hadoop-2.10.1/logs/hadoop-yarn-journalnode-peter-kafka002.out
    [yarn@peter-kafka002 hadoop]$ jps
    53824 Jps
    53768 JournalNode            # <--- 실행후 추가됨.

    [yarn@peter-kafka003 ~]$ cd /opt/hadoop
    [yarn@peter-kafka003 hadoop]$ sbin/hadoop-daemon.sh start journalnode
    starting journalnode, logging to /opt/hadoop-2.10.1/logs/hadoop-yarn-journalnode-peter-kafka003.out
    [yarn@peter-kafka003 hadoop]$ jps
    55857 Jps
    55801 JournalNode            # <--- 실행후 추가됨.

    4. [참고] JournalNode 중지. (Namenode format을 위해서는 중지 하면 안됨)
    # su - yarn
    $ cd /opt/hadoop
    $ sbin/hadoop-daemon.sh stop journalnode
    % ssh yarn@peter-kafka002 "/opt/hadoop/sbin/hadoop-daemon.sh stop journalnode"
    % ssh yarn@peter-kafka003 "/opt/hadoop/sbin/hadoop-daemon.sh stop journalnode"


o NameNode
    1. NameNode 포맷 : in peter-kafka001(journalnode 가 실행된 상태에서 실행해야함)
    # su - hdfs
    $ cd /opt/hadoop
    $ bin/hdfs namenode -format

    [hdfs@peter-kafka001 hadoop]$ bin/hdfs namenode -format
    21/02/21 13:35:52 INFO namenode.NameNode: STARTUP_MSG: 
    /************************************************************
    STARTUP_MSG: Starting NameNode
    STARTUP_MSG:   host = peter-kafka001/192.168.126.71
    STARTUP_MSG:   args = [-format]
    STARTUP_MSG:   version = 2.10.1
    STARTUP_MSG:   classpath = /opt/hadoop/etc/hadoop:/opt/hadoop-2.10.1/share/hadoop/common/lib/commons-collections-3.2.2.jar:/opt/hadoop-2.10.1/share/hadoop/common/lib/servlet-api-2.5.jar:/opt/hadoop-2.10.1/share/hadoop/common/lib/jetty-6.1.26.jar:/opt/hadoop-2.10.1/share/hadoop/common/lib/jetty-util-6.1.26.jar:/opt/hadoop-2.10.1/share/hadoop/common/lib/jetty-sslengine-6.1.26.jar:/opt/hadoop-2.10.1/share/hadoop/common/lib/jsp-api-2.1.jar:/opt/hadoop-2.10.1/share/hadoop/common/lib/jersey-core-1.9.jar:/opt/hadoop-2.10.1/share/hadoop/common/lib/jersey-json-1.9.jar:/opt/hadoop-2.10.1/share/hadoop/common/lib/jettison-1.1.jar:/opt/hadoop-2.10.1/share/hadoop/common/lib/jaxb-impl-2.2.3-1.jar:/opt/hadoop-2.10.1/share/hadoop/common/lib/jaxb-api-2.2.2.jar:/opt/hadoop-2.10.1/share/hadoop/common/lib/stax-api-1.0-2.jar:/opt/hadoop-2.10.1/share/hadoop/common/lib/activation-1.1.jar:/opt/hadoop-2.10.1/share/hadoop/common/lib/jackson-core-asl-1.9.13.jar:/opt/hadoop-2.10.1/share/hadoop/common/lib/jackson-mapper-asl-1.9.13.jar:/opt/hadoop-2.10.1/share/hadoop/common/lib/jackson-jaxrs-1.9.13.jar:/opt/hadoop-2.10.1/share/hadoop/common/lib/jackson-xc-1.9.13.jar:/opt/hadoop-2.10.1/share/hadoop/common/lib/jersey-server-1.9.jar:/opt/hadoop-2.10.1/share/hadoop/common/lib/asm-3.2.jar:/opt/hadoop-2.10.1/share/hadoop/common/lib/log4j-1.2.17.jar:/opt/hadoop-2.10.1/share/hadoop/common/lib/jets3t-0.9.0.jar:/opt/hadoop-2.10.1/share/hadoop/common/lib/java-xmlbuilder-0.4.jar:/opt/hadoop-2.10.1/share/hadoop/common/lib/commons-lang-2.6.jar:/opt/hadoop-2.10.1/share/hadoop/common/lib/commons-configuration-1.6.jar:/opt/hadoop-2.10.1/share/hadoop/common/lib/commons-digester-1.8.jar:/opt/hadoop-2.10.1/share/hadoop/common/lib/commons-beanutils-1.9.4.jar:/opt/hadoop-2.10.1/share/hadoop/common/lib/commons-lang3-3.4.jar:/opt/hadoop-2.10.1/share/hadoop/common/lib/slf4j-api-1.7.25.jar:/opt/hadoop-2.10.1/share/hadoop/common/lib/slf4j-log4j12-1.7.25.jar:/opt/hadoop-2.10.1/share/hadoop/common/lib/avro-1.7.7.jar:/opt/hadoop-2.10.1/share/hadoop/common/lib/paranamer-2.3.jar:/opt/hadoop-2.10.1/share/hadoop/common/lib/snappy-java-1.0.5.jar:/opt/hadoop-2.10.1/share/hadoop/common/lib/commons-compress-1.19.jar:/opt/hadoop-2.10.1/share/hadoop/common/lib/protobuf-java-2.5.0.jar:/opt/hadoop-2.10.1/share/hadoop/common/lib/gson-2.2.4.jar:/opt/hadoop-2.10.1/share/hadoop/common/lib/hadoop-auth-2.10.1.jar:/opt/hadoop-2.10.1/share/hadoop/common/lib/nimbus-jose-jwt-7.9.jar:/opt/hadoop-2.10.1/share/hadoop/common/lib/jcip-annotations-1.0-1.jar:/opt/hadoop-2.10.1/share/hadoop/common/lib/json-smart-1.3.1.jar:/opt/hadoop-2.10.1/share/hadoop/common/lib/apacheds-kerberos-codec-2.0.0-M15.jar:/opt/hadoop-2.10.1/share/hadoop/common/lib/apacheds-i18n-2.0.0-M15.jar:/opt/hadoop-2.10.1/share/hadoop/common/lib/api-asn1-api-1.0.0-M20.jar:/opt/hadoop-2.10.1/share/hadoop/common/lib/api-util-1.0.0-M20.jar:/opt/hadoop-2.10.1/share/hadoop/common/lib/zookeeper-3.4.14.jar:/opt/hadoop-2.10.1/share/hadoop/common/lib/spotbugs-annotations-3.1.9.jar:/opt/hadoop-2.10.1/share/hadoop/common/lib/audience-annotations-0.5.0.jar:/opt/hadoop-2.10.1/share/hadoop/common/lib/netty-3.10.6.Final.jar:/opt/hadoop-2.10.1/share/hadoop/common/lib/curator-framework-2.13.0.jar:/opt/hadoop-2.10.1/share/hadoop/common/lib/curator-client-2.13.0.jar:/opt/hadoop-2.10.1/share/hadoop/common/lib/jsch-0.1.55.jar:/opt/hadoop-2.10.1/share/hadoop/common/lib/curator-recipes-2.13.0.jar:/opt/hadoop-2.10.1/share/hadoop/common/lib/htrace-core4-4.1.0-incubating.jar:/opt/hadoop-2.10.1/share/hadoop/common/lib/stax2-api-3.1.4.jar:/opt/hadoop-2.10.1/share/hadoop/common/lib/woodstox-core-5.0.3.jar:/opt/hadoop-2.10.1/share/hadoop/common/lib/junit-4.11.jar:/opt/hadoop-2.10.1/share/hadoop/common/lib/hamcrest-core-1.3.jar:/opt/hadoop-2.10.1/share/hadoop/common/lib/mockito-all-1.8.5.jar:/opt/hadoop-2.10.1/share/hadoop/common/lib/hadoop-annotations-2.10.1.jar:/opt/hadoop-2.10.1/share/hadoop/common/lib/guava-11.0.2.jar:/opt/hadoop-2.10.1/share/hadoop/common/lib/jsr305-3.0.2.jar:/opt/hadoop-2.10.1/share/hadoop/common/lib/commons-cli-1.2.jar:/opt/hadoop-2.10.1/share/hadoop/common/lib/commons-math3-3.1.1.jar:/opt/hadoop-2.10.1/share/hadoop/common/lib/xmlenc-0.52.jar:/opt/hadoop-2.10.1/share/hadoop/common/lib/httpclient-4.5.2.jar:/opt/hadoop-2.10.1/share/hadoop/common/lib/httpcore-4.4.4.jar:/opt/hadoop-2.10.1/share/hadoop/common/lib/commons-logging-1.1.3.jar:/opt/hadoop-2.10.1/share/hadoop/common/lib/commons-codec-1.4.jar:/opt/hadoop-2.10.1/share/hadoop/common/lib/commons-io-2.4.jar:/opt/hadoop-2.10.1/share/hadoop/common/lib/commons-net-3.1.jar:/opt/hadoop-2.10.1/share/hadoop/common/hadoop-common-2.10.1.jar:/opt/hadoop-2.10.1/share/hadoop/common/hadoop-common-2.10.1-tests.jar:/opt/hadoop-2.10.1/share/hadoop/common/hadoop-nfs-2.10.1.jar:/opt/hadoop-2.10.1/share/hadoop/hdfs:/opt/hadoop-2.10.1/share/hadoop/hdfs/lib/commons-codec-1.4.jar:/opt/hadoop-2.10.1/share/hadoop/hdfs/lib/log4j-1.2.17.jar:/opt/hadoop-2.10.1/share/hadoop/hdfs/lib/commons-logging-1.1.3.jar:/opt/hadoop-2.10.1/share/hadoop/hdfs/lib/jsr305-3.0.2.jar:/opt/hadoop-2.10.1/share/hadoop/hdfs/lib/netty-3.10.6.Final.jar:/opt/hadoop-2.10.1/share/hadoop/hdfs/lib/guava-11.0.2.jar:/opt/hadoop-2.10.1/share/hadoop/hdfs/lib/commons-cli-1.2.jar:/opt/hadoop-2.10.1/share/hadoop/hdfs/lib/xmlenc-0.52.jar:/opt/hadoop-2.10.1/share/hadoop/hdfs/lib/commons-io-2.4.jar:/opt/hadoop-2.10.1/share/hadoop/hdfs/lib/servlet-api-2.5.jar:/opt/hadoop-2.10.1/share/hadoop/hdfs/lib/jetty-6.1.26.jar:/opt/hadoop-2.10.1/share/hadoop/hdfs/lib/jetty-util-6.1.26.jar:/opt/hadoop-2.10.1/share/hadoop/hdfs/lib/jersey-core-1.9.jar:/opt/hadoop-2.10.1/share/hadoop/hdfs/lib/jackson-core-asl-1.9.13.jar:/opt/hadoop-2.10.1/share/hadoop/hdfs/lib/jackson-mapper-asl-1.9.13.jar:/opt/hadoop-2.10.1/share/hadoop/hdfs/lib/jersey-server-1.9.jar:/opt/hadoop-2.10.1/share/hadoop/hdfs/lib/asm-3.2.jar:/opt/hadoop-2.10.1/share/hadoop/hdfs/lib/commons-lang-2.6.jar:/opt/hadoop-2.10.1/share/hadoop/hdfs/lib/protobuf-java-2.5.0.jar:/opt/hadoop-2.10.1/share/hadoop/hdfs/lib/htrace-core4-4.1.0-incubating.jar:/opt/hadoop-2.10.1/share/hadoop/hdfs/lib/hadoop-hdfs-client-2.10.1.jar:/opt/hadoop-2.10.1/share/hadoop/hdfs/lib/okhttp-2.7.5.jar:/opt/hadoop-2.10.1/share/hadoop/hdfs/lib/okio-1.6.0.jar:/opt/hadoop-2.10.1/share/hadoop/hdfs/lib/commons-daemon-1.0.13.jar:/opt/hadoop-2.10.1/share/hadoop/hdfs/lib/netty-all-4.1.50.Final.jar:/opt/hadoop-2.10.1/share/hadoop/hdfs/lib/xercesImpl-2.12.0.jar:/opt/hadoop-2.10.1/share/hadoop/hdfs/lib/xml-apis-1.4.01.jar:/opt/hadoop-2.10.1/share/hadoop/hdfs/lib/leveldbjni-all-1.8.jar:/opt/hadoop-2.10.1/share/hadoop/hdfs/lib/jackson-databind-2.9.10.6.jar:/opt/hadoop-2.10.1/share/hadoop/hdfs/lib/jackson-annotations-2.9.10.jar:/opt/hadoop-2.10.1/share/hadoop/hdfs/lib/jackson-core-2.9.10.jar:/opt/hadoop-2.10.1/share/hadoop/hdfs/hadoop-hdfs-2.10.1.jar:/opt/hadoop-2.10.1/share/hadoop/hdfs/hadoop-hdfs-2.10.1-tests.jar:/opt/hadoop-2.10.1/share/hadoop/hdfs/hadoop-hdfs-client-2.10.1.jar:/opt/hadoop-2.10.1/share/hadoop/hdfs/hadoop-hdfs-client-2.10.1-tests.jar:/opt/hadoop-2.10.1/share/hadoop/hdfs/hadoop-hdfs-native-client-2.10.1.jar:/opt/hadoop-2.10.1/share/hadoop/hdfs/hadoop-hdfs-native-client-2.10.1-tests.jar:/opt/hadoop-2.10.1/share/hadoop/hdfs/hadoop-hdfs-rbf-2.10.1.jar:/opt/hadoop-2.10.1/share/hadoop/hdfs/hadoop-hdfs-rbf-2.10.1-tests.jar:/opt/hadoop-2.10.1/share/hadoop/hdfs/hadoop-hdfs-nfs-2.10.1.jar:/opt/hadoop-2.10.1/share/hadoop/yarn:/opt/hadoop-2.10.1/share/hadoop/yarn/lib/commons-cli-1.2.jar:/opt/hadoop-2.10.1/share/hadoop/yarn/lib/log4j-1.2.17.jar:/opt/hadoop-2.10.1/share/hadoop/yarn/lib/commons-compress-1.19.jar:/opt/hadoop-2.10.1/share/hadoop/yarn/lib/servlet-api-2.5.jar:/opt/hadoop-2.10.1/share/hadoop/yarn/lib/commons-codec-1.4.jar:/opt/hadoop-2.10.1/share/hadoop/yarn/lib/jetty-util-6.1.26.jar:/opt/hadoop-2.10.1/share/hadoop/yarn/lib/jersey-core-1.9.jar:/opt/hadoop-2.10.1/share/hadoop/yarn/lib/jersey-client-1.9.jar:/opt/hadoop-2.10.1/share/hadoop/yarn/lib/jackson-core-asl-1.9.13.jar:/opt/hadoop-2.10.1/share/hadoop/yarn/lib/jackson-mapper-asl-1.9.13.jar:/opt/hadoop-2.10.1/share/hadoop/yarn/lib/jackson-jaxrs-1.9.13.jar:/opt/hadoop-2.10.1/share/hadoop/yarn/lib/jackson-xc-1.9.13.jar:/opt/hadoop-2.10.1/share/hadoop/yarn/lib/guice-servlet-3.0.jar:/opt/hadoop-2.10.1/share/hadoop/yarn/lib/guice-3.0.jar:/opt/hadoop-2.10.1/share/hadoop/yarn/lib/javax.inject-1.jar:/opt/hadoop-2.10.1/share/hadoop/yarn/lib/aopalliance-1.0.jar:/opt/hadoop-2.10.1/share/hadoop/yarn/lib/commons-io-2.4.jar:/opt/hadoop-2.10.1/share/hadoop/yarn/lib/jersey-server-1.9.jar:/opt/hadoop-2.10.1/share/hadoop/yarn/lib/asm-3.2.jar:/opt/hadoop-2.10.1/share/hadoop/yarn/lib/jersey-json-1.9.jar:/opt/hadoop-2.10.1/share/hadoop/yarn/lib/jettison-1.1.jar:/opt/hadoop-2.10.1/share/hadoop/yarn/lib/jaxb-impl-2.2.3-1.jar:/opt/hadoop-2.10.1/share/hadoop/yarn/lib/jersey-guice-1.9.jar:/opt/hadoop-2.10.1/share/hadoop/yarn/lib/commons-math3-3.1.1.jar:/opt/hadoop-2.10.1/share/hadoop/yarn/lib/xmlenc-0.52.jar:/opt/hadoop-2.10.1/share/hadoop/yarn/lib/httpclient-4.5.2.jar:/opt/hadoop-2.10.1/share/hadoop/yarn/lib/httpcore-4.4.4.jar:/opt/hadoop-2.10.1/share/hadoop/yarn/lib/commons-net-3.1.jar:/opt/hadoop-2.10.1/share/hadoop/yarn/lib/commons-collections-3.2.2.jar:/opt/hadoop-2.10.1/share/hadoop/yarn/lib/jetty-6.1.26.jar:/opt/hadoop-2.10.1/share/hadoop/yarn/lib/jetty-sslengine-6.1.26.jar:/opt/hadoop-2.10.1/share/hadoop/yarn/lib/jsp-api-2.1.jar:/opt/hadoop-2.10.1/share/hadoop/yarn/lib/jets3t-0.9.0.jar:/opt/hadoop-2.10.1/share/hadoop/yarn/lib/java-xmlbuilder-0.4.jar:/opt/hadoop-2.10.1/share/hadoop/yarn/lib/commons-configuration-1.6.jar:/opt/hadoop-2.10.1/share/hadoop/yarn/lib/commons-digester-1.8.jar:/opt/hadoop-2.10.1/share/hadoop/yarn/lib/commons-beanutils-1.9.4.jar:/opt/hadoop-2.10.1/share/hadoop/yarn/lib/commons-lang3-3.4.jar:/opt/hadoop-2.10.1/share/hadoop/yarn/lib/avro-1.7.7.jar:/opt/hadoop-2.10.1/share/hadoop/yarn/lib/paranamer-2.3.jar:/opt/hadoop-2.10.1/share/hadoop/yarn/lib/snappy-java-1.0.5.jar:/opt/hadoop-2.10.1/share/hadoop/yarn/lib/gson-2.2.4.jar:/opt/hadoop-2.10.1/share/hadoop/yarn/lib/nimbus-jose-jwt-7.9.jar:/opt/hadoop-2.10.1/share/hadoop/yarn/lib/jcip-annotations-1.0-1.jar:/opt/hadoop-2.10.1/share/hadoop/yarn/lib/json-smart-1.3.1.jar:/opt/hadoop-2.10.1/share/hadoop/yarn/lib/apacheds-kerberos-codec-2.0.0-M15.jar:/opt/hadoop-2.10.1/share/hadoop/yarn/lib/apacheds-i18n-2.0.0-M15.jar:/opt/hadoop-2.10.1/share/hadoop/yarn/lib/api-asn1-api-1.0.0-M20.jar:/opt/hadoop-2.10.1/share/hadoop/yarn/lib/api-util-1.0.0-M20.jar:/opt/hadoop-2.10.1/share/hadoop/yarn/lib/zookeeper-3.4.14.jar:/opt/hadoop-2.10.1/share/hadoop/yarn/lib/spotbugs-annotations-3.1.9.jar:/opt/hadoop-2.10.1/share/hadoop/yarn/lib/audience-annotations-0.5.0.jar:/opt/hadoop-2.10.1/share/hadoop/yarn/lib/netty-3.10.6.Final.jar:/opt/hadoop-2.10.1/share/hadoop/yarn/lib/curator-framework-2.13.0.jar:/opt/hadoop-2.10.1/share/hadoop/yarn/lib/curator-client-2.13.0.jar:/opt/hadoop-2.10.1/share/hadoop/yarn/lib/jsch-0.1.55.jar:/opt/hadoop-2.10.1/share/hadoop/yarn/lib/curator-recipes-2.13.0.jar:/opt/hadoop-2.10.1/share/hadoop/yarn/lib/htrace-core4-4.1.0-incubating.jar:/opt/hadoop-2.10.1/share/hadoop/yarn/lib/stax2-api-3.1.4.jar:/opt/hadoop-2.10.1/share/hadoop/yarn/lib/woodstox-core-5.0.3.jar:/opt/hadoop-2.10.1/share/hadoop/yarn/lib/leveldbjni-all-1.8.jar:/opt/hadoop-2.10.1/share/hadoop/yarn/lib/geronimo-jcache_1.0_spec-1.0-alpha-1.jar:/opt/hadoop-2.10.1/share/hadoop/yarn/lib/ehcache-3.3.1.jar:/opt/hadoop-2.10.1/share/hadoop/yarn/lib/HikariCP-java7-2.4.12.jar:/opt/hadoop-2.10.1/share/hadoop/yarn/lib/mssql-jdbc-6.2.1.jre7.jar:/opt/hadoop-2.10.1/share/hadoop/yarn/lib/metrics-core-3.0.1.jar:/opt/hadoop-2.10.1/share/hadoop/yarn/lib/fst-2.50.jar:/opt/hadoop-2.10.1/share/hadoop/yarn/lib/java-util-1.9.0.jar:/opt/hadoop-2.10.1/share/hadoop/yarn/lib/json-io-2.5.1.jar:/opt/hadoop-2.10.1/share/hadoop/yarn/lib/commons-lang-2.6.jar:/opt/hadoop-2.10.1/share/hadoop/yarn/lib/guava-11.0.2.jar:/opt/hadoop-2.10.1/share/hadoop/yarn/lib/jsr305-3.0.2.jar:/opt/hadoop-2.10.1/share/hadoop/yarn/lib/commons-logging-1.1.3.jar:/opt/hadoop-2.10.1/share/hadoop/yarn/lib/jaxb-api-2.2.2.jar:/opt/hadoop-2.10.1/share/hadoop/yarn/lib/stax-api-1.0-2.jar:/opt/hadoop-2.10.1/share/hadoop/yarn/lib/activation-1.1.jar:/opt/hadoop-2.10.1/share/hadoop/yarn/lib/protobuf-java-2.5.0.jar:/opt/hadoop-2.10.1/share/hadoop/yarn/lib/spark-2.3.4-yarn-shuffle.jar:/opt/hadoop-2.10.1/share/hadoop/yarn/hadoop-yarn-api-2.10.1.jar:/opt/hadoop-2.10.1/share/hadoop/yarn/hadoop-yarn-common-2.10.1.jar:/opt/hadoop-2.10.1/share/hadoop/yarn/hadoop-yarn-registry-2.10.1.jar:/opt/hadoop-2.10.1/share/hadoop/yarn/hadoop-yarn-server-common-2.10.1.jar:/opt/hadoop-2.10.1/share/hadoop/yarn/hadoop-yarn-server-nodemanager-2.10.1.jar:/opt/hadoop-2.10.1/share/hadoop/yarn/hadoop-yarn-server-web-proxy-2.10.1.jar:/opt/hadoop-2.10.1/share/hadoop/yarn/hadoop-yarn-server-applicationhistoryservice-2.10.1.jar:/opt/hadoop-2.10.1/share/hadoop/yarn/hadoop-yarn-server-resourcemanager-2.10.1.jar:/opt/hadoop-2.10.1/share/hadoop/yarn/hadoop-yarn-server-tests-2.10.1.jar:/opt/hadoop-2.10.1/share/hadoop/yarn/hadoop-yarn-client-2.10.1.jar:/opt/hadoop-2.10.1/share/hadoop/yarn/hadoop-yarn-server-sharedcachemanager-2.10.1.jar:/opt/hadoop-2.10.1/share/hadoop/yarn/hadoop-yarn-server-timeline-pluginstorage-2.10.1.jar:/opt/hadoop-2.10.1/share/hadoop/yarn/hadoop-yarn-server-router-2.10.1.jar:/opt/hadoop-2.10.1/share/hadoop/yarn/hadoop-yarn-applications-distributedshell-2.10.1.jar:/opt/hadoop-2.10.1/share/hadoop/yarn/hadoop-yarn-applications-unmanaged-am-launcher-2.10.1.jar:/opt/hadoop-2.10.1/share/hadoop/mapreduce/lib/protobuf-java-2.5.0.jar:/opt/hadoop-2.10.1/share/hadoop/mapreduce/lib/avro-1.7.7.jar:/opt/hadoop-2.10.1/share/hadoop/mapreduce/lib/jackson-core-asl-1.9.13.jar:/opt/hadoop-2.10.1/share/hadoop/mapreduce/lib/jackson-mapper-asl-1.9.13.jar:/opt/hadoop-2.10.1/share/hadoop/mapreduce/lib/paranamer-2.3.jar:/opt/hadoop-2.10.1/share/hadoop/mapreduce/lib/snappy-java-1.0.5.jar:/opt/hadoop-2.10.1/share/hadoop/mapreduce/lib/commons-compress-1.19.jar:/opt/hadoop-2.10.1/share/hadoop/mapreduce/lib/hadoop-annotations-2.10.1.jar:/opt/hadoop-2.10.1/share/hadoop/mapreduce/lib/commons-io-2.4.jar:/opt/hadoop-2.10.1/share/hadoop/mapreduce/lib/jersey-core-1.9.jar:/opt/hadoop-2.10.1/share/hadoop/mapreduce/lib/jersey-server-1.9.jar:/opt/hadoop-2.10.1/share/hadoop/mapreduce/lib/asm-3.2.jar:/opt/hadoop-2.10.1/share/hadoop/mapreduce/lib/log4j-1.2.17.jar:/opt/hadoop-2.10.1/share/hadoop/mapreduce/lib/netty-3.10.6.Final.jar:/opt/hadoop-2.10.1/share/hadoop/mapreduce/lib/leveldbjni-all-1.8.jar:/opt/hadoop-2.10.1/share/hadoop/mapreduce/lib/guice-3.0.jar:/opt/hadoop-2.10.1/share/hadoop/mapreduce/lib/javax.inject-1.jar:/opt/hadoop-2.10.1/share/hadoop/mapreduce/lib/aopalliance-1.0.jar:/opt/hadoop-2.10.1/share/hadoop/mapreduce/lib/jersey-guice-1.9.jar:/opt/hadoop-2.10.1/share/hadoop/mapreduce/lib/guice-servlet-3.0.jar:/opt/hadoop-2.10.1/share/hadoop/mapreduce/lib/junit-4.11.jar:/opt/hadoop-2.10.1/share/hadoop/mapreduce/lib/hamcrest-core-1.3.jar:/opt/hadoop-2.10.1/share/hadoop/mapreduce/hadoop-mapreduce-client-core-2.10.1.jar:/opt/hadoop-2.10.1/share/hadoop/mapreduce/hadoop-mapreduce-client-common-2.10.1.jar:/opt/hadoop-2.10.1/share/hadoop/mapreduce/hadoop-mapreduce-client-shuffle-2.10.1.jar:/opt/hadoop-2.10.1/share/hadoop/mapreduce/hadoop-mapreduce-client-app-2.10.1.jar:/opt/hadoop-2.10.1/share/hadoop/mapreduce/hadoop-mapreduce-client-hs-2.10.1.jar:/opt/hadoop-2.10.1/share/hadoop/mapreduce/hadoop-mapreduce-client-jobclient-2.10.1.jar:/opt/hadoop-2.10.1/share/hadoop/mapreduce/hadoop-mapreduce-client-hs-plugins-2.10.1.jar:/opt/hadoop-2.10.1/share/hadoop/mapreduce/hadoop-mapreduce-examples-2.10.1.jar:/opt/hadoop-2.10.1/share/hadoop/mapreduce/hadoop-mapreduce-client-jobclient-2.10.1-tests.jar:/opt/hadoop/contrib/capacity-scheduler/*.jar
    STARTUP_MSG:   build = https://github.com/apache/hadoop -r 1827467c9a56f133025f28557bfc2c562d78e816; compiled by 'centos' on 2020-09-14T13:17Z
    STARTUP_MSG:   java = 1.8.0_282
    ************************************************************/
    21/02/21 13:35:52 INFO namenode.NameNode: registered UNIX signal handlers for [TERM, HUP, INT]
    21/02/21 13:35:52 INFO namenode.NameNode: createNameNode [-format]
    21/02/21 13:35:53 INFO common.Util: Assuming 'file' scheme for path /dfs/nn in configuration.
    21/02/21 13:35:53 INFO common.Util: Assuming 'file' scheme for path /dfs/nn in configuration.
    Formatting using clusterid: CID-a94e54b2-0de8-45d7-b88f-7009b166327a
    21/02/21 13:35:53 INFO namenode.FSEditLog: Edit logging is async:true
    21/02/21 13:35:53 INFO namenode.FSNamesystem: KeyProvider: null
    21/02/21 13:35:53 INFO namenode.FSNamesystem: fsLock is fair: true
    21/02/21 13:35:53 INFO namenode.FSNamesystem: Detailed lock hold time metrics enabled: false
    21/02/21 13:35:53 INFO namenode.FSNamesystem: fsOwner             = hdfs (auth:SIMPLE)
    21/02/21 13:35:53 INFO namenode.FSNamesystem: supergroup          = supergroup
    21/02/21 13:35:53 INFO namenode.FSNamesystem: isPermissionEnabled = true
    21/02/21 13:35:53 INFO namenode.FSNamesystem: Determined nameservice ID: peter-cluster
    21/02/21 13:35:53 INFO namenode.FSNamesystem: HA Enabled: true
    21/02/21 13:35:53 INFO common.Util: dfs.datanode.fileio.profiling.sampling.percentage set to 0. Disabling file IO profiling
    21/02/21 13:35:53 INFO util.HostsFileReader: Adding a node "peter-kafka001" to the list of included hosts from /opt/hadoop/etc/hadoop/include_server
    21/02/21 13:35:53 INFO util.HostsFileReader: Adding a node "peter-kafka002" to the list of included hosts from /opt/hadoop/etc/hadoop/include_server
    21/02/21 13:35:53 INFO util.HostsFileReader: Adding a node "peter-kafka003" to the list of included hosts from /opt/hadoop/etc/hadoop/include_server
    21/02/21 13:35:53 INFO blockmanagement.DatanodeManager: dfs.block.invalidate.limit: configured=1000, counted=60, effected=1000
    21/02/21 13:35:53 INFO blockmanagement.DatanodeManager: dfs.namenode.datanode.registration.ip-hostname-check=true
    21/02/21 13:35:53 INFO blockmanagement.BlockManager: dfs.namenode.startup.delay.block.deletion.sec is set to 000:00:00:00.000
    21/02/21 13:35:53 INFO blockmanagement.BlockManager: The block deletion will start around 2021 2월 21 13:35:53
    21/02/21 13:35:53 INFO util.GSet: Computing capacity for map BlocksMap
    21/02/21 13:35:53 INFO util.GSet: VM type       = 64-bit
    21/02/21 13:35:53 INFO util.GSet: 2.0% max memory 889 MB = 17.8 MB
    21/02/21 13:35:53 INFO util.GSet: capacity      = 2^21 = 2097152 entries
    21/02/21 13:35:53 INFO blockmanagement.BlockManager: dfs.block.access.token.enable=false
    21/02/21 13:35:53 WARN conf.Configuration: No unit for dfs.heartbeat.interval(3) assuming SECONDS
    21/02/21 13:35:53 WARN conf.Configuration: No unit for dfs.namenode.safemode.extension(30000) assuming MILLISECONDS
    21/02/21 13:35:53 INFO blockmanagement.BlockManagerSafeMode: dfs.namenode.safemode.threshold-pct = 0.9990000128746033
    21/02/21 13:35:53 INFO blockmanagement.BlockManagerSafeMode: dfs.namenode.safemode.min.datanodes = 0
    21/02/21 13:35:53 INFO blockmanagement.BlockManagerSafeMode: dfs.namenode.safemode.extension = 30000
    21/02/21 13:35:53 INFO blockmanagement.BlockManager: defaultReplication         = 2
    21/02/21 13:35:53 INFO blockmanagement.BlockManager: maxReplication             = 512
    21/02/21 13:35:53 INFO blockmanagement.BlockManager: minReplication             = 1
    21/02/21 13:35:53 INFO blockmanagement.BlockManager: maxReplicationStreams      = 2
    21/02/21 13:35:53 INFO blockmanagement.BlockManager: replicationRecheckInterval = 3000
    21/02/21 13:35:53 INFO blockmanagement.BlockManager: encryptDataTransfer        = false
    21/02/21 13:35:53 INFO blockmanagement.BlockManager: maxNumBlocksToLog          = 1000
    21/02/21 13:35:53 INFO namenode.FSNamesystem: Append Enabled: true
    21/02/21 13:35:53 INFO namenode.FSDirectory: GLOBAL serial map: bits=24 maxEntries=16777215
    21/02/21 13:35:53 INFO util.GSet: Computing capacity for map INodeMap
    21/02/21 13:35:53 INFO util.GSet: VM type       = 64-bit
    21/02/21 13:35:53 INFO util.GSet: 1.0% max memory 889 MB = 8.9 MB
    21/02/21 13:35:53 INFO util.GSet: capacity      = 2^20 = 1048576 entries
    21/02/21 13:35:53 INFO namenode.FSDirectory: ACLs enabled? false
    21/02/21 13:35:53 INFO namenode.FSDirectory: XAttrs enabled? true
    21/02/21 13:35:53 INFO namenode.NameNode: Caching file names occurring more than 10 times
    21/02/21 13:35:53 INFO snapshot.SnapshotManager: Loaded config captureOpenFiles: falseskipCaptureAccessTimeOnlyChange: false
    21/02/21 13:35:53 INFO util.GSet: Computing capacity for map cachedBlocks
    21/02/21 13:35:53 INFO util.GSet: VM type       = 64-bit
    21/02/21 13:35:53 INFO util.GSet: 0.25% max memory 889 MB = 2.2 MB
    21/02/21 13:35:53 INFO util.GSet: capacity      = 2^18 = 262144 entries
    21/02/21 13:35:53 INFO metrics.TopMetrics: NNTop conf: dfs.namenode.top.window.num.buckets = 10
    21/02/21 13:35:53 INFO metrics.TopMetrics: NNTop conf: dfs.namenode.top.num.users = 10
    21/02/21 13:35:53 INFO metrics.TopMetrics: NNTop conf: dfs.namenode.top.windows.minutes = 1,5,25
    21/02/21 13:35:53 INFO namenode.FSNamesystem: Retry cache on namenode is enabled
    21/02/21 13:35:53 INFO namenode.FSNamesystem: Retry cache will use 0.03 of total heap and retry cache entry expiry time is 600000 millis
    21/02/21 13:35:53 INFO util.GSet: Computing capacity for map NameNodeRetryCache
    21/02/21 13:35:53 INFO util.GSet: VM type       = 64-bit
    21/02/21 13:35:53 INFO util.GSet: 0.029999999329447746% max memory 889 MB = 273.1 KB
    21/02/21 13:35:53 INFO util.GSet: capacity      = 2^15 = 32768 entries
    21/02/21 13:35:54 INFO namenode.FSImage: Allocated new BlockPoolId: BP-1796558398-192.168.126.71-1613882154343
    21/02/21 13:35:54 INFO common.Storage: Storage directory /dfs/nn has been successfully formatted.
    21/02/21 13:35:54 INFO namenode.FSImageFormatProtobuf: Saving image file /dfs/nn/current/fsimage.ckpt_0000000000000000000 using no compression
    21/02/21 13:35:54 INFO namenode.FSImageFormatProtobuf: Image file /dfs/nn/current/fsimage.ckpt_0000000000000000000 of size 322 bytes saved in 0 seconds .
    21/02/21 13:35:54 INFO namenode.NNStorageRetentionManager: Going to retain 1 images with txid >= 0
    21/02/21 13:35:54 INFO namenode.FSImage: FSImageSaver clean checkpoint: txid = 0 when meet shutdown.
    21/02/21 13:35:54 INFO namenode.NameNode: SHUTDOWN_MSG: 
    /************************************************************
    SHUTDOWN_MSG: Shutting down NameNode at peter-kafka001/192.168.126.71
    ************************************************************/
    [hdfs@peter-kafka001 hadoop]$ 
    [hdfs@peter-kafka001 hadoop]$ ll /dfs/nn/current/
    합계 16
    -rw-r--r-- 1 hdfs hadoop 218  2월 21 13:35 VERSION
    -rw-r--r-- 1 hdfs hadoop 322  2월 21 13:35 fsimage_0000000000000000000
    -rw-r--r-- 1 hdfs hadoop  62  2월 21 13:35 fsimage_0000000000000000000.md5
    -rw-r--r-- 1 hdfs hadoop   2  2월 21 13:35 seen_txid
    [hdfs@peter-kafka001 hadoop]$ 

    2. Secondary Namenode data 동기화를 위해 namenode 실행 : in peter-kafka001
    # su - hdfs
    $ cd /opt/hadoop
    $ sbin/hadoop-daemon.sh start namenode

    [hdfs@peter-kafka001 ~]$ cd /opt/hadoop
    [hdfs@peter-kafka001 hadoop]$ jps
    5621 Jps
    [hdfs@peter-kafka001 hadoop]$ sbin/hadoop-daemon.sh start namenode
    starting namenode, logging to /opt/hadoop-2.10.1/logs/hadoop-hdfs-namenode-peter-kafka001.out
    [hdfs@peter-kafka001 hadoop]$ jps
    5649 NameNode            # <--- 실행후 추가됨.
    5802 Jps
    [hdfs@peter-kafka001 hadoop]$ 

    3. Secondary Namenode data 동기화 : in peter-kafka002
    # su - hdfs
    $ cd /opt/hadoop
    $ bin/hdfs namenode -bootstrapStandby

    [hdfs@peter-kafka002 ~]$ cd /opt/hadoop
    [hdfs@peter-kafka002 hadoop]$ bin/hdfs namenode -bootstrapStandby
    21/02/21 14:02:29 INFO namenode.NameNode: STARTUP_MSG: 
    /************************************************************
    STARTUP_MSG: Starting NameNode
    STARTUP_MSG:   host = peter-kafka002/192.168.126.72
    STARTUP_MSG:   args = [-bootstrapStandby]
    STARTUP_MSG:   version = 2.10.1
    STARTUP_MSG:   classpath = /opt/hadoop/etc/hadoop:/opt/hadoop-2.10.1/share/hadoop/common/lib/commons-collections-3.2.2.jar:/opt/hadoop-2.10.1/share/hadoop/common/lib/servlet-api-2.5.jar:/opt/hadoop-2.10.1/share/hadoop/common/lib/jetty-6.1.26.jar:/opt/hadoop-2.10.1/share/hadoop/common/lib/jetty-util-6.1.26.jar:/opt/hadoop-2.10.1/share/hadoop/common/lib/jetty-sslengine-6.1.26.jar:/opt/hadoop-2.10.1/share/hadoop/common/lib/jsp-api-2.1.jar:/opt/hadoop-2.10.1/share/hadoop/common/lib/jersey-core-1.9.jar:/opt/hadoop-2.10.1/share/hadoop/common/lib/jersey-json-1.9.jar:/opt/hadoop-2.10.1/share/hadoop/common/lib/jettison-1.1.jar:/opt/hadoop-2.10.1/share/hadoop/common/lib/jaxb-impl-2.2.3-1.jar:/opt/hadoop-2.10.1/share/hadoop/common/lib/jaxb-api-2.2.2.jar:/opt/hadoop-2.10.1/share/hadoop/common/lib/stax-api-1.0-2.jar:/opt/hadoop-2.10.1/share/hadoop/common/lib/activation-1.1.jar:/opt/hadoop-2.10.1/share/hadoop/common/lib/jackson-core-asl-1.9.13.jar:/opt/hadoop-2.10.1/share/hadoop/common/lib/jackson-mapper-asl-1.9.13.jar:/opt/hadoop-2.10.1/share/hadoop/common/lib/jackson-jaxrs-1.9.13.jar:/opt/hadoop-2.10.1/share/hadoop/common/lib/jackson-xc-1.9.13.jar:/opt/hadoop-2.10.1/share/hadoop/common/lib/jersey-server-1.9.jar:/opt/hadoop-2.10.1/share/hadoop/common/lib/asm-3.2.jar:/opt/hadoop-2.10.1/share/hadoop/common/lib/log4j-1.2.17.jar:/opt/hadoop-2.10.1/share/hadoop/common/lib/jets3t-0.9.0.jar:/opt/hadoop-2.10.1/share/hadoop/common/lib/java-xmlbuilder-0.4.jar:/opt/hadoop-2.10.1/share/hadoop/common/lib/commons-lang-2.6.jar:/opt/hadoop-2.10.1/share/hadoop/common/lib/commons-configuration-1.6.jar:/opt/hadoop-2.10.1/share/hadoop/common/lib/commons-digester-1.8.jar:/opt/hadoop-2.10.1/share/hadoop/common/lib/commons-beanutils-1.9.4.jar:/opt/hadoop-2.10.1/share/hadoop/common/lib/commons-lang3-3.4.jar:/opt/hadoop-2.10.1/share/hadoop/common/lib/slf4j-api-1.7.25.jar:/opt/hadoop-2.10.1/share/hadoop/common/lib/slf4j-log4j12-1.7.25.jar:/opt/hadoop-2.10.1/share/hadoop/common/lib/avro-1.7.7.jar:/opt/hadoop-2.10.1/share/hadoop/common/lib/paranamer-2.3.jar:/opt/hadoop-2.10.1/share/hadoop/common/lib/snappy-java-1.0.5.jar:/opt/hadoop-2.10.1/share/hadoop/common/lib/commons-compress-1.19.jar:/opt/hadoop-2.10.1/share/hadoop/common/lib/protobuf-java-2.5.0.jar:/opt/hadoop-2.10.1/share/hadoop/common/lib/gson-2.2.4.jar:/opt/hadoop-2.10.1/share/hadoop/common/lib/hadoop-auth-2.10.1.jar:/opt/hadoop-2.10.1/share/hadoop/common/lib/nimbus-jose-jwt-7.9.jar:/opt/hadoop-2.10.1/share/hadoop/common/lib/jcip-annotations-1.0-1.jar:/opt/hadoop-2.10.1/share/hadoop/common/lib/json-smart-1.3.1.jar:/opt/hadoop-2.10.1/share/hadoop/common/lib/apacheds-kerberos-codec-2.0.0-M15.jar:/opt/hadoop-2.10.1/share/hadoop/common/lib/apacheds-i18n-2.0.0-M15.jar:/opt/hadoop-2.10.1/share/hadoop/common/lib/api-asn1-api-1.0.0-M20.jar:/opt/hadoop-2.10.1/share/hadoop/common/lib/api-util-1.0.0-M20.jar:/opt/hadoop-2.10.1/share/hadoop/common/lib/zookeeper-3.4.14.jar:/opt/hadoop-2.10.1/share/hadoop/common/lib/spotbugs-annotations-3.1.9.jar:/opt/hadoop-2.10.1/share/hadoop/common/lib/audience-annotations-0.5.0.jar:/opt/hadoop-2.10.1/share/hadoop/common/lib/netty-3.10.6.Final.jar:/opt/hadoop-2.10.1/share/hadoop/common/lib/curator-framework-2.13.0.jar:/opt/hadoop-2.10.1/share/hadoop/common/lib/curator-client-2.13.0.jar:/opt/hadoop-2.10.1/share/hadoop/common/lib/jsch-0.1.55.jar:/opt/hadoop-2.10.1/share/hadoop/common/lib/curator-recipes-2.13.0.jar:/opt/hadoop-2.10.1/share/hadoop/common/lib/htrace-core4-4.1.0-incubating.jar:/opt/hadoop-2.10.1/share/hadoop/common/lib/stax2-api-3.1.4.jar:/opt/hadoop-2.10.1/share/hadoop/common/lib/woodstox-core-5.0.3.jar:/opt/hadoop-2.10.1/share/hadoop/common/lib/junit-4.11.jar:/opt/hadoop-2.10.1/share/hadoop/common/lib/hamcrest-core-1.3.jar:/opt/hadoop-2.10.1/share/hadoop/common/lib/mockito-all-1.8.5.jar:/opt/hadoop-2.10.1/share/hadoop/common/lib/hadoop-annotations-2.10.1.jar:/opt/hadoop-2.10.1/share/hadoop/common/lib/guava-11.0.2.jar:/opt/hadoop-2.10.1/share/hadoop/common/lib/jsr305-3.0.2.jar:/opt/hadoop-2.10.1/share/hadoop/common/lib/commons-cli-1.2.jar:/opt/hadoop-2.10.1/share/hadoop/common/lib/commons-math3-3.1.1.jar:/opt/hadoop-2.10.1/share/hadoop/common/lib/xmlenc-0.52.jar:/opt/hadoop-2.10.1/share/hadoop/common/lib/httpclient-4.5.2.jar:/opt/hadoop-2.10.1/share/hadoop/common/lib/httpcore-4.4.4.jar:/opt/hadoop-2.10.1/share/hadoop/common/lib/commons-logging-1.1.3.jar:/opt/hadoop-2.10.1/share/hadoop/common/lib/commons-codec-1.4.jar:/opt/hadoop-2.10.1/share/hadoop/common/lib/commons-io-2.4.jar:/opt/hadoop-2.10.1/share/hadoop/common/lib/commons-net-3.1.jar:/opt/hadoop-2.10.1/share/hadoop/common/hadoop-common-2.10.1.jar:/opt/hadoop-2.10.1/share/hadoop/common/hadoop-common-2.10.1-tests.jar:/opt/hadoop-2.10.1/share/hadoop/common/hadoop-nfs-2.10.1.jar:/opt/hadoop-2.10.1/share/hadoop/hdfs:/opt/hadoop-2.10.1/share/hadoop/hdfs/lib/commons-codec-1.4.jar:/opt/hadoop-2.10.1/share/hadoop/hdfs/lib/log4j-1.2.17.jar:/opt/hadoop-2.10.1/share/hadoop/hdfs/lib/commons-logging-1.1.3.jar:/opt/hadoop-2.10.1/share/hadoop/hdfs/lib/jsr305-3.0.2.jar:/opt/hadoop-2.10.1/share/hadoop/hdfs/lib/netty-3.10.6.Final.jar:/opt/hadoop-2.10.1/share/hadoop/hdfs/lib/guava-11.0.2.jar:/opt/hadoop-2.10.1/share/hadoop/hdfs/lib/commons-cli-1.2.jar:/opt/hadoop-2.10.1/share/hadoop/hdfs/lib/xmlenc-0.52.jar:/opt/hadoop-2.10.1/share/hadoop/hdfs/lib/commons-io-2.4.jar:/opt/hadoop-2.10.1/share/hadoop/hdfs/lib/servlet-api-2.5.jar:/opt/hadoop-2.10.1/share/hadoop/hdfs/lib/jetty-6.1.26.jar:/opt/hadoop-2.10.1/share/hadoop/hdfs/lib/jetty-util-6.1.26.jar:/opt/hadoop-2.10.1/share/hadoop/hdfs/lib/jersey-core-1.9.jar:/opt/hadoop-2.10.1/share/hadoop/hdfs/lib/jackson-core-asl-1.9.13.jar:/opt/hadoop-2.10.1/share/hadoop/hdfs/lib/jackson-mapper-asl-1.9.13.jar:/opt/hadoop-2.10.1/share/hadoop/hdfs/lib/jersey-server-1.9.jar:/opt/hadoop-2.10.1/share/hadoop/hdfs/lib/asm-3.2.jar:/opt/hadoop-2.10.1/share/hadoop/hdfs/lib/commons-lang-2.6.jar:/opt/hadoop-2.10.1/share/hadoop/hdfs/lib/protobuf-java-2.5.0.jar:/opt/hadoop-2.10.1/share/hadoop/hdfs/lib/htrace-core4-4.1.0-incubating.jar:/opt/hadoop-2.10.1/share/hadoop/hdfs/lib/hadoop-hdfs-client-2.10.1.jar:/opt/hadoop-2.10.1/share/hadoop/hdfs/lib/okhttp-2.7.5.jar:/opt/hadoop-2.10.1/share/hadoop/hdfs/lib/okio-1.6.0.jar:/opt/hadoop-2.10.1/share/hadoop/hdfs/lib/commons-daemon-1.0.13.jar:/opt/hadoop-2.10.1/share/hadoop/hdfs/lib/netty-all-4.1.50.Final.jar:/opt/hadoop-2.10.1/share/hadoop/hdfs/lib/xercesImpl-2.12.0.jar:/opt/hadoop-2.10.1/share/hadoop/hdfs/lib/xml-apis-1.4.01.jar:/opt/hadoop-2.10.1/share/hadoop/hdfs/lib/leveldbjni-all-1.8.jar:/opt/hadoop-2.10.1/share/hadoop/hdfs/lib/jackson-databind-2.9.10.6.jar:/opt/hadoop-2.10.1/share/hadoop/hdfs/lib/jackson-annotations-2.9.10.jar:/opt/hadoop-2.10.1/share/hadoop/hdfs/lib/jackson-core-2.9.10.jar:/opt/hadoop-2.10.1/share/hadoop/hdfs/hadoop-hdfs-2.10.1.jar:/opt/hadoop-2.10.1/share/hadoop/hdfs/hadoop-hdfs-2.10.1-tests.jar:/opt/hadoop-2.10.1/share/hadoop/hdfs/hadoop-hdfs-client-2.10.1.jar:/opt/hadoop-2.10.1/share/hadoop/hdfs/hadoop-hdfs-client-2.10.1-tests.jar:/opt/hadoop-2.10.1/share/hadoop/hdfs/hadoop-hdfs-native-client-2.10.1.jar:/opt/hadoop-2.10.1/share/hadoop/hdfs/hadoop-hdfs-native-client-2.10.1-tests.jar:/opt/hadoop-2.10.1/share/hadoop/hdfs/hadoop-hdfs-rbf-2.10.1.jar:/opt/hadoop-2.10.1/share/hadoop/hdfs/hadoop-hdfs-rbf-2.10.1-tests.jar:/opt/hadoop-2.10.1/share/hadoop/hdfs/hadoop-hdfs-nfs-2.10.1.jar:/opt/hadoop-2.10.1/share/hadoop/yarn:/opt/hadoop-2.10.1/share/hadoop/yarn/lib/commons-cli-1.2.jar:/opt/hadoop-2.10.1/share/hadoop/yarn/lib/log4j-1.2.17.jar:/opt/hadoop-2.10.1/share/hadoop/yarn/lib/commons-compress-1.19.jar:/opt/hadoop-2.10.1/share/hadoop/yarn/lib/servlet-api-2.5.jar:/opt/hadoop-2.10.1/share/hadoop/yarn/lib/commons-codec-1.4.jar:/opt/hadoop-2.10.1/share/hadoop/yarn/lib/jetty-util-6.1.26.jar:/opt/hadoop-2.10.1/share/hadoop/yarn/lib/jersey-core-1.9.jar:/opt/hadoop-2.10.1/share/hadoop/yarn/lib/jersey-client-1.9.jar:/opt/hadoop-2.10.1/share/hadoop/yarn/lib/jackson-core-asl-1.9.13.jar:/opt/hadoop-2.10.1/share/hadoop/yarn/lib/jackson-mapper-asl-1.9.13.jar:/opt/hadoop-2.10.1/share/hadoop/yarn/lib/jackson-jaxrs-1.9.13.jar:/opt/hadoop-2.10.1/share/hadoop/yarn/lib/jackson-xc-1.9.13.jar:/opt/hadoop-2.10.1/share/hadoop/yarn/lib/guice-servlet-3.0.jar:/opt/hadoop-2.10.1/share/hadoop/yarn/lib/guice-3.0.jar:/opt/hadoop-2.10.1/share/hadoop/yarn/lib/javax.inject-1.jar:/opt/hadoop-2.10.1/share/hadoop/yarn/lib/aopalliance-1.0.jar:/opt/hadoop-2.10.1/share/hadoop/yarn/lib/commons-io-2.4.jar:/opt/hadoop-2.10.1/share/hadoop/yarn/lib/jersey-server-1.9.jar:/opt/hadoop-2.10.1/share/hadoop/yarn/lib/asm-3.2.jar:/opt/hadoop-2.10.1/share/hadoop/yarn/lib/jersey-json-1.9.jar:/opt/hadoop-2.10.1/share/hadoop/yarn/lib/jettison-1.1.jar:/opt/hadoop-2.10.1/share/hadoop/yarn/lib/jaxb-impl-2.2.3-1.jar:/opt/hadoop-2.10.1/share/hadoop/yarn/lib/jersey-guice-1.9.jar:/opt/hadoop-2.10.1/share/hadoop/yarn/lib/commons-math3-3.1.1.jar:/opt/hadoop-2.10.1/share/hadoop/yarn/lib/xmlenc-0.52.jar:/opt/hadoop-2.10.1/share/hadoop/yarn/lib/httpclient-4.5.2.jar:/opt/hadoop-2.10.1/share/hadoop/yarn/lib/httpcore-4.4.4.jar:/opt/hadoop-2.10.1/share/hadoop/yarn/lib/commons-net-3.1.jar:/opt/hadoop-2.10.1/share/hadoop/yarn/lib/commons-collections-3.2.2.jar:/opt/hadoop-2.10.1/share/hadoop/yarn/lib/jetty-6.1.26.jar:/opt/hadoop-2.10.1/share/hadoop/yarn/lib/jetty-sslengine-6.1.26.jar:/opt/hadoop-2.10.1/share/hadoop/yarn/lib/jsp-api-2.1.jar:/opt/hadoop-2.10.1/share/hadoop/yarn/lib/jets3t-0.9.0.jar:/opt/hadoop-2.10.1/share/hadoop/yarn/lib/java-xmlbuilder-0.4.jar:/opt/hadoop-2.10.1/share/hadoop/yarn/lib/commons-configuration-1.6.jar:/opt/hadoop-2.10.1/share/hadoop/yarn/lib/commons-digester-1.8.jar:/opt/hadoop-2.10.1/share/hadoop/yarn/lib/commons-beanutils-1.9.4.jar:/opt/hadoop-2.10.1/share/hadoop/yarn/lib/commons-lang3-3.4.jar:/opt/hadoop-2.10.1/share/hadoop/yarn/lib/avro-1.7.7.jar:/opt/hadoop-2.10.1/share/hadoop/yarn/lib/paranamer-2.3.jar:/opt/hadoop-2.10.1/share/hadoop/yarn/lib/snappy-java-1.0.5.jar:/opt/hadoop-2.10.1/share/hadoop/yarn/lib/gson-2.2.4.jar:/opt/hadoop-2.10.1/share/hadoop/yarn/lib/nimbus-jose-jwt-7.9.jar:/opt/hadoop-2.10.1/share/hadoop/yarn/lib/jcip-annotations-1.0-1.jar:/opt/hadoop-2.10.1/share/hadoop/yarn/lib/json-smart-1.3.1.jar:/opt/hadoop-2.10.1/share/hadoop/yarn/lib/apacheds-kerberos-codec-2.0.0-M15.jar:/opt/hadoop-2.10.1/share/hadoop/yarn/lib/apacheds-i18n-2.0.0-M15.jar:/opt/hadoop-2.10.1/share/hadoop/yarn/lib/api-asn1-api-1.0.0-M20.jar:/opt/hadoop-2.10.1/share/hadoop/yarn/lib/api-util-1.0.0-M20.jar:/opt/hadoop-2.10.1/share/hadoop/yarn/lib/zookeeper-3.4.14.jar:/opt/hadoop-2.10.1/share/hadoop/yarn/lib/spotbugs-annotations-3.1.9.jar:/opt/hadoop-2.10.1/share/hadoop/yarn/lib/audience-annotations-0.5.0.jar:/opt/hadoop-2.10.1/share/hadoop/yarn/lib/netty-3.10.6.Final.jar:/opt/hadoop-2.10.1/share/hadoop/yarn/lib/curator-framework-2.13.0.jar:/opt/hadoop-2.10.1/share/hadoop/yarn/lib/curator-client-2.13.0.jar:/opt/hadoop-2.10.1/share/hadoop/yarn/lib/jsch-0.1.55.jar:/opt/hadoop-2.10.1/share/hadoop/yarn/lib/curator-recipes-2.13.0.jar:/opt/hadoop-2.10.1/share/hadoop/yarn/lib/htrace-core4-4.1.0-incubating.jar:/opt/hadoop-2.10.1/share/hadoop/yarn/lib/stax2-api-3.1.4.jar:/opt/hadoop-2.10.1/share/hadoop/yarn/lib/woodstox-core-5.0.3.jar:/opt/hadoop-2.10.1/share/hadoop/yarn/lib/leveldbjni-all-1.8.jar:/opt/hadoop-2.10.1/share/hadoop/yarn/lib/geronimo-jcache_1.0_spec-1.0-alpha-1.jar:/opt/hadoop-2.10.1/share/hadoop/yarn/lib/ehcache-3.3.1.jar:/opt/hadoop-2.10.1/share/hadoop/yarn/lib/HikariCP-java7-2.4.12.jar:/opt/hadoop-2.10.1/share/hadoop/yarn/lib/mssql-jdbc-6.2.1.jre7.jar:/opt/hadoop-2.10.1/share/hadoop/yarn/lib/metrics-core-3.0.1.jar:/opt/hadoop-2.10.1/share/hadoop/yarn/lib/fst-2.50.jar:/opt/hadoop-2.10.1/share/hadoop/yarn/lib/java-util-1.9.0.jar:/opt/hadoop-2.10.1/share/hadoop/yarn/lib/json-io-2.5.1.jar:/opt/hadoop-2.10.1/share/hadoop/yarn/lib/commons-lang-2.6.jar:/opt/hadoop-2.10.1/share/hadoop/yarn/lib/guava-11.0.2.jar:/opt/hadoop-2.10.1/share/hadoop/yarn/lib/jsr305-3.0.2.jar:/opt/hadoop-2.10.1/share/hadoop/yarn/lib/commons-logging-1.1.3.jar:/opt/hadoop-2.10.1/share/hadoop/yarn/lib/jaxb-api-2.2.2.jar:/opt/hadoop-2.10.1/share/hadoop/yarn/lib/stax-api-1.0-2.jar:/opt/hadoop-2.10.1/share/hadoop/yarn/lib/activation-1.1.jar:/opt/hadoop-2.10.1/share/hadoop/yarn/lib/protobuf-java-2.5.0.jar:/opt/hadoop-2.10.1/share/hadoop/yarn/lib/spark-2.3.4-yarn-shuffle.jar:/opt/hadoop-2.10.1/share/hadoop/yarn/hadoop-yarn-api-2.10.1.jar:/opt/hadoop-2.10.1/share/hadoop/yarn/hadoop-yarn-common-2.10.1.jar:/opt/hadoop-2.10.1/share/hadoop/yarn/hadoop-yarn-registry-2.10.1.jar:/opt/hadoop-2.10.1/share/hadoop/yarn/hadoop-yarn-server-common-2.10.1.jar:/opt/hadoop-2.10.1/share/hadoop/yarn/hadoop-yarn-server-nodemanager-2.10.1.jar:/opt/hadoop-2.10.1/share/hadoop/yarn/hadoop-yarn-server-web-proxy-2.10.1.jar:/opt/hadoop-2.10.1/share/hadoop/yarn/hadoop-yarn-server-applicationhistoryservice-2.10.1.jar:/opt/hadoop-2.10.1/share/hadoop/yarn/hadoop-yarn-server-resourcemanager-2.10.1.jar:/opt/hadoop-2.10.1/share/hadoop/yarn/hadoop-yarn-server-tests-2.10.1.jar:/opt/hadoop-2.10.1/share/hadoop/yarn/hadoop-yarn-client-2.10.1.jar:/opt/hadoop-2.10.1/share/hadoop/yarn/hadoop-yarn-server-sharedcachemanager-2.10.1.jar:/opt/hadoop-2.10.1/share/hadoop/yarn/hadoop-yarn-server-timeline-pluginstorage-2.10.1.jar:/opt/hadoop-2.10.1/share/hadoop/yarn/hadoop-yarn-server-router-2.10.1.jar:/opt/hadoop-2.10.1/share/hadoop/yarn/hadoop-yarn-applications-distributedshell-2.10.1.jar:/opt/hadoop-2.10.1/share/hadoop/yarn/hadoop-yarn-applications-unmanaged-am-launcher-2.10.1.jar:/opt/hadoop-2.10.1/share/hadoop/mapreduce/lib/protobuf-java-2.5.0.jar:/opt/hadoop-2.10.1/share/hadoop/mapreduce/lib/avro-1.7.7.jar:/opt/hadoop-2.10.1/share/hadoop/mapreduce/lib/jackson-core-asl-1.9.13.jar:/opt/hadoop-2.10.1/share/hadoop/mapreduce/lib/jackson-mapper-asl-1.9.13.jar:/opt/hadoop-2.10.1/share/hadoop/mapreduce/lib/paranamer-2.3.jar:/opt/hadoop-2.10.1/share/hadoop/mapreduce/lib/snappy-java-1.0.5.jar:/opt/hadoop-2.10.1/share/hadoop/mapreduce/lib/commons-compress-1.19.jar:/opt/hadoop-2.10.1/share/hadoop/mapreduce/lib/hadoop-annotations-2.10.1.jar:/opt/hadoop-2.10.1/share/hadoop/mapreduce/lib/commons-io-2.4.jar:/opt/hadoop-2.10.1/share/hadoop/mapreduce/lib/jersey-core-1.9.jar:/opt/hadoop-2.10.1/share/hadoop/mapreduce/lib/jersey-server-1.9.jar:/opt/hadoop-2.10.1/share/hadoop/mapreduce/lib/asm-3.2.jar:/opt/hadoop-2.10.1/share/hadoop/mapreduce/lib/log4j-1.2.17.jar:/opt/hadoop-2.10.1/share/hadoop/mapreduce/lib/netty-3.10.6.Final.jar:/opt/hadoop-2.10.1/share/hadoop/mapreduce/lib/leveldbjni-all-1.8.jar:/opt/hadoop-2.10.1/share/hadoop/mapreduce/lib/guice-3.0.jar:/opt/hadoop-2.10.1/share/hadoop/mapreduce/lib/javax.inject-1.jar:/opt/hadoop-2.10.1/share/hadoop/mapreduce/lib/aopalliance-1.0.jar:/opt/hadoop-2.10.1/share/hadoop/mapreduce/lib/jersey-guice-1.9.jar:/opt/hadoop-2.10.1/share/hadoop/mapreduce/lib/guice-servlet-3.0.jar:/opt/hadoop-2.10.1/share/hadoop/mapreduce/lib/junit-4.11.jar:/opt/hadoop-2.10.1/share/hadoop/mapreduce/lib/hamcrest-core-1.3.jar:/opt/hadoop-2.10.1/share/hadoop/mapreduce/hadoop-mapreduce-client-core-2.10.1.jar:/opt/hadoop-2.10.1/share/hadoop/mapreduce/hadoop-mapreduce-client-common-2.10.1.jar:/opt/hadoop-2.10.1/share/hadoop/mapreduce/hadoop-mapreduce-client-shuffle-2.10.1.jar:/opt/hadoop-2.10.1/share/hadoop/mapreduce/hadoop-mapreduce-client-app-2.10.1.jar:/opt/hadoop-2.10.1/share/hadoop/mapreduce/hadoop-mapreduce-client-hs-2.10.1.jar:/opt/hadoop-2.10.1/share/hadoop/mapreduce/hadoop-mapreduce-client-jobclient-2.10.1.jar:/opt/hadoop-2.10.1/share/hadoop/mapreduce/hadoop-mapreduce-client-hs-plugins-2.10.1.jar:/opt/hadoop-2.10.1/share/hadoop/mapreduce/hadoop-mapreduce-examples-2.10.1.jar:/opt/hadoop-2.10.1/share/hadoop/mapreduce/hadoop-mapreduce-client-jobclient-2.10.1-tests.jar:/opt/hadoop/contrib/capacity-scheduler/*.jar
    STARTUP_MSG:   build = https://github.com/apache/hadoop -r 1827467c9a56f133025f28557bfc2c562d78e816; compiled by 'centos' on 2020-09-14T13:17Z
    STARTUP_MSG:   java = 1.8.0_282
    ************************************************************/
    21/02/21 14:02:29 INFO namenode.NameNode: registered UNIX signal handlers for [TERM, HUP, INT]
    21/02/21 14:02:29 INFO namenode.NameNode: createNameNode [-bootstrapStandby]
    21/02/21 14:02:29 INFO ha.BootstrapStandby: Found nn: nn1, ipc: peter-kafka001/192.168.126.71:8020
    21/02/21 14:02:29 INFO common.Util: Assuming 'file' scheme for path /dfs/nn in configuration.
    21/02/21 14:02:29 INFO common.Util: Assuming 'file' scheme for path /dfs/nn in configuration.
    =====================================================
    About to bootstrap Standby ID nn2 from:
           Nameservice ID: peter-cluster
        Other Namenode ID: nn1
      Other NN's HTTP address: http://peter-kafka001:50070
      Other NN's IPC  address: peter-kafka001/192.168.126.71:8020
             Namespace ID: 190723722
            Block pool ID: BP-1796558398-192.168.126.71-1613882154343
               Cluster ID: CID-a94e54b2-0de8-45d7-b88f-7009b166327a
           Layout version: -63
           isUpgradeFinalized: true
    =====================================================
    21/02/21 14:02:30 INFO common.Storage: Storage directory /dfs/nn has been successfully formatted.
    21/02/21 14:02:30 INFO common.Util: Assuming 'file' scheme for path /dfs/nn in configuration.
    21/02/21 14:02:30 INFO common.Util: Assuming 'file' scheme for path /dfs/nn in configuration.
    21/02/21 14:02:30 INFO namenode.FSEditLog: Edit logging is async:true
    21/02/21 14:02:30 INFO namenode.TransferFsImage: Opening connection to http://peter-kafka001:50070/imagetransfer?getimage=1&txid=0&storageInfo=-63:190723722:1613882154343:CID-a94e54b2-0de8-45d7-b88f-7009b166327a&bootstrapstandby=true
    21/02/21 14:02:30 INFO common.Util: Combined time for fsimage download and fsync to all disks took 0.00s. The fsimage download took 0.00s at 0.00 KB/s. Synchronous (fsync) write to disk of /dfs/nn/current/fsimage.ckpt_0000000000000000000 took 0.00s.
    21/02/21 14:02:30 INFO namenode.TransferFsImage: Downloaded file fsimage.ckpt_0000000000000000000 size 322 bytes.
    21/02/21 14:02:30 INFO namenode.NameNode: SHUTDOWN_MSG: 
    /************************************************************
    SHUTDOWN_MSG: Shutting down NameNode at peter-kafka002/192.168.126.72
    ************************************************************/
    [hdfs@peter-kafka002 hadoop]$ 

    4. Secondary Namenode 실행: in peter-kafka002
    # su - hdfs
    $ cd /opt/hadoop
    $ sbin/hadoop-daemon.sh start namenode

    [hdfs@peter-kafka002 opt]$ cd /opt/hadoop
    [hdfs@peter-kafka002 hadoop]$ sbin/hadoop-daemon.sh start namenode
    starting namenode, logging to /opt/hadoop-2.10.1/logs/hadoop-hdfs-namenode-peter-kafka002.out
    [hdfs@peter-kafka002 hadoop]$ jps
    54516 NameNode            # <--- 실행후 추가됨.
    54670 Jps
    [hdfs@peter-kafka002 hadoop]$ 

5. 주키퍼 장애 컨트롤러(ZKFC) : in peter-kafka001, peter-kafka002
    # su - hdfs
    $ cd /opt/hadoop
    $ sbin/hadoop-daemon.sh start zkfc

    [hdfs@peter-kafka001 hadoop]$ cd /opt/hadoop
    [hdfs@peter-kafka001 hadoop]$ sbin/hadoop-daemon.sh start zkfc
    starting zkfc, logging to /opt/hadoop-2.10.1/logs/hadoop-hdfs-zkfc-peter-kafka001.out
    [hdfs@peter-kafka001 hadoop]$ jps
    5649 NameNode
    7864 DFSZKFailoverController
    7935 Jps
    [hdfs@peter-kafka001 hadoop]$ 

    [hdfs@peter-kafka002 hadoop]$ cd /opt/hadoop
    [hdfs@peter-kafka002 hadoop]$ sbin/hadoop-daemon.sh start zkfc
    starting zkfc, logging to /opt/hadoop-2.10.1/logs/hadoop-hdfs-zkfc-peter-kafka002.out
    [hdfs@peter-kafka002 hadoop]$ jps
    54786 Jps
    54516 NameNode
    54733 DFSZKFailoverController
    [hdfs@peter-kafka002 hadoop]$ 


o ResourceManger : in peter-kafka001
    # su - yarn
    $ cd /opt/hadoop
    $ sbin/yarn-daemon.sh start resourcemanager

    [root@peter-kafka001 ~]# su - yarn
    [yarn@peter-kafka001 opt]$ cd /opt/hadoop
    [yarn@peter-kafka001 hadoop]$ sbin/yarn-daemon.sh start resourcemanager
    starting resourcemanager, logging to /opt/hadoop-2.10.1/logs/yarn-yarn-resourcemanager-peter-kafka001.out
    [yarn@peter-kafka001 hadoop]$ jps
    6249 Jps
    6011 ResourceManager            # <--- 실행후 추가됨.
    5292 JournalNode
    [yarn@peter-kafka001 hadoop]$ 

o 여기까지 진행시 프로세스 확인
    [root@peter-kafka001 ~]# jps
    5649 NameNode
    6306 Jps
    2102 Kafka
    1560 -- process information unavailable
    1546 QuorumPeerMain
    6011 ResourceManager
    5292 JournalNode
    [root@peter-kafka001 ~]# 

    [root@peter-kafka002 ~]# jps
    54004 Jps
    53768 JournalNode
    2842 Kafka
    1535 QuorumPeerMain
    [root@peter-kafka002 ~]# 

    [root@peter-kafka003 ~]# jps
    55801 JournalNode
    2827 Kafka
    55932 Jps
    1534 QuorumPeerMain
    [root@peter-kafka003 ~]# 

    NOTE. 현재 DataNode는 실행하지 않음.
          hdfs, yarn, mapred 계정으로 실행하므로 root계정에서 jps로 전체프로세스 확인 가능

o JobHistoryServer : in peter-kafka003
    # su - mapred
    $ cd /opt/hadoop
    $ sbin/mr-jobhistory-daemon.sh start historyserver

    [root@peter-kafka001 ~]# su - mapred
    [mapred@peter-kafka001 ~]$ cd /opt/hadoop
    [mapred@peter-kafka001 hadoop]$ sbin/mr-jobhistory-daemon.sh start historyserver
    starting historyserver, logging to /opt/hadoop-2.10.1/logs/mapred-mapred-historyserver-peter-kafka001.out
    [mapred@peter-kafka001 hadoop]$ jps
    6530 Jps
    6487 JobHistoryServer            # <--- 실행후 추가됨.
    [mapred@peter-kafka001 hadoop]$ 

o DataNode : in peter-kafka001, peter-kafka002, peter-kafka003
    # su - hdfs
    $ cd /opt/hadoop
    $ sbin/hadoop-daemon.sh start datanode

    [hdfs@peter-kafka001 ~]$ cd /opt/hadoop
    [hdfs@peter-kafka001 hadoop]$ sbin/hadoop-daemon.sh start datanode
    starting datanode, logging to /opt/hadoop-2.10.1/logs/hadoop-hdfs-datanode-peter-kafka001.out
    [hdfs@peter-kafka001 hadoop]$ jps
    5649 NameNode
    7170 DataNode            # <--- 실행후 추가됨.
    7447 Jps
    [hdfs@peter-kafka001 hadoop]$ 

    [hdfs@peter-kafka002 ~]$ cd /opt/hadoop
    [hdfs@peter-kafka002 hadoop]$ sbin/hadoop-daemon.sh start datanode
    starting datanode, logging to /opt/hadoop-2.10.1/logs/hadoop-hdfs-datanode-peter-kafka002.out
    [hdfs@peter-kafka002 hadoop]$ jps
    54387 Jps
    54110 DataNode            # <--- 실행후 추가됨.

    [hdfs@peter-kafka002 hadoop]$ 

    [hdfs@peter-kafka003 ~]$ cd /opt/hadoop
    [hdfs@peter-kafka003 hadoop]$ sbin/hadoop-daemon.sh start datanode
    starting datanode, logging to /opt/hadoop-2.10.1/logs/hadoop-hdfs-datanode-peter-kafka003.out
    [hdfs@peter-kafka003 hadoop]$ jps
    56039 DataNode            # <--- 실행후 추가됨.
    56316 Jps
    [hdfs@peter-kafka003 hadoop]$ 

o Hadoop 중지 후 재시작
    CAUTION. hdfs, yarn, mapred 계정으로 각각 실행하므로 아래의 스크립트를 이용한 실행은 하면 안됨.
    @@@stop-all.sh
    @@@start-all.sh

    1. 중지
    ref) https://m.blog.naver.com/lionlyloveil/220777609903
        historyserver, yarn, zkfc, namenode, datanode, journalnode 순으로 종료 후,
        역순으로 기동.
        zookeeper 프로세스는 내리지 않고 계속 기동상태로 놔뒀다.

    (in peter-kafka001)
    # su - mapred -c "/opt/hadoop/sbin/mr-jobhistory-daemon.sh stop historyserver"
    # su - yarn -c "/opt/hadoop/sbin/stop-yarn.sh"
    (# su - hdfs -c "sbin/hadoop-daemon.sh stop zkfc")
    # su - hdfs -c "/opt/hadoop/sbin/stop-dfs.sh"
    # su - yarn -c "/opt/hadoop/sbin/hadoop-daemon.sh stop journalnode"
    # su - yarn -c "ssh yarn@peter-kafka002 \"/opt/hadoop/sbin/hadoop-daemon.sh stop journalnode\""
    # su - yarn -c "ssh yarn@peter-kafka003 \"/opt/hadoop/sbin/hadoop-daemon.sh stop journalnode\""

    2. 실행
    (in peter-kafka001)
    # su - yarn -c "/opt/hadoop/sbin/hadoop-daemon.sh start journalnode"
    # su - yarn -c "ssh yarn@peter-kafka002 \"/opt/hadoop/sbin/hadoop-daemon.sh start journalnode\""
    # su - yarn -c "ssh yarn@peter-kafka003 \"/opt/hadoop/sbin/hadoop-daemon.sh start journalnode\""
    # su - hdfs -c "/opt/hadoop/sbin/start-dfs.sh"
    (# su - hdfs -c "/opt/hadoop/sbin/hadoop-daemon.sh start zkfc")
    # su - yarn -c "/opt/hadoop/sbin/start-yarn.sh"
    # su - mapred -c "/opt/hadoop/sbin/mr-jobhistory-daemon.sh start historyserver"

    <프로세스 확인>
    [root@peter-kafka001 ~]# jps
    12965 NodeManager                 # <--- 
    2102 Kafka
    12855 ResourceManager             # <--- 
    12712 DFSZKFailoverController     # <--- 
    1546 QuorumPeerMain
    11996 NameNode                    # <--- 
    12124 DataNode                    # <--- 
    9692 RunNiFi
    11757 JournalNode                 # <--- 
    9709 NiFi
    13582 Jps
    [root@peter-kafka001 ~]# 

    [root@peter-kafka002 ~]# jps
    57537 DFSZKFailoverController     # <--- 
    57636 NodeManager                 # <--- 
    56933 JournalNode                 # <--- 
    57129 DataNode                    # <--- 
    2842 Kafka
    57037 NameNode                    # <--- 
    55646 RunNiFi
    1535 QuorumPeerMain
    55663 NiFi
    57871 Jps
    [root@peter-kafka002 ~]# 

    [root@peter-kafka003 ~]# jps
    58866 Jps
    58375 DataNode                    # <--- 
    2827 Kafka
    57131 RunNiFi
    57148 NiFi
    58269 JournalNode                 # <--- 
    1534 QuorumPeerMain
    58703 NodeManager                 # <--- 
    [root@peter-kafka003 ~]# 

o Hadoop 테스트 (wordcount)
    1. 디렉토리 구성
    # su - hdfs
    $ hadoop fs -ls /
    $ hadoop fs -mkdir /user/
    $ hadoop fs -mkdir /user/hdfs/
    $ hadoop fs -mkdir /user/hdfs/work/
    $ hadoop fs -mkdir /user/hdfs/work/wordcount/
    $ hadoop fs -mkdir /user/hdfs/work/wordcount/wc-in/

    2. 테스트 데이터를 HDFS에 업로드: in peter-kafka001 (test용도)
    # su - hdfs
    $ mkdir -p ~/work/wordcount/wc-in/
    $ cd ~/work/wordcount/wc-in/
    $ echo "bla 한글 bla" > a.txt
    $ echo "한글 wa bla wa" > b.txt
    $ echo "Hello 한글" > c.txt

    $ hadoop fs  -put *.txt /user/hdfs/work/wordcount/wc-in/.

    3. 실행
    $ hadoop jar $HADOOP_HOME/share/hadoop/mapreduce/hadoop-mapreduce-examples-2.10.1.jar wordcount work/wordcount/wc-in/ work/wordcount/wc-out/

    [hdfs@peter-kafka001 ~]$ hadoop jar $HADOOP_HOME/share/hadoop/mapreduce/hadoop-mapreduce-examples-2.10.1.jar wordcount work/wordcount/wc-in/ work/wordcount/wc-out/
    21/02/21 15:30:46 INFO Configuration.deprecation: session.id is deprecated. Instead, use dfs.metrics.session-id
    21/02/21 15:30:46 INFO jvm.JvmMetrics: Initializing JVM Metrics with processName=JobTracker, sessionId=
    21/02/21 15:30:46 INFO input.FileInputFormat: Total input files to process : 3
    21/02/21 15:30:46 INFO mapreduce.JobSubmitter: number of splits:3
    21/02/21 15:30:46 INFO mapreduce.JobSubmitter: Submitting tokens for job: job_local1133098537_0001
    21/02/21 15:30:46 INFO mapreduce.Job: The url to track the job: http://localhost:8080/
    21/02/21 15:30:46 INFO mapreduce.Job: Running job: job_local1133098537_0001
    21/02/21 15:30:46 INFO mapred.LocalJobRunner: OutputCommitter set in config null
    21/02/21 15:30:46 INFO output.FileOutputCommitter: File Output Committer Algorithm version is 1
    21/02/21 15:30:46 INFO output.FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
    21/02/21 15:30:46 INFO mapred.LocalJobRunner: OutputCommitter is org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter
    21/02/21 15:30:46 INFO mapred.LocalJobRunner: Waiting for map tasks
    21/02/21 15:30:46 INFO mapred.LocalJobRunner: Starting task: attempt_local1133098537_0001_m_000000_0
    21/02/21 15:30:46 INFO output.FileOutputCommitter: File Output Committer Algorithm version is 1
    21/02/21 15:30:46 INFO output.FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
    21/02/21 15:30:46 INFO mapred.Task:  Using ResourceCalculatorProcessTree : [ ]
    21/02/21 15:30:46 INFO mapred.MapTask: Processing split: hdfs://peter-cluster/user/hdfs/work/wordcount/wc-in/b.txt:0+17
    21/02/21 15:30:47 INFO mapred.MapTask: (EQUATOR) 0 kvi 26214396(104857584)
    21/02/21 15:30:47 INFO mapred.MapTask: mapreduce.task.io.sort.mb: 100
    21/02/21 15:30:47 INFO mapred.MapTask: soft limit at 83886080
    21/02/21 15:30:47 INFO mapred.MapTask: bufstart = 0; bufvoid = 104857600
    21/02/21 15:30:47 INFO mapred.MapTask: kvstart = 26214396; length = 6553600
    21/02/21 15:30:47 INFO mapred.MapTask: Map output collector class = org.apache.hadoop.mapred.MapTask$MapOutputBuffer
    21/02/21 15:30:47 INFO mapred.LocalJobRunner: 
    21/02/21 15:30:47 INFO mapred.MapTask: Starting flush of map output
    21/02/21 15:30:47 INFO mapred.MapTask: Spilling map output
    21/02/21 15:30:47 INFO mapred.MapTask: bufstart = 0; bufend = 33; bufvoid = 104857600
    21/02/21 15:30:47 INFO mapred.MapTask: kvstart = 26214396(104857584); kvend = 26214384(104857536); length = 13/6553600
    21/02/21 15:30:47 INFO mapred.MapTask: Finished spill 0
    21/02/21 15:30:47 INFO mapred.Task: Task:attempt_local1133098537_0001_m_000000_0 is done. And is in the process of committing
    21/02/21 15:30:47 INFO mapred.LocalJobRunner: map
    21/02/21 15:30:47 INFO mapred.Task: Task 'attempt_local1133098537_0001_m_000000_0' done.
    21/02/21 15:30:47 INFO mapred.Task: Final Counters for attempt_local1133098537_0001_m_000000_0: Counters: 23
        File System Counters
            FILE: Number of bytes read=303832
            FILE: Number of bytes written=804103
            FILE: Number of read operations=0
            FILE: Number of large read operations=0
            FILE: Number of write operations=0
            HDFS: Number of bytes read=17
            HDFS: Number of bytes written=0
            HDFS: Number of read operations=5
            HDFS: Number of large read operations=0
            HDFS: Number of write operations=1
        Map-Reduce Framework
            Map input records=1
            Map output records=4
            Map output bytes=33
            Map output materialized bytes=38
            Input split bytes=122
            Combine input records=4
            Combine output records=3
            Spilled Records=3
            Failed Shuffles=0
            Merged Map outputs=0
            GC time elapsed (ms)=0
            Total committed heap usage (bytes)=245366784
        File Input Format Counters 
            Bytes Read=17
    21/02/21 15:30:47 INFO mapred.LocalJobRunner: Finishing task: attempt_local1133098537_0001_m_000000_0
    21/02/21 15:30:47 INFO mapred.LocalJobRunner: Starting task: attempt_local1133098537_0001_m_000001_0
    21/02/21 15:30:47 INFO output.FileOutputCommitter: File Output Committer Algorithm version is 1
    21/02/21 15:30:47 INFO output.FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
    21/02/21 15:30:47 INFO mapred.Task:  Using ResourceCalculatorProcessTree : [ ]
    21/02/21 15:30:47 INFO mapred.MapTask: Processing split: hdfs://peter-cluster/user/hdfs/work/wordcount/wc-in/a.txt:0+15
    21/02/21 15:30:47 INFO mapred.MapTask: (EQUATOR) 0 kvi 26214396(104857584)
    21/02/21 15:30:47 INFO mapred.MapTask: mapreduce.task.io.sort.mb: 100
    21/02/21 15:30:47 INFO mapred.MapTask: soft limit at 83886080
    21/02/21 15:30:47 INFO mapred.MapTask: bufstart = 0; bufvoid = 104857600
    21/02/21 15:30:47 INFO mapred.MapTask: kvstart = 26214396; length = 6553600
    21/02/21 15:30:47 INFO mapred.MapTask: Map output collector class = org.apache.hadoop.mapred.MapTask$MapOutputBuffer
    21/02/21 15:30:47 INFO mapred.LocalJobRunner: 
    21/02/21 15:30:47 INFO mapred.MapTask: Starting flush of map output
    21/02/21 15:30:47 INFO mapred.MapTask: Spilling map output
    21/02/21 15:30:47 INFO mapred.MapTask: bufstart = 0; bufend = 27; bufvoid = 104857600
    21/02/21 15:30:47 INFO mapred.MapTask: kvstart = 26214396(104857584); kvend = 26214388(104857552); length = 9/6553600
    21/02/21 15:30:47 INFO mapred.MapTask: Finished spill 0
    21/02/21 15:30:47 INFO mapred.Task: Task:attempt_local1133098537_0001_m_000001_0 is done. And is in the process of committing
    21/02/21 15:30:47 INFO mapred.LocalJobRunner: map
    21/02/21 15:30:47 INFO mapred.Task: Task 'attempt_local1133098537_0001_m_000001_0' done.
    21/02/21 15:30:47 INFO mapred.Task: Final Counters for attempt_local1133098537_0001_m_000001_0: Counters: 23
        File System Counters
            FILE: Number of bytes read=304217
            FILE: Number of bytes written=804164
            FILE: Number of read operations=0
            FILE: Number of large read operations=0
            FILE: Number of write operations=0
            HDFS: Number of bytes read=32
            HDFS: Number of bytes written=0
            HDFS: Number of read operations=7
            HDFS: Number of large read operations=0
            HDFS: Number of write operations=1
        Map-Reduce Framework
            Map input records=1
            Map output records=3
            Map output bytes=27
            Map output materialized bytes=29
            Input split bytes=122
            Combine input records=3
            Combine output records=2
            Spilled Records=2
            Failed Shuffles=0
            Merged Map outputs=0
            GC time elapsed (ms)=0
            Total committed heap usage (bytes)=350748672
        File Input Format Counters 
            Bytes Read=15
    21/02/21 15:30:47 INFO mapred.LocalJobRunner: Finishing task: attempt_local1133098537_0001_m_000001_0
    21/02/21 15:30:47 INFO mapred.LocalJobRunner: Starting task: attempt_local1133098537_0001_m_000002_0
    21/02/21 15:30:47 INFO output.FileOutputCommitter: File Output Committer Algorithm version is 1
    21/02/21 15:30:47 INFO output.FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
    21/02/21 15:30:47 INFO mapred.Task:  Using ResourceCalculatorProcessTree : [ ]
    21/02/21 15:30:47 INFO mapred.MapTask: Processing split: hdfs://peter-cluster/user/hdfs/work/wordcount/wc-in/c.txt:0+13
    21/02/21 15:30:47 INFO mapred.MapTask: (EQUATOR) 0 kvi 26214396(104857584)
    21/02/21 15:30:47 INFO mapred.MapTask: mapreduce.task.io.sort.mb: 100
    21/02/21 15:30:47 INFO mapred.MapTask: soft limit at 83886080
    21/02/21 15:30:47 INFO mapred.MapTask: bufstart = 0; bufvoid = 104857600
    21/02/21 15:30:47 INFO mapred.MapTask: kvstart = 26214396; length = 6553600
    21/02/21 15:30:47 INFO mapred.MapTask: Map output collector class = org.apache.hadoop.mapred.MapTask$MapOutputBuffer
    21/02/21 15:30:47 INFO mapred.LocalJobRunner: 
    21/02/21 15:30:47 INFO mapred.MapTask: Starting flush of map output
    21/02/21 15:30:47 INFO mapred.MapTask: Spilling map output
    21/02/21 15:30:47 INFO mapred.MapTask: bufstart = 0; bufend = 21; bufvoid = 104857600
    21/02/21 15:30:47 INFO mapred.MapTask: kvstart = 26214396(104857584); kvend = 26214392(104857568); length = 5/6553600
    21/02/21 15:30:47 INFO mapred.MapTask: Finished spill 0
    21/02/21 15:30:47 INFO mapred.Task: Task:attempt_local1133098537_0001_m_000002_0 is done. And is in the process of committing
    21/02/21 15:30:47 INFO mapred.LocalJobRunner: map
    21/02/21 15:30:47 INFO mapred.Task: Task 'attempt_local1133098537_0001_m_000002_0' done.
    21/02/21 15:30:47 INFO mapred.Task: Final Counters for attempt_local1133098537_0001_m_000002_0: Counters: 23
        File System Counters
            FILE: Number of bytes read=304602
            FILE: Number of bytes written=804227
            FILE: Number of read operations=0
            FILE: Number of large read operations=0
            FILE: Number of write operations=0
            HDFS: Number of bytes read=45
            HDFS: Number of bytes written=0
            HDFS: Number of read operations=9
            HDFS: Number of large read operations=0
            HDFS: Number of write operations=1
        Map-Reduce Framework
            Map input records=1
            Map output records=2
            Map output bytes=21
            Map output materialized bytes=31
            Input split bytes=122
            Combine input records=2
            Combine output records=2
            Spilled Records=2
            Failed Shuffles=0
            Merged Map outputs=0
            GC time elapsed (ms)=0
            Total committed heap usage (bytes)=456130560
        File Input Format Counters 
            Bytes Read=13
    21/02/21 15:30:47 INFO mapred.LocalJobRunner: Finishing task: attempt_local1133098537_0001_m_000002_0
    21/02/21 15:30:47 INFO mapred.LocalJobRunner: map task executor complete.
    21/02/21 15:30:47 INFO mapred.LocalJobRunner: Waiting for reduce tasks
    21/02/21 15:30:47 INFO mapred.LocalJobRunner: Starting task: attempt_local1133098537_0001_r_000000_0
    21/02/21 15:30:47 INFO output.FileOutputCommitter: File Output Committer Algorithm version is 1
    21/02/21 15:30:47 INFO output.FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
    21/02/21 15:30:47 INFO mapred.Task:  Using ResourceCalculatorProcessTree : [ ]
    21/02/21 15:30:47 INFO mapred.ReduceTask: Using ShuffleConsumerPlugin: org.apache.hadoop.mapreduce.task.reduce.Shuffle@3d2dd917
    21/02/21 15:30:47 INFO reduce.MergeManagerImpl: MergerManager: memoryLimit=334338464, maxSingleShuffleLimit=83584616, mergeThreshold=220663392, ioSortFactor=10, memToMemMergeOutputsThreshold=10
    21/02/21 15:30:47 INFO reduce.EventFetcher: attempt_local1133098537_0001_r_000000_0 Thread started: EventFetcher for fetching Map Completion Events
    21/02/21 15:30:47 INFO reduce.LocalFetcher: localfetcher#1 about to shuffle output of map attempt_local1133098537_0001_m_000001_0 decomp: 25 len: 29 to MEMORY
    21/02/21 15:30:47 INFO reduce.InMemoryMapOutput: Read 25 bytes from map-output for attempt_local1133098537_0001_m_000001_0
    21/02/21 15:30:47 INFO reduce.MergeManagerImpl: closeInMemoryFile -> map-output of size: 25, inMemoryMapOutputs.size() -> 1, commitMemory -> 0, usedMemory ->25
    21/02/21 15:30:47 INFO reduce.LocalFetcher: localfetcher#1 about to shuffle output of map attempt_local1133098537_0001_m_000000_0 decomp: 34 len: 38 to MEMORY
    21/02/21 15:30:47 INFO reduce.InMemoryMapOutput: Read 34 bytes from map-output for attempt_local1133098537_0001_m_000000_0
    21/02/21 15:30:47 INFO reduce.MergeManagerImpl: closeInMemoryFile -> map-output of size: 34, inMemoryMapOutputs.size() -> 2, commitMemory -> 25, usedMemory ->59
    21/02/21 15:30:47 INFO reduce.LocalFetcher: localfetcher#1 about to shuffle output of map attempt_local1133098537_0001_m_000002_0 decomp: 27 len: 31 to MEMORY
    21/02/21 15:30:47 INFO reduce.InMemoryMapOutput: Read 27 bytes from map-output for attempt_local1133098537_0001_m_000002_0
    21/02/21 15:30:47 INFO reduce.MergeManagerImpl: closeInMemoryFile -> map-output of size: 27, inMemoryMapOutputs.size() -> 3, commitMemory -> 59, usedMemory ->86
    21/02/21 15:30:47 INFO reduce.EventFetcher: EventFetcher is interrupted.. Returning
    21/02/21 15:30:47 WARN io.ReadaheadPool: Failed readahead on ifile
    EBADF: Bad file descriptor
        at org.apache.hadoop.io.nativeio.NativeIO$POSIX.posix_fadvise(Native Method)
        at org.apache.hadoop.io.nativeio.NativeIO$POSIX.posixFadviseIfPossible(NativeIO.java:267)
        at org.apache.hadoop.io.nativeio.NativeIO$POSIX$CacheManipulator.posixFadviseIfPossible(NativeIO.java:146)
        at org.apache.hadoop.io.ReadaheadPool$ReadaheadRequestImpl.run(ReadaheadPool.java:208)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
        at java.lang.Thread.run(Thread.java:748)
    21/02/21 15:30:47 INFO mapred.LocalJobRunner: 3 / 3 copied.
    21/02/21 15:30:47 INFO reduce.MergeManagerImpl: finalMerge called with 3 in-memory map-outputs and 0 on-disk map-outputs
    21/02/21 15:30:47 INFO mapred.Merger: Merging 3 sorted segments
    21/02/21 15:30:47 INFO mapred.Merger: Down to the last merge-pass, with 3 segments left of total size: 66 bytes
    21/02/21 15:30:47 INFO reduce.MergeManagerImpl: Merged 3 segments, 86 bytes to disk to satisfy reduce memory limit
    21/02/21 15:30:47 INFO reduce.MergeManagerImpl: Merging 1 files, 86 bytes from disk
    21/02/21 15:30:47 INFO reduce.MergeManagerImpl: Merging 0 segments, 0 bytes from memory into reduce
    21/02/21 15:30:47 INFO mapred.Merger: Merging 1 sorted segments
    21/02/21 15:30:47 INFO mapred.Merger: Down to the last merge-pass, with 1 segments left of total size: 74 bytes
    21/02/21 15:30:47 INFO mapred.LocalJobRunner: 3 / 3 copied.
    21/02/21 15:30:47 INFO Configuration.deprecation: mapred.skip.on is deprecated. Instead, use mapreduce.job.skiprecords
    21/02/21 15:30:47 INFO mapreduce.Job: Job job_local1133098537_0001 running in uber mode : false
    21/02/21 15:30:47 INFO mapreduce.Job:  map 100% reduce 0%
    21/02/21 15:30:48 INFO mapred.Task: Task:attempt_local1133098537_0001_r_000000_0 is done. And is in the process of committing
    21/02/21 15:30:48 INFO mapred.LocalJobRunner: 3 / 3 copied.
    21/02/21 15:30:48 INFO mapred.Task: Task attempt_local1133098537_0001_r_000000_0 is allowed to commit now
    21/02/21 15:30:48 INFO output.FileOutputCommitter: Saved output of task 'attempt_local1133098537_0001_r_000000_0' to hdfs://peter-cluster/user/hdfs/work/wordcount/wc-out/_temporary/0/task_local1133098537_0001_r_000000
    21/02/21 15:30:48 INFO mapred.LocalJobRunner: reduce > reduce
    21/02/21 15:30:48 INFO mapred.Task: Task 'attempt_local1133098537_0001_r_000000_0' done.
    21/02/21 15:30:48 INFO mapred.Task: Final Counters for attempt_local1133098537_0001_r_000000_0: Counters: 29
        File System Counters
            FILE: Number of bytes read=304882
            FILE: Number of bytes written=804313
            FILE: Number of read operations=0
            FILE: Number of large read operations=0
            FILE: Number of write operations=0
            HDFS: Number of bytes read=45
            HDFS: Number of bytes written=28
            HDFS: Number of read operations=12
            HDFS: Number of large read operations=0
            HDFS: Number of write operations=3
        Map-Reduce Framework
            Combine input records=0
            Combine output records=0
            Reduce input groups=4
            Reduce shuffle bytes=98
            Reduce input records=7
            Reduce output records=4
            Spilled Records=7
            Shuffled Maps =3
            Failed Shuffles=0
            Merged Map outputs=3
            GC time elapsed (ms)=8
            Total committed heap usage (bytes)=456130560
        Shuffle Errors
            BAD_ID=0
            CONNECTION=0
            IO_ERROR=0
            WRONG_LENGTH=0
            WRONG_MAP=0
            WRONG_REDUCE=0
        File Output Format Counters 
            Bytes Written=28
    21/02/21 15:30:48 INFO mapred.LocalJobRunner: Finishing task: attempt_local1133098537_0001_r_000000_0
    21/02/21 15:30:48 INFO mapred.LocalJobRunner: reduce task executor complete.
    21/02/21 15:30:48 INFO mapreduce.Job:  map 100% reduce 100%
    21/02/21 15:30:48 INFO mapreduce.Job: Job job_local1133098537_0001 completed successfully
    21/02/21 15:30:48 INFO mapreduce.Job: Counters: 35
        File System Counters
            FILE: Number of bytes read=1217533
            FILE: Number of bytes written=3216807
            FILE: Number of read operations=0
            FILE: Number of large read operations=0
            FILE: Number of write operations=0
            HDFS: Number of bytes read=139
            HDFS: Number of bytes written=28
            HDFS: Number of read operations=33
            HDFS: Number of large read operations=0
            HDFS: Number of write operations=6
        Map-Reduce Framework
            Map input records=3
            Map output records=9
            Map output bytes=81
            Map output materialized bytes=98
            Input split bytes=366
            Combine input records=9
            Combine output records=7
            Reduce input groups=4
            Reduce shuffle bytes=98
            Reduce input records=7
            Reduce output records=4
            Spilled Records=14
            Shuffled Maps =3
            Failed Shuffles=0
            Merged Map outputs=3
            GC time elapsed (ms)=8
            Total committed heap usage (bytes)=1508376576
        Shuffle Errors
            BAD_ID=0
            CONNECTION=0
            IO_ERROR=0
            WRONG_LENGTH=0
            WRONG_MAP=0
            WRONG_REDUCE=0
        File Input Format Counters 
            Bytes Read=45
        File Output Format Counters 
            Bytes Written=28
    [hdfs@peter-kafka001 ~]$ hadoop fs -ls  /user/hdfs/work/wordcount/wc-out/
    Found 2 items
    -rw-r--r--   2 hdfs supergroup          0 2021-02-21 15:30 /user/hdfs/work/wordcount/wc-out/_SUCCESS
    -rw-r--r--   2 hdfs supergroup         28 2021-02-21 15:30 /user/hdfs/work/wordcount/wc-out/part-r-00000
    [hdfs@peter-kafka001 ~]$ hadoop fs -cat /user/hdfs/work/wordcount/wc-out/part-r-00000
    Hello   1
    bla     3
    wa      2
    한글    3
    [hdfs@peter-kafka001 ~]$ 

    4. 확인 (in WEB)
    http://peter-kafka001:50070/ : Namenode info
    http://peter-kafka001:8088/  : 

.END OF HADOOP

MariaDB 설치
============
ref) https://downloads.mariadb.org/mariadb/repositories/#mirror=yongbok
     https://bamdule.tistory.com/66

1. 계정 추가
# groupadd -g 3306 mysql
# useradd mysql -u 3306 -g mysql

2. 설치
(in peter-kafka001, peter-kafka002)
# vi /etc/yum.repos.d/MariaDB.repo
<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
# MariaDB 10.5 CentOS repository list - created 2021-02-27 05:30 UTC
# http://downloads.mariadb.org/mariadb/repositories/
[mariadb]
name = MariaDB
baseurl = http://yum.mariadb.org/10.5/centos7-amd64
gpgkey=https://yum.mariadb.org/RPM-GPG-KEY-MariaDB
gpgcheck=1
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>

# yum install galera-4
# yum install MariaDB-server MariaDB-client

3. 설정
# vi /etc/my.cnf
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
[mysql]
default-character-set = utf8

[mysqld]
datadir=/var/lib/mysql
socket=/var/lib/mysql/mysql.sock
transaction-isolation = READ-COMMITTED
# Disabling symbolic-links is recommended to prevent assorted security risks;
# to do so, uncomment this line:
symbolic-links = 0
# Settings user and group are ignored when systemd is used.
# If you need to run mysqld under a different user or group,
# customize your systemd unit file for mariadb according to the
# instructions in http://fedoraproject.org/wiki/Systemd

key_buffer = 32M
# (deprecated) key_buffer_size = 32M
max_allowed_packet = 32M
thread_stack = 256K
thread_cache_size = 64
query_cache_limit = 8M
query_cache_size = 64M
query_cache_type = 1

max_connections = 550
#expire_logs_days = 10
#max_binlog_size = 100M

#log_bin should be on a disk with enough free space.
#Replace '/var/lib/mysql/mysql_binary_log' with an appropriate path for your
#system and chown the specified folder to the mysql user.
log_bin=/var/lib/mysql/mysql_binary_log

#In later versions of MariaDB, if you enable the binary log and do not set
#a server_id, MariaDB will not start. The server_id must be unique within
#the replicating group.
server_id=1

binlog_format = mixed

read_buffer_size = 2M
read_rnd_buffer_size = 16M
sort_buffer_size = 8M
join_buffer_size = 8M

# InnoDB settings
innodb_file_per_table = 1
innodb_flush_log_at_trx_commit  = 2
innodb_log_buffer_size = 64M
innodb_buffer_pool_size = 4G
innodb_thread_concurrency = 8
innodb_flush_method = O_DIRECT
innodb_log_file_size = 512M

##
init_connect = "SET collation_connection = utf8_general_ci"
init_connect = "SET NAMES utf8"
character-set-server = utf8
collation-server = utf8_general_ci

[client]
default-character-set = utf8

[mysqld_safe]
log-error=/var/log/mariadb/mariadb.log
pid-file=/var/run/mariadb/mariadb.pid

#
# include all files from the config directory
#
!includedir /etc/my.cnf.d
<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<

4. DB 초기화 작업
    (in peter-kafka001)
    # systemctl start mariadb
    # systemctl status mariadb
    # systemctl enable mariadb

    # /usr/bin/mysql_secure_installation
    <<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
    [root@peter-kafka001 my.cnf.d]# /usr/bin/mysql_secure_installation

    NOTE: RUNNING ALL PARTS OF THIS SCRIPT IS RECOMMENDED FOR ALL MariaDB
          SERVERS IN PRODUCTION USE!  PLEASE READ EACH STEP CAREFULLY!

    In order to log into MariaDB to secure it, we'll need the current
    password for the root user. If you've just installed MariaDB, and
    haven't set the root password yet, you should just press enter here.

    Enter current password for root (enter for none): 
    OK, successfully used password, moving on...

    Setting the root password or using the unix_socket ensures that nobody
    can log into the MariaDB root user without the proper authorisation.

    You already have your root account protected, so you can safely answer 'n'.

    Switch to unix_socket authentication [Y/n] n
     ... skipping.

    You already have your root account protected, so you can safely answer 'n'.

    Change the root password? [Y/n] y
    New password: 
    Re-enter new password: 
    Password updated successfully!
    Reloading privilege tables..
     ... Success!


    By default, a MariaDB installation has an anonymous user, allowing anyone
    to log into MariaDB without having to have a user account created for
    them.  This is intended only for testing, and to make the installation
    go a bit smoother.  You should remove them before moving into a
    production environment.

    Remove anonymous users? [Y/n] 
     ... Success!

    Normally, root should only be allowed to connect from 'localhost'.  This
    ensures that someone cannot guess at the root password from the network.

    Disallow root login remotely? [Y/n] n
     ... skipping.

    By default, MariaDB comes with a database named 'test' that anyone can
    access.  This is also intended only for testing, and should be removed
    before moving into a production environment.

    Remove test database and access to it? [Y/n] 
     - Dropping test database...
     ... Success!
     - Removing privileges on test database...
     ... Success!

    Reloading the privilege tables will ensure that all changes made so far
    will take effect immediately.

    Reload privilege tables now? [Y/n] 
     ... Success!

    Cleaning up...

    All done!  If you've completed all of the above steps, your MariaDB
    installation should now be secure.

    Thanks for using MariaDB!
    [root@peter-kafka001 my.cnf.d]# 
    >>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>

5. galera 설정
# vi /etc/my.cnf.d/server.cnf
NOTE. 서버별로 설정값이 다름.(wsrep_node_address)
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
#
# These groups are read by MariaDB server.
# Use it for options that only the server (but not clients) should see
#
# See the examples of server my.cnf files in /usr/share/mysql/
#

# this is read by the standalone daemon and embedded servers
[server]

# this is only for the mysqld standalone daemon
[mysqld]

#
# * Galera-related settings
#
[galera]
# Mandatory settings
wsrep_on=ON
wsrep_provider=/usr/lib64/galera-4/libgalera_smm.so
wsrep_cluster_address="gcomm://peter-kafka001,peter-kafka002"
binlog_format=row
default_storage_engine=InnoDB
innodb_autoinc_lock_mode=2
#
# Allow server to accept connections on all interfaces.
#
bind-address=0.0.0.0
#
# Optional setting
#wsrep_slave_threads=1
#innodb_flush_log_at_trx_commit=0

# Logical cluster name. Should be the same for all nodes.
#wsrep_cluster_name="mariadb_lightningdb_cluster" ## <-[ADD]
wsrep_cluster_name="SCcomz_cluster" ## <-[ADD]

# Human-readable node name (non-unique). Hostname by default.
#wsrep_node_name=

# Base replication <address|hostname>[:port] of the node.
# The values supplied will be used as defaults for state transfer receiving,
# listening ports and so on. Default: address of the first network interface.
wsrep_node_address=peter-kafka001 ## <-[ADD]

# this is only for embedded server
[embedded]

# This group is only read by MariaDB servers, not by MySQL.
# If you use the same .cnf file for MySQL and MariaDB,
# you can put MariaDB-only options here
[mariadb]

# This group is only read by MariaDB-10.5 servers.
# If you use the same .cnf file for MariaDB of different versions,
# use this group for options that older servers don't understand
[mariadb-10.5]

<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<

6.cluster 작업
    1) 설정
    (in peter-kafka001)
    # galera_new_cluster

    @@@(in peter-kafka001, peter-kafka002)
    @@@# mkdir /var/log/mariadb/
    @@@# mkdir /var/run/mariadb/
    @@@# chown mysql:mysql /var/log/mariadb/
    @@@# chown mysql:mysql /var/run/mariadb/
    @@@# /usr/bin/mysqld_safe --wsrep-new-cluster
    @@@# ps -ef|grep mariadb
    @@@mysql     33978  33720  2 16:00 pts/3    00:00:00 /usr/sbin/mariadbd
    @@@--basedir=/usr
    @@@--datadir=/var/lib/mysql
    @@@--plugin-dir=/usr/lib64/mysql/plugin
    @@@--user=mysql
    @@@--wsrep_on=ON
    @@@--wsrep_provider=/usr/lib64/galera-4/libgalera_smm.so
    @@@--wsrep-new-cluster
    @@@--log-error=/var/log/mariadb/mariadb.log
    @@@--pid-file=/var/run/mariadb/mariadb.pid
    @@@--socket=/var/lib/mysql/mysql.sock
    @@@--wsrep_start_position=00000000-0000-0000-0000-000000000000:-1

    (in peter-kafka002)
    # systemctl start mariadb
    # systemctl status mariadb
    # systemctl enable mariadb

7. 확인
    (in peter-kafka001 or peter-kafka002)
    # mysql -u root -p
    Enter password: root
    MariaDB [(none)]> show status like 'wsrep%';
    MariaDB [(none)]> show global status like 'wsrep_cluster_size';

    NOTE. Hive를 위한 DB및 계정 생성

    MariaDB [(none)]> show databases;
    MariaDB [(none)]> create database metastore DEFAULT CHARACTER SET utf8 DEFAULT COLLATE utf8_general_ci;
    MariaDB [(none)]> show databases;

    MariaDB [(none)]> select host,user,password from mysql.user;
    MariaDB [(none)]> grant all privileges on metastore.* to 'hive'@'%' identified by 'hive';
    MariaDB [(none)]> select host,user,password from mysql.user;

    MariaDB [(none)]> flush privileges;

    MariaDB [(none)]> exit

    NOTE. 동기화 되는지 확인

.END OF MARIADB

Hive 설치
=========
1. 계정 추가
    (in peter-kafka001, peter-kafka002)
    # groupadd -g 9083 hive
    # useradd hive -u 9083 -g hive

2. 설치
    (in peter-kafka001, peter-kafka002)
    # cd ~/work
    # wget https://downloads.apache.org/hive/hive-2.3.8/apache-hive-2.3.8-bin.tar.gz
    # cd /opt
    # tar zxvf ~/work/apache-hive-2.3.8-bin.tar.gz 
    # ln -s apache-hive-2.3.8-bin hive

3. 설정(for metastore 서비스)
    (in peter-kafka001, peter-kafka002)
    # cd /opt/hive/conf/
    # mkdir hive-metastoreserver
    # cd hive-metastoreserver
    # cp -p ../hive-log4j2.properties.template hive-log4j2.properties
    # cp -p ../hive-env.sh.template hive-env.sh

    1) hive-log4j2.properties
    # vi /opt/hive/conf/hive-metastoreserver/hive-log4j2.properties
    <<<<<<<<<<
    ...
    property.hive.log.dir = /var/log/hive                     # <- 로그 디렉토리 경로 변경
    property.hive.log.file = hive-metastoreserver.log         # <- 로그 파일명 변경
    ...
    # >>>> Delete log files older than x days
    appender.DRFA.strategy.action.type = Delete
    appender.DRFA.strategy.action.basepath = ${sys:hive.log.dir}
    appender.DRFA.strategy.action.maxDepth = 1
    appender.DRFA.strategy.action.condition.type = IfFileName
    appender.DRFA.strategy.action.condition.glob = ${sys:hive.log.file}.*
    appender.DRFA.strategy.action.IfAny.type = IfAny
    appender.DRFA.strategy.action.IfAny.IfLastModified.type = IfLastModified
    appender.DRFA.strategy.action.IfAny.IfLastModified.age = 7d
    # <<<<
    >>>>>>>>>>

    2) hive-env.sh
    # vi /opt/hive/conf/hive-metastoreserver/hive-env.sh
    <<<<<<<<<<
    ...
    export HIVE_AUX_JARS_PATH=${HIVE_HOME}/auxjars
    >>>>>>>>>>

    3) hive-site.xml
    # vi /opt/hive/conf/hive-metastoreserver/hive-site.xml
    <<<<<<<<<<
<?xml version="1.0"?>
<?xml-stylesheet type="text/xsl" href="configuration.xsl"?>

<configuration>
  <property>
    <name>hive.exec.stagingdir</name>
    <value>/tmp/hive-staging/.hive</value>
  </property>

  <!-- Metastore DB -->
  <property>
    <name>javax.jdo.option.ConnectionURL</name>
    <value>jdbc:mariadb://peter-kafka001:3306,peter-kafka002:3306/metastore</value>
  </property>
  <property>
    <name>javax.jdo.option.ConnectionDriverName</name>
    <value>org.mariadb.jdbc.Driver</value>
  </property>
  <property>
    <name>javax.jdo.option.ConnectionUserName</name>
    <value>hive</value> #<- DB접속계정
  </property>
  <property>
    <name>javax.jdo.option.ConnectionPassword</name>
    <value>hive</value> #<-DB접속 패스워드
  </property>
  <property>
    <name>datanucleus.connectionPoolingType</name>
    <value>HikariCP</value>
  </property>
  <property>
    <name>datanucleus.connectionPool.maxPoolSize</name>
    <value>50</value>
  </property>

  <!-- Authorization / Authentication -->
  <!--
  <property>
    <name>hive.users.in.admin.role</name>
    <value>hive</value>
  </property>
  <property>
    <name>hive.metastore.pre.event.listeners</name>
    <value>org.apache.hadoop.hive.ql.security.authorization.AuthorizationPreEventListener</value>
  </property>
  <property>
    <name>hive.security.metastore.authorization.auth.reads</name>
    <value>true</value>
  </property>
  <property>
    <name>hive.security.metastore.authorization.manager</name>
    <value>org.apache.hadoop.hive.ql.security.authorization.StorageBasedAuthorizationProvider</value>
  </property>
  <property>
    <name>hive.security.metastore.authenticator.manager</name>
    <value>org.apache.hadoop.hive.ql.security.HadoopDefaultMetastoreAuthenticator</value>
  </property>
  -->

  <!-- Metastore Server -->
  <property>
    <name>hive.metastore.execute.setugi</name>
    <value>true</value>
  </property>
  <property>
    <name>hive.metastore.schema.verification</name>
    <value>false</value>
  </property>
  <property>
    <name>hive.metastore.server.min.threads</name>
    <value>10</value>
    <description>Minimum number of worker threads in the Thrift server's pool.</description>
  </property>
  <property>
    <name>hive.metastore.server.max.threads</name>
    <value>500</value>
    <description>Maximum number of worker threads in the Thrift server's pool.</description>
  </property>

  <!-- etc... -->
  <property>
    <name>hive.resultset.use.unique.column.names</name>
    <value>false</value>
  </property>
</configuration>
    >>>>>>>>>>

    4) MariaDB JDBC Connector 설치
    # cd ~/work
    # wget https://downloads.mariadb.com/Connectors/java/connector-java-2.3.0/mariadb-java-client-2.3.0.jar
    @@@# wget https://downloads.mariadb.com/Connectors/java/connector-java-2.7.2/mariadb-java-client-2.7.2.jar (나중에 서비스 시작시에 Self-test query [select "DB_ID" from "DBS"] failed; direct SQL is disabled 에러 발생)
    # /opt/hive
    # mkdir auxjars
    # cd auxjars
    # cp -p ~/work/mariadb-java-client-2.3.0.jar .
    # ln -s mariadb-java-client-2.3.0.jar mariadb-java-client.jar

    5) 기타 디렉토리 생성
    # mkdir /var/log/hive

4. 실행 계정 설정
    (in peter-kafka001, peter-kafka002)
    # chown -R hive:hive /opt/apache-hive-2.3.8-bin/
    # chown -R hive:hive /opt/hive

5. Metastore DB 생성
   1) 생성
    (in peter-kafka001)
    # vi /opt/hive/conf/hive-site.xml
    NOTE. schema 생성을 위해서 임시로 hive-site.xml 생성
    <<<<<<<<<<<<<<<
<?xml version="1.0"?>
<?xml-stylesheet type="text/xsl" href="configuration.xsl"?>

<configuration>
  <!-- Metastore DB -->
  <property>
    <name>javax.jdo.option.ConnectionURL</name>
    <value>jdbc:mariadb://peter-kafka001:3306,peter-kafka002:3306/metastore</value>
  </property>
  <property>
    <name>javax.jdo.option.ConnectionDriverName</name>
    <value>org.mariadb.jdbc.Driver</value>
  </property>
  <property>
    <name>javax.jdo.option.ConnectionUserName</name>
    <value>hive</value>
  </property>
  <property>
    <name>javax.jdo.option.ConnectionPassword</name>
    <value>hive</value>
  </property>
</configuration>
    >>>>>>>>>>>>>>>
    # chown hive:hive /opt/hive/conf/hive-site.xml

    # cd /opt/hive/conf/
    # cp -p hive-metastoreserver/hive-env.sh .
    # chown hive:hive /opt/hive/conf/hive-env.sh
    NOTE. hive-env.sh 파일도 역시 임시로 생성

    # su - hive
    @@@$ hive --service schemaTool -dbType mysql -initSchema -userName hive -passWord hive
    $ hive --service schemaTool -dbType mysql -initSchema

    실행 예)
    [hive@peter-kafka001 ~]$ hive --service schemaTool -dbType mysql -initSchema
    SLF4J: Class path contains multiple SLF4J bindings.
    SLF4J: Found binding in [jar:file:/opt/apache-hive-2.3.8-bin/lib/log4j-slf4j-impl-2.6.2.jar!/org/slf4j/impl/StaticLoggerBinder.class]
    SLF4J: Found binding in [jar:file:/opt/hadoop-2.10.1/share/hadoop/common/lib/slf4j-log4j12-1.7.25.jar!/org/slf4j/impl/StaticLoggerBinder.class]
    SLF4J: See http://www.slf4j.org/codes.html#multiple_bindings for an explanation.
    SLF4J: Actual binding is of type [org.apache.logging.slf4j.Log4jLoggerFactory]
    Metastore connection URL:        jdbc:mariadb://peter-kafka001:3306,peter-kafka002:3306/metastore
    Metastore Connection Driver :    org.mariadb.jdbc.Driver
    Metastore connection User:       hive
    Starting metastore schema initialization to 2.3.0
    Initialization script hive-schema-2.3.0.mysql.sql
    Initialization script completed
    schemaTool completed

    $ exit
    # rm -f /opt/hive/conf/hive-site.xml
    # rm -f /opt/hive/conf/hive-env.sh

    2)확인
    (in peter-kafka001 or peter-kafka002)
    # mysql -u root -p
    Enter password: root
    MariaDB [(none)]> use metastore;
    MariaDB [(none)]> show tables;

    MariaDB [(none)]> exit

    NOTE. 동기화 되는지 확인

    3) mariadb 한글 깨짐 해결
    ref) https://heum-story.tistory.com/34
         https://www.lesstif.com/dbms/mysql-rhel-centos-ubuntu-20775198.html

    ##table coulmn character-set 변경
    (in peter-kafka001)
    # mysql -u root -p
    Enter password: root
    alter table COLUMNS_V2 modify COMMENT varchar(256) character set utf8 collate utf8_general_ci;
    alter table TABLE_PARAMS modify PARAM_VALUE mediumtext character set utf8 collate utf8_general_ci;
    alter table SERDE_PARAMS modify PARAM_VALUE mediumtext character set utf8 collate utf8_general_ci;
    alter table SD_PARAMS modify PARAM_VALUE mediumtext character set utf8 collate utf8_general_ci;
    alter table PARTITION_PARAMS modify PARAM_VALUE varchar(4000) character set utf8 collate utf8_general_ci;
    alter table PARTITION_KEYS modify PKEY_COMMENT varchar(4000) character set utf8 collate utf8_general_ci;
    alter table INDEX_PARAMS modify PARAM_VALUE varchar(4000) character set utf8 collate utf8_general_ci;
    alter table DATABASE_PARAMS modify PARAM_VALUE varchar(4000) character set utf8 collate utf8_general_ci;
    alter table DBS modify `DESC` varchar(4000) character set utf8 collate utf8_general_ci;

    ## table 확인
    show full columns from COLUMNS_V2;
    show full columns from TABLE_PARAMS;
    show full columns from SERDE_PARAMS;
    show full columns from SD_PARAMS;
    show full columns from PARTITION_PARAMS;
    show full columns from PARTITION_KEYS;
    show full columns from INDEX_PARAMS;
    show full columns from DATABASE_PARAMS;
    show full columns from DBS;

    실행예)
    MariaDB [metastore]>     show full columns from COLUMNS_V2;
    +-------------+--------------+-------------------+------+-----+---------+-------+---------------------------------+---------+
    | Field       | Type         | Collation         | Null | Key | Default | Extra | Privileges                      | Comment |
    +-------------+--------------+-------------------+------+-----+---------+-------+---------------------------------+---------+
    | CD_ID       | bigint(20)   | NULL              | NO   | PRI | NULL    |       | select,insert,update,references |         |
    | COMMENT     | varchar(256) | latin1_bin        | YES  |     | NULL    |       | select,insert,update,references |         |
    | COLUMN_NAME | varchar(767) | latin1_bin        | NO   | PRI | NULL    |       | select,insert,update,references |         |
    | TYPE_NAME   | mediumtext   | latin1_swedish_ci | YES  |     | NULL    |       | select,insert,update,references |         |
    | INTEGER_IDX | int(11)      | NULL              | NO   |     | NULL    |       | select,insert,update,references |         |
    +-------------+--------------+-------------------+------+-----+---------+-------+---------------------------------+---------+
    5 rows in set (0.001 sec)

    MariaDB [metastore]>     show full columns from TABLE_PARAMS;
    +-------------+--------------+------------+------+-----+---------+-------+---------------------------------+---------+
    | Field       | Type         | Collation  | Null | Key | Default | Extra | Privileges                      | Comment |
    +-------------+--------------+------------+------+-----+---------+-------+---------------------------------+---------+
    | TBL_ID      | bigint(20)   | NULL       | NO   | PRI | NULL    |       | select,insert,update,references |         |
    | PARAM_KEY   | varchar(256) | latin1_bin | NO   | PRI | NULL    |       | select,insert,update,references |         |
    | PARAM_VALUE | mediumtext   | latin1_bin | YES  |     | NULL    |       | select,insert,update,references |         |
    +-------------+--------------+------------+------+-----+---------+-------+---------------------------------+---------+
    3 rows in set (0.001 sec)

    MariaDB [metastore]>     show full columns from SERDE_PARAMS;
    +-------------+--------------+------------+------+-----+---------+-------+---------------------------------+---------+
    | Field       | Type         | Collation  | Null | Key | Default | Extra | Privileges                      | Comment |
    +-------------+--------------+------------+------+-----+---------+-------+---------------------------------+---------+
    | SERDE_ID    | bigint(20)   | NULL       | NO   | PRI | NULL    |       | select,insert,update,references |         |
    | PARAM_KEY   | varchar(256) | latin1_bin | NO   | PRI | NULL    |       | select,insert,update,references |         |
    | PARAM_VALUE | mediumtext   | latin1_bin | YES  |     | NULL    |       | select,insert,update,references |         |
    +-------------+--------------+------------+------+-----+---------+-------+---------------------------------+---------+
    3 rows in set (0.001 sec)

    MariaDB [metastore]>     show full columns from SD_PARAMS;
    +-------------+--------------+------------+------+-----+---------+-------+---------------------------------+---------+
    | Field       | Type         | Collation  | Null | Key | Default | Extra | Privileges                      | Comment |
    +-------------+--------------+------------+------+-----+---------+-------+---------------------------------+---------+
    | SD_ID       | bigint(20)   | NULL       | NO   | PRI | NULL    |       | select,insert,update,references |         |
    | PARAM_KEY   | varchar(256) | latin1_bin | NO   | PRI | NULL    |       | select,insert,update,references |         |
    | PARAM_VALUE | mediumtext   | latin1_bin | YES  |     | NULL    |       | select,insert,update,references |         |
    +-------------+--------------+------------+------+-----+---------+-------+---------------------------------+---------+
    3 rows in set (0.001 sec)

    MariaDB [metastore]>     show full columns from PARTITION_PARAMS;
    +-------------+---------------+------------+------+-----+---------+-------+---------------------------------+---------+
    | Field       | Type          | Collation  | Null | Key | Default | Extra | Privileges                      | Comment |
    +-------------+---------------+------------+------+-----+---------+-------+---------------------------------+---------+
    | PART_ID     | bigint(20)    | NULL       | NO   | PRI | NULL    |       | select,insert,update,references |         |
    | PARAM_KEY   | varchar(256)  | latin1_bin | NO   | PRI | NULL    |       | select,insert,update,references |         |
    | PARAM_VALUE | varchar(4000) | latin1_bin | YES  |     | NULL    |       | select,insert,update,references |         |
    +-------------+---------------+------------+------+-----+---------+-------+---------------------------------+---------+
    3 rows in set (0.001 sec)

    MariaDB [metastore]>     show full columns from PARTITION_KEYS;
    +--------------+---------------+------------+------+-----+---------+-------+---------------------------------+---------+
    | Field        | Type          | Collation  | Null | Key | Default | Extra | Privileges                      | Comment |
    +--------------+---------------+------------+------+-----+---------+-------+---------------------------------+---------+
    | TBL_ID       | bigint(20)    | NULL       | NO   | PRI | NULL    |       | select,insert,update,references |         |
    | PKEY_COMMENT | varchar(4000) | latin1_bin | YES  |     | NULL    |       | select,insert,update,references |         |
    | PKEY_NAME    | varchar(128)  | latin1_bin | NO   | PRI | NULL    |       | select,insert,update,references |         |
    | PKEY_TYPE    | varchar(767)  | latin1_bin | NO   |     | NULL    |       | select,insert,update,references |         |
    | INTEGER_IDX  | int(11)       | NULL       | NO   |     | NULL    |       | select,insert,update,references |         |
    +--------------+---------------+------------+------+-----+---------+-------+---------------------------------+---------+
    5 rows in set (0.001 sec)

    MariaDB [metastore]>     show full columns from INDEX_PARAMS;
    +-------------+---------------+------------+------+-----+---------+-------+---------------------------------+---------+
    | Field       | Type          | Collation  | Null | Key | Default | Extra | Privileges                      | Comment |
    +-------------+---------------+------------+------+-----+---------+-------+---------------------------------+---------+
    | INDEX_ID    | bigint(20)    | NULL       | NO   | PRI | NULL    |       | select,insert,update,references |         |
    | PARAM_KEY   | varchar(256)  | latin1_bin | NO   | PRI | NULL    |       | select,insert,update,references |         |
    | PARAM_VALUE | varchar(4000) | latin1_bin | YES  |     | NULL    |       | select,insert,update,references |         |
    +-------------+---------------+------------+------+-----+---------+-------+---------------------------------+---------+
    3 rows in set (0.001 sec)

    MariaDB [metastore]>     show full columns from DATABASE_PARAMS;
    +-------------+---------------+------------+------+-----+---------+-------+---------------------------------+---------+
    | Field       | Type          | Collation  | Null | Key | Default | Extra | Privileges                      | Comment |
    +-------------+---------------+------------+------+-----+---------+-------+---------------------------------+---------+
    | DB_ID       | bigint(20)    | NULL       | NO   | PRI | NULL    |       | select,insert,update,references |         |
    | PARAM_KEY   | varchar(180)  | latin1_bin | NO   | PRI | NULL    |       | select,insert,update,references |         |
    | PARAM_VALUE | varchar(4000) | latin1_bin | YES  |     | NULL    |       | select,insert,update,references |         |
    +-------------+---------------+------------+------+-----+---------+-------+---------------------------------+---------+
    3 rows in set (0.001 sec)

    MariaDB [metastore]>     show full columns from DBS;
    +-----------------+---------------+------------+------+-----+---------+-------+---------------------------------+---------+
    | Field           | Type          | Collation  | Null | Key | Default | Extra | Privileges                      | Comment |
    +-----------------+---------------+------------+------+-----+---------+-------+---------------------------------+---------+
    | DB_ID           | bigint(20)    | NULL       | NO   | PRI | NULL    |       | select,insert,update,references |         |
    | DESC            | varchar(4000) | latin1_bin | YES  |     | NULL    |       | select,insert,update,references |         |
    | DB_LOCATION_URI | varchar(4000) | latin1_bin | NO   |     | NULL    |       | select,insert,update,references |         |
    | NAME            | varchar(128)  | latin1_bin | YES  | UNI | NULL    |       | select,insert,update,references |         |
    | OWNER_NAME      | varchar(128)  | latin1_bin | YES  |     | NULL    |       | select,insert,update,references |         |
    | OWNER_TYPE      | varchar(10)   | latin1_bin | YES  |     | NULL    |       | select,insert,update,references |         |
    +-----------------+---------------+------------+------+-----+---------+-------+---------------------------------+---------+
    6 rows in set (0.001 sec)

    MariaDB [metastore]> alter table COLUMNS_V2 modify COMMENT varchar(256) character set utf8 collate utf8_general_ci;
        alter table TABLE_PARAMS modify PARAM_VALUE mediumtext character set utf8 collate utf8_general_ci;
    Query OK, 0 rows affected (0.021 sec)              
    Records: 0  Duplicates: 0  Warnings: 0

    MariaDB [metastore]>     alter table TABLE_PARAMS modify PARAM_VALUE mediumtext character set utf8 collate utf8_general_ci;
        alter table SERDE_PARAMS modify PARAM_VALUE mediumtext character set utf8 collate utf8_general_ci;
        alter table SD_PARAMS modify PARAM_VALUE mediumtext character set utf8 collate utf8_general_ci;
    Query OK, 0 rows affected (0.016 sec)              
    Records: 0  Duplicates: 0  Warnings: 0

    MariaDB [metastore]>     alter table SERDE_PARAMS modify PARAM_VALUE mediumtext character set utf8 collate utf8_general_ci;
    Query OK, 0 rows affected (0.015 sec)              
    Records: 0  Duplicates: 0  Warnings: 0

    MariaDB [metastore]>     alter table SD_PARAMS modify PARAM_VALUE mediumtext character set utf8 collate utf8_general_ci;
        alter table PARTITION_PARAMS modify PARAM_VALUE varchar(4000) character set utf8 collate utf8_general_ci;
        alter table PARTITION_KEYS modify PKEY_COMMENT varchar(4000) character set utf8 collate utf8_general_ci;
    Query OK, 0 rows affected (0.014 sec)              
    Records: 0  Duplicates: 0  Warnings: 0

    MariaDB [metastore]>     alter table PARTITION_PARAMS modify PARAM_VALUE varchar(4000) character set utf8 collate utf8_general_ci;
        alter table INDEX_PARAMS modify PARAM_VALUE varchar(4000) character set utf8 collate utf8_general_ci;
    Query OK, 0 rows affected (0.015 sec)              
    Records: 0  Duplicates: 0  Warnings: 0

    MariaDB [metastore]>     alter table PARTITION_KEYS modify PKEY_COMMENT varchar(4000) character set utf8 collate utf8_general_ci;
        alter table DATABASE_PARAMS modify PARAM_VALUE varchar(4000) character set utf8 collate utf8_general_ci;
    Query OK, 0 rows affected (0.016 sec)              
    Records: 0  Duplicates: 0  Warnings: 0

    MariaDB [metastore]>     alter table INDEX_PARAMS modify PARAM_VALUE varchar(4000) character set utf8 collate utf8_general_ci;
    Query OK, 0 rows affected (0.015 sec)              racter set utf8 collate utf8_general_ci;Stage: 2 of 2 'Enabling keys'      0% of stage done
    Records: 0  Duplicates: 0  Warnings: 0

    MariaDB [metastore]>     alter table DATABASE_PARAMS modify PARAM_VALUE varchar(4000) character set utf8 collate utf8_general_ci;
    Query OK, 0 rows affected (0.016 sec)              
    Records: 0  Duplicates: 0  Warnings: 0

    MariaDB [metastore]>     alter table DBS modify `DESC` varchar(4000) character set utf8 collate utf8_general_ci;
    Query OK, 0 rows affected (0.010 sec)              
    Records: 0  Duplicates: 0  Warnings: 0

    MariaDB [metastore]> show full columns from COLUMNS_V2;
    +-------------+--------------+-------------------+------+-----+---------+-------+---------------------------------+---------+
    | Field       | Type         | Collation         | Null | Key | Default | Extra | Privileges                      | Comment |
    +-------------+--------------+-------------------+------+-----+---------+-------+---------------------------------+---------+
    | CD_ID       | bigint(20)   | NULL              | NO   | PRI | NULL    |       | select,insert,update,references |         |
    | COMMENT     | varchar(256) | utf8_general_ci   | YES  |     | NULL    |       | select,insert,update,references |         |
    | COLUMN_NAME | varchar(767) | latin1_bin        | NO   | PRI | NULL    |       | select,insert,update,references |         |
    | TYPE_NAME   | mediumtext   | latin1_swedish_ci | YES  |     | NULL    |       | select,insert,update,references |         |
    | INTEGER_IDX | int(11)      | NULL              | NO   |     | NULL    |       | select,insert,update,references |         |
    +-------------+--------------+-------------------+------+-----+---------+-------+---------------------------------+---------+
    5 rows in set (0.001 sec)

    MariaDB [metastore]>     show full columns from TABLE_PARAMS;
        show full columns from SERDE_PARAMS;
    +-------------+--------------+-----------------+------+-----+---------+-------+---------------------------------+---------+
    | Field       | Type         | Collation       | Null | Key | Default | Extra | Privileges                      | Comment |
    +-------------+--------------+-----------------+------+-----+---------+-------+---------------------------------+---------+
    | TBL_ID      | bigint(20)   | NULL            | NO   | PRI | NULL    |       | select,insert,update,references |         |
    | PARAM_KEY   | varchar(256) | latin1_bin      | NO   | PRI | NULL    |       | select,insert,update,references |         |
    | PARAM_VALUE | mediumtext   | utf8_general_ci | YES  |     | NULL    |       | select,insert,update,references |         |
    +-------------+--------------+-----------------+------+-----+---------+-------+---------------------------------+---------+
    3 rows in set (0.011 sec)

    MariaDB [metastore]>     show full columns from SERDE_PARAMS;
    +-------------+--------------+-----------------+------+-----+---------+-------+---------------------------------+---------+
    | Field       | Type         | Collation       | Null | Key | Default | Extra | Privileges                      | Comment |
    +-------------+--------------+-----------------+------+-----+---------+-------+---------------------------------+---------+
    | SERDE_ID    | bigint(20)   | NULL            | NO   | PRI | NULL    |       | select,insert,update,references |         |
    | PARAM_KEY   | varchar(256) | latin1_bin      | NO   | PRI | NULL    |       | select,insert,update,references |         |
    | PARAM_VALUE | mediumtext   | utf8_general_ci | YES  |     | NULL    |       | select,insert,update,references |         |
    +-------------+--------------+-----------------+------+-----+---------+-------+---------------------------------+---------+
    3 rows in set (0.001 sec)

    MariaDB [metastore]>     show full columns from SD_PARAMS;
    +-------------+--------------+-----------------+------+-----+---------+-------+---------------------------------+---------+
    | Field       | Type         | Collation       | Null | Key | Default | Extra | Privileges                      | Comment |
    +-------------+--------------+-----------------+------+-----+---------+-------+---------------------------------+---------+
    | SD_ID       | bigint(20)   | NULL            | NO   | PRI | NULL    |       | select,insert,update,references |         |
    | PARAM_KEY   | varchar(256) | latin1_bin      | NO   | PRI | NULL    |       | select,insert,update,references |         |
    | PARAM_VALUE | mediumtext   | utf8_general_ci | YES  |     | NULL    |       | select,insert,update,references |         |
    +-------------+--------------+-----------------+------+-----+---------+-------+---------------------------------+---------+
    3 rows in set (0.001 sec)

    MariaDB [metastore]>     show full columns from PARTITION_PARAMS;
    +-------------+---------------+-----------------+------+-----+---------+-------+---------------------------------+---------+
    | Field       | Type          | Collation       | Null | Key | Default | Extra | Privileges                      | Comment |
    +-------------+---------------+-----------------+------+-----+---------+-------+---------------------------------+---------+
    | PART_ID     | bigint(20)    | NULL            | NO   | PRI | NULL    |       | select,insert,update,references |         |
    | PARAM_KEY   | varchar(256)  | latin1_bin      | NO   | PRI | NULL    |       | select,insert,update,references |         |
    | PARAM_VALUE | varchar(4000) | utf8_general_ci | YES  |     | NULL    |       | select,insert,update,references |         |
    +-------------+---------------+-----------------+------+-----+---------+-------+---------------------------------+---------+
    3 rows in set (0.001 sec)

    MariaDB [metastore]>     show full columns from PARTITION_KEYS;
    +--------------+---------------+-----------------+------+-----+---------+-------+---------------------------------+---------+
    | Field        | Type          | Collation       | Null | Key | Default | Extra | Privileges                      | Comment |
    +--------------+---------------+-----------------+------+-----+---------+-------+---------------------------------+---------+
    | TBL_ID       | bigint(20)    | NULL            | NO   | PRI | NULL    |       | select,insert,update,references |         |
    | PKEY_COMMENT | varchar(4000) | utf8_general_ci | YES  |     | NULL    |       | select,insert,update,references |         |
    | PKEY_NAME    | varchar(128)  | latin1_bin      | NO   | PRI | NULL    |       | select,insert,update,references |         |
    | PKEY_TYPE    | varchar(767)  | latin1_bin      | NO   |     | NULL    |       | select,insert,update,references |         |
    | INTEGER_IDX  | int(11)       | NULL            | NO   |     | NULL    |       | select,insert,update,references |         |
    +--------------+---------------+-----------------+------+-----+---------+-------+---------------------------------+---------+
    5 rows in set (0.001 sec)

    MariaDB [metastore]>     show full columns from INDEX_PARAMS;
    +-------------+---------------+-----------------+------+-----+---------+-------+---------------------------------+---------+
    | Field       | Type          | Collation       | Null | Key | Default | Extra | Privileges                      | Comment |
    +-------------+---------------+-----------------+------+-----+---------+-------+---------------------------------+---------+
    | INDEX_ID    | bigint(20)    | NULL            | NO   | PRI | NULL    |       | select,insert,update,references |         |
    | PARAM_KEY   | varchar(256)  | latin1_bin      | NO   | PRI | NULL    |       | select,insert,update,references |         |
    | PARAM_VALUE | varchar(4000) | utf8_general_ci | YES  |     | NULL    |       | select,insert,update,references |         |
    +-------------+---------------+-----------------+------+-----+---------+-------+---------------------------------+---------+
    3 rows in set (0.001 sec)

    MariaDB [metastore]>     show full columns from DATABASE_PARAMS;
    +-------------+---------------+-----------------+------+-----+---------+-------+---------------------------------+---------+
    | Field       | Type          | Collation       | Null | Key | Default | Extra | Privileges                      | Comment |
    +-------------+---------------+-----------------+------+-----+---------+-------+---------------------------------+---------+
    | DB_ID       | bigint(20)    | NULL            | NO   | PRI | NULL    |       | select,insert,update,references |         |
    | PARAM_KEY   | varchar(180)  | latin1_bin      | NO   | PRI | NULL    |       | select,insert,update,references |         |
    | PARAM_VALUE | varchar(4000) | utf8_general_ci | YES  |     | NULL    |       | select,insert,update,references |         |
    +-------------+---------------+-----------------+------+-----+---------+-------+---------------------------------+---------+
    3 rows in set (0.001 sec)

    MariaDB [metastore]>     show full columns from DBS;
    +-----------------+---------------+-----------------+------+-----+---------+-------+---------------------------------+---------+
    | Field           | Type          | Collation       | Null | Key | Default | Extra | Privileges                      | Comment |
    +-----------------+---------------+-----------------+------+-----+---------+-------+---------------------------------+---------+
    | DB_ID           | bigint(20)    | NULL            | NO   | PRI | NULL    |       | select,insert,update,references |         |
    | DESC            | varchar(4000) | utf8_general_ci | YES  |     | NULL    |       | select,insert,update,references |         |
    | DB_LOCATION_URI | varchar(4000) | latin1_bin      | NO   |     | NULL    |       | select,insert,update,references |         |
    | NAME            | varchar(128)  | latin1_bin      | YES  | UNI | NULL    |       | select,insert,update,references |         |
    | OWNER_NAME      | varchar(128)  | latin1_bin      | YES  |     | NULL    |       | select,insert,update,references |         |
    | OWNER_TYPE      | varchar(10)   | latin1_bin      | YES  |     | NULL    |       | select,insert,update,references |         |
    +-----------------+---------------+-----------------+------+-----+---------+-------+---------------------------------+---------+
    6 rows in set (0.002 sec)

    MariaDB [metastore]> 


6. 실행
    (in peter-kafka001, peter-kafka002)
    # vi /etc/profile.d/hive.sh
    export HIVE_HOME=/opt/hive
    export PATH=$PATH:$HIVE_HOME/bin/:$HIVE_HOME/sbin/

    # cd /opt/hive/
    # mkdir sbin
    # chown hive:hive sbin/
    # cd sbin
    # touch hive-metastoreserver.sh
    # chown hive:hive hive-metastoreserver.sh
    # chmod +x hive-metastoreserver.sh
    # vi hive-metastoreserver.sh
    <<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
#!/bin/bash
################################################################################
# hive-metastoreserver.sh
################################################################################
cd ~

if [[ "${HIVE_HOME}" == "" ]]; then
    echo -e "\e[33mThe HIVE_HOME environment variable is not set\e[0m"
    exit 1
fi

PORT="9083"
CONF_DIR="${HIVE_HOME}/conf/hive-metastoreserver"
LOG_FILE="/var/log/hive/hive-metastoreserver.log"

function PrintUsage() {
    local COMMAND=${0##*/}
    echo "Usage : shell> ${COMMAND} { start | stop | restart | status | pid | log }"
    echo "        ex> ${COMMAND} start"
    echo "        ex> ${COMMAND} stop"
    echo "        ex> ${COMMAND} restart"
    echo "        ex> ${COMMAND} status"
    echo "        ex> ${COMMAND} pid"
    echo "        ex> ${COMMAND} log"
    echo ""
    exit 1
}

function Pid() {
    local pid=$(ps -ef | grep -v grep | grep "org.apache.hadoop.hive.metastore.HiveMetaStore" | grep "p ${PORT}" | awk '{print $2}')
    echo ${pid} # <-- return PID
}

function Start() {
    local pid=$(Pid)
    if [[ "${pid}" == "" ]]; then
        ${HIVE_HOME}/bin/hive --config ${CONF_DIR} \
                              --service metastore -p ${PORT} \
                              > /dev/null 2>&1 &
        echo "staring hive-metastoreserver ..."
    else
        echo "hive-metastoreserver is running! (PID=${pid})"
    fi
}

function Stop() {
    local pid=$(Pid)
    if [[ "${pid}" == "" ]]; then
        echo "hive-metastoreserver is not running!"
    else
        echo "stopping hive-metastoreserver ... (PID=${pid})"
        kill ${pid}
    fi
}

function Restart() {
    local pid=$(Pid)
    if [ "${pid}" == "" ]; then
        Start
    else
        Stop
        while true; do
            local pid=$(Pid)
            if [[ "${pid}" == "" ]]; then
                break
            fi
            sleep 1
        done
        sleep 1
        Start
    fi
}

function Status() {
    local pid=$(Pid)
    if [[ "${pid}" == "" ]]; then
        echo "hive-metastoreserver is not running!"
    else
        echo "hive-metastoreserver is running! (PID = ${pid})"
        ps -ef | grep ${pid}
    fi
}

function Log() {
    echo -e "tail -F \e[33m${LOG_FILE}\e[0m"
    tail -F ${LOG_FILE}
}

ACTION=${1}
case "${ACTION}" in
    pid)
        Pid
        ;;
    start)
        Start
        ;;
    stop)
        Stop
        ;;
    restart)
        Restart
        ;;
    status)
        Status
        ;;
    log)
        Log
        ;;
    *)
        PrintUsage
        ;;
esac
    >>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>

    # su - hive
    $ hive-metastoreserver.sh start
    $ hive-metastoreserver.sh status
    ($ hive-metastoreserver.sh stop)


7. 설정(for ThriftServer)
    # su - hive
    $ cd /opt/hive/conf
    $ mkdir hive-thriftserver
    $ cd hive-thriftserver
    $ cp -p ../hive-metastoreserver/hive-env.sh .

    $ cp -p ../hive-metastoreserver/hive-log4j2.properties .
    $ vi hive-log4j2.properties 
    <<<<<<<<<<<<<<<<<<<<<<
    ...
    property.hive.log.file = hive-thriftserver.log
    ...
    >>>>>>>>>>>>>>>>>>>>>>>

    $ vi hive-site.xml
<<<<<<<<<<<<<<<<<
<?xml version="1.0"?>
<?xml-stylesheet type="text/xsl" href="configuration.xsl"?>

<configuration>
  <!--
  <property>
    <name>fs.defaultFS</name>
    <value>hdfs://nv-accel-w01:15000</value>
    <value>hdfs://peter-cluster</value>
  </property>
  -->
  <property>
    <name>hive.exec.stagingdir</name>
    <value>/tmp/hive-staging/.hive</value>
  </property>

  <!-- Metastore Server -->
  <property>
    <name>hive.metastore.uris</name>
    <value>thrift://peter-kafka001:9083,thrift://peter-kafka002:9083</value>
  </property>
  <property>
    <name>hive.metastore.execute.setugi</name>
    <value>true</value>
  </property>
  <property>
    <name>hive.metastore.schema.verification</name>
    <value>false</value>
  </property>

  <!-- Authorization -->
  <!--
  <property>
    <name>hive.users.in.admin.role</name>
    <value>hive</value>
  </property>
  <property>
    <name>hive.security.authorization.enabled</name>
    <value>true</value>
    <description>enable or disable the Hive client authorization</description>
  </property>
  <property>
    <name>hive.security.authorization.manager</name>
    <value>org.apache.hadoop.hive.ql.security.authorization.plugin.sqlstd.SQLStdHiveAuthorizerFactory</value>
  </property>
  -->

  <!-- Authentication -->
  <property>
    <name>hive.security.authenticator.manager</name>
    <value>org.apache.hadoop.hive.ql.security.SessionStateUserAuthenticator</value>
  </property>
  <property>
    <name>hive.server2.enable.doAs</name>
    <value>true</value>
  </property>
  <property>
    <name>hive.server2.authentication</name>
    <value>NONE</value> <!-- ### CUSTOM, NONE -->
  </property>

  <!-- Hive Server2 -->
  <property>
    <name>hive.server2.webui.port</name>
    <value>0</value>
  </property>
  <property>
    <name>hive.server2.logging.operation.enabled</name>
    <value>false</value>
  </property>
  <property>
    <name>hive.server2.thrift.min.worker.threads</name>
    <value>1</value>
  </property>
  <property>
    <name>hive.server2.thrift.max.worker.threads</name>
    <value>50</value>
  </property>
  <property>
    <name>hive.server2.close.session.on.disconnect</name>
    <value>true</value>
  </property>
  <property>
    <name>hive.server2.session.check.interval</name>
    <value>5m</value>
  </property>
  <property>
    <name>hive.server2.idle.operation.timeout</name>
    <value>1h</value>
  </property>
  <property>
    <name>hive.server2.idle.session.timeout</name>
    <value>1d</value>
  </property>

  <!-- Yarn Queue -->
  <property>
    <name>mapreduce.job.queuename</name>
    <value>default</value>
  </property>

  <!-- etc... -->
  <property>
    <name>hive.strict.checks.large.query</name>
    <value>true</value>
  </property>
  <property>
    <name>hive.merge.smallfiles.avgsize</name>
    <value>4194304</value>
  </property>
  <property>
    <name>hive.merge.mapredfiles</name>
    <value>true</value>
  </property>
  <property>
    <name>hive.resultset.use.unique.column.names</name>
    <value>false</value>
  </property>
</configuration>
>>>>>>>>>>>>>>>>>


8. 실행(for ThriftServer)
    1) 설정
    # su - hive
    $ cd /opt/hive/sbin
    $ cp -p hive-metastoreserver.sh hive-thriftserver.sh
    $ vi hive-thriftserver.sh
<<<<<<<<<<<<<<<<<<<<
#!/bin/bash
################################################################################
# hive-thriftserver.sh
################################################################################
cd ~

if [[ "${HIVE_HOME}" == "" ]]; then
    echo -e "\e[33mThe HIVE_HOME environment variable is not set\e[0m"
    exit 1
fi

PORT="10000"
CONF_DIR="${HIVE_HOME}/conf/hive-thriftserver"
LOG_FILE="/var/log/hive/hive-thriftserver.log"

function PrintUsage() {
    local COMMAND=${0##*/}
    echo "Usage : shell> ${COMMAND} { start | stop | restart | status | pid | log }"
    echo "        ex> ${COMMAND} start"
    echo "        ex> ${COMMAND} stop"
    echo "        ex> ${COMMAND} restart"
    echo "        ex> ${COMMAND} status"
    echo "        ex> ${COMMAND} pid"
    echo "        ex> ${COMMAND} log"
    echo ""
    exit 1
}

function Pid() {
    local pid=$(ps -ef | grep -v grep | grep -v tail | grep "org.apache.hive.service.server.HiveServer2" | grep "hive.server2.thrift.port=${PORT}" | awk '{print $2}')
    echo ${pid} # <-- return PID
}

function Start() {
    local pid=$(Pid)
    if [[ "${pid}" == "" ]]; then
        ${HIVE_HOME}/bin/hive --config ${CONF_DIR} \
                              --service hiveserver2 \
                              --hiveconf hive.server2.thrift.port=${PORT} \
                              > /dev/null 2>&1 &
        echo "staring hive-thriftserver ..."
    else
        echo "hive-thriftserver is running! (PID=${pid})"
    fi
}

function Stop() {
    local pid=$(Pid)
    if [[ "${pid}" == "" ]]; then
        echo "hive-thriftserver is not running!"
    else
        echo "stopping hive-thriftserver ... (PID=${pid})"
        kill ${pid}
    fi
}

function Restart() {
    local pid=$(Pid)
    if [ "${pid}" == "" ]; then
        Start
    else
        Stop
        while true; do
            local pid=$(Pid)
            if [[ "${pid}" == "" ]]; then
                break
            fi
            sleep 1
        done
        sleep 1
        Start
    fi
}

function Status() {
    local pid=$(Pid)
    if [[ "${pid}" == "" ]]; then
        echo "hive-thriftserver is not running!"
    else
        echo "hive-thriftserver is running! (PID = ${pid})"
        ps -ef | grep ${pid}
    fi
}

function Log() {
    echo -e "tail -F \e[33m${LOG_FILE}\e[0m"
    tail -F ${LOG_FILE}
}

ACTION=${1}
case "${ACTION}" in
    pid)
        Pid
        ;;
    start)
        Start
        ;;
    stop)
        Stop
        ;;
    restart)
        Restart
        ;;
    status)
        Status
        ;;
    log)
        Log
        ;;
    *)
        PrintUsage
        ;;
esac
>>>>>>>>>>>>>>>>>>>>

    2) 실행
    $ hive-thriftserver.sh start
    $ hive-thriftserver.sh status
    ($ hive-thriftserver.sh stop)

    3) 확인
    (테스트계정 생성)
    # useradd pjy -u 5287 -g wheel

    (hdfs상에 hive를 위한 디렉토리 생성)
    # su - hdfs

    $ hadoop fs -mkdir /user/hive
    $ hadoop fs -chown hive:hive /user/hive
    $ hadoop fs -chmod 1777 /user/hive

    $ hadoop fs -mkdir /user/hive/warehouse
    $ hadoop fs -chown hive:hive /user/hive/warehouse
    $ hadoop fs -chmod 1777 /user/hive/warehouse

    $ hadoop fs -mkdir /tmp
    $ hadoop fs -chmod 1777 /tmp

    $ hadoop fs -mkdir /tmp/hive-staging
    $ hadoop fs -chmod 1777 /tmp/hive-staging

    $ hadoop fs -mkdir /user/pjy
    $ hadoop fs -chown pjy:pjy /user/pjy

    # su - hive
    $ beeline -u jdbc:hive2://peter-kafka001:10000 -n hive
    NOTE. -n 계정명은 HDFS상에 사용할 계정명 입력

    4. DBeaver에서 접속 설정
    General
      JDBC URL => jdbc:hive2://peter-kafka001:10000,peter-kafka002:10000
      Database/Schema: => ()
    Authentification (Database Navie)
      Username => pjy (HDFS상에 저장시 사용될 계정)
      Password => ()
    NOTE. HDFS상의 계정별 설정시에 필요한 설정은
     hadoop의 core-site.xml에 
        hadoop.proxyuser.hive.groups=*
        hadoop.proxyuser.hive.hosts=*
     hive의 hive-site.xml (thriftserver용)
        hive.server2.enable.doAs=true
    (아직 인증은 어떻게 처리해야할지 고민중...)

.END OF HIVE

